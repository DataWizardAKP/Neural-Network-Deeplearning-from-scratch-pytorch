{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"vscode":{"languageId":"plaintext"},"id":"MP_aRgC1Xsl3","executionInfo":{"status":"ok","timestamp":1752003750768,"user_tz":-330,"elapsed":42,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deRmTscqGrd0","executionInfo":{"status":"ok","timestamp":1752003760839,"user_tz":-330,"elapsed":10089,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"80f18103-b4f2-495e-cc04-098052d317ab"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Convert images to PyTorch tensors and then we normalize pixel values to [-1, 1]\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])"],"metadata":{"id":"iIJMuu_93F2w","executionInfo":{"status":"ok","timestamp":1752003760841,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Subset\n","\n","# Download and load the training dataset\n","train_dataset = datasets.FashionMNIST(\n","    root='/content/gdrive/MyDrive/Lecture Notes & Study Materials/Practical Deep Learning using Pytorch/fashion_mnist',     # directory to store the data\n","    train=True,        # load training data\n","    download=True,     # download if not already downloaded\n","    transform=transform  # apply transform\n",")\n","\n","# Load the test dataset\n","test_dataset = datasets.FashionMNIST(\n","    root='/content/gdrive/MyDrive/Lecture Notes & Study Materials/Practical Deep Learning using Pytorch/fashion_mnist',\n","    train=False,\n","    download=True,\n","    transform=transform\n",")\n","#Selecting 6000 instances for training\n","train_dataset= Subset(train_dataset, indices=range(6000))\n","\n","#Selecting 6000 instances for training\n","test_dataset= Subset(train_dataset, indices=range(500))"],"metadata":{"id":"VkE-tA4G9xo5","executionInfo":{"status":"ok","timestamp":1752003765035,"user_tz":-330,"elapsed":4199,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["label_names = [\n","    \"T-shirt/top\",  # 0\n","    \"Trouser\",      # 1\n","    \"Pullover\",     # 2\n","    \"Dress\",        # 3\n","    \"Coat\",         # 4\n","    \"Sandal\",       # 5\n","    \"Shirt\",        # 6\n","    \"Sneaker\",      # 7\n","    \"Bag\",          # 8\n","    \"Ankle boot\"    # 9\n","]"],"metadata":{"id":"4o-nLwC15JbW","executionInfo":{"status":"ok","timestamp":1752003765039,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["image, label = train_dataset[0]"],"metadata":{"id":"1jWmzvP64NPd","executionInfo":{"status":"ok","timestamp":1752003765046,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def show_image(index):\n","\n","    # Get a single sample using __getitem__\n","    img, label = train_dataset[index]  # change index as needed\n","\n","    print(\"Label:\", label)  # will print the class index (0–9)\n","    print(\"It is a :\", label_names[label])\n","\n","    # Plot the image (it's a single-channel grayscale)\n","    plt.imshow(img.squeeze())  # remove channel dimension with squeeze()\n","    plt.title(f\"Label: {label}\")\n","    plt.axis('off')\n","    plt.show()\n","\n","show_image(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464},"id":"132k4_lw4nQl","executionInfo":{"status":"ok","timestamp":1752003765258,"user_tz":-330,"elapsed":210,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"ae851053-6374-4233-834a-21c9f2eadb29"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Label: 0\n","It is a : T-shirt/top\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Wrap datasets in DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"],"metadata":{"id":"XA7aVolJHRHN","executionInfo":{"status":"ok","timestamp":1752003765289,"user_tz":-330,"elapsed":27,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def flatten_image(x):\n","  return x.view(x.shape[0], -1)"],"metadata":{"id":"5ZfECArQC4uG","executionInfo":{"status":"ok","timestamp":1752003765369,"user_tz":-330,"elapsed":76,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# create model class\n","import torch\n","import torch.nn as nn\n","\n","class SimpleANN(nn.Module):\n","  def __init__(self, feature_size, num_classes):\n","    super().__init__()\n","\n","    self.model=nn.Sequential(nn.Linear(feature_size,128),\n","    nn.ReLU(),\n","    nn.Linear(128,64),\n","    nn.ReLU(),\n","    nn.Linear(64, num_classes),\n","    nn.Softmax(dim=1)) # Added dim=1 here\n","\n","  def forward(self,x):\n","    x=self.model(x)\n","    return x"],"metadata":{"id":"XrTareXJjOkq","executionInfo":{"status":"ok","timestamp":1752003765373,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["epochs=100\n","lr=0.1"],"metadata":{"id":"pSwYvlN527_8","executionInfo":{"status":"ok","timestamp":1752003765377,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["temp=flatten_image(train_dataset[0][0])\n","num_features=temp.shape[1]"],"metadata":{"id":"UcMN0lVRCUQn","executionInfo":{"status":"ok","timestamp":1752003765381,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model=SimpleANN(num_features,10)\n","\n","#defining loss\n","criterion=nn.CrossEntropyLoss()\n","\n","#defining optimizer\n","optimizer=torch.optim.SGD(model.parameters(),lr=lr)"],"metadata":{"id":"i1SNXL4qEF7-","executionInfo":{"status":"ok","timestamp":1752003765384,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(\"Number of batches\",len(train_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXx3mObTFm57","executionInfo":{"status":"ok","timestamp":1752003765395,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"df8b74e7-4843-4513-8fbc-7d887e5181f6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of batches 94\n"]}]},{"cell_type":"code","source":["#training loop\n","\n","for epoch in range(epochs):\n","  total_epoch_loss=0\n","  for batch_features, batch_labels in train_loader:\n","    #flatten_image\n","    flattend_features=flatten_image(batch_features)\n","\n","    #forward pass\n","    outputs=model(flattend_features)\n","\n","    #calculate loss\n","    loss=criterion(outputs, batch_labels)\n","\n","    #backward pass\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    #update weights\n","    optimizer.step()\n","\n","    total_epoch_loss+=loss.item()\n","\n","  #printing the average loss\n","  average_loss=total_epoch_loss/len(train_loader)\n","  print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bz8ONBewEjC9","executionInfo":{"status":"ok","timestamp":1752003934144,"user_tz":-330,"elapsed":168748,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"9c2ef51b-def5-4272-cd8f-3172a8b40d96"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 2.2658339916391577\n","Epoch 2/100, Loss: 2.0771240084729294\n","Epoch 3/100, Loss: 1.9002405861590772\n","Epoch 4/100, Loss: 1.7706499340686392\n","Epoch 5/100, Loss: 1.7298028989041105\n","Epoch 6/100, Loss: 1.7090741852496534\n","Epoch 7/100, Loss: 1.6986050567728408\n","Epoch 8/100, Loss: 1.6877366494625172\n","Epoch 9/100, Loss: 1.6805608234506972\n","Epoch 10/100, Loss: 1.6741664308182738\n","Epoch 11/100, Loss: 1.6697475047821695\n","Epoch 12/100, Loss: 1.6644752647014374\n","Epoch 13/100, Loss: 1.663905401179131\n","Epoch 14/100, Loss: 1.6608152795345226\n","Epoch 15/100, Loss: 1.65619972284804\n","Epoch 16/100, Loss: 1.654868077724538\n","Epoch 17/100, Loss: 1.6511880159378052\n","Epoch 18/100, Loss: 1.648863283877677\n","Epoch 19/100, Loss: 1.6476588566252526\n","Epoch 20/100, Loss: 1.6448858101317223\n","Epoch 21/100, Loss: 1.6462711260673848\n","Epoch 22/100, Loss: 1.6420110349959516\n","Epoch 23/100, Loss: 1.641227643540565\n","Epoch 24/100, Loss: 1.6394154099707907\n","Epoch 25/100, Loss: 1.6357001852481923\n","Epoch 26/100, Loss: 1.6385814572902435\n","Epoch 27/100, Loss: 1.634384312528245\n","Epoch 28/100, Loss: 1.6346065947350035\n","Epoch 29/100, Loss: 1.6338134407997131\n","Epoch 30/100, Loss: 1.6307046565603702\n","Epoch 31/100, Loss: 1.6310113681123612\n","Epoch 32/100, Loss: 1.6309259419745588\n","Epoch 33/100, Loss: 1.629729873322426\n","Epoch 34/100, Loss: 1.6282443531016086\n","Epoch 35/100, Loss: 1.6249917289043994\n","Epoch 36/100, Loss: 1.6267143386475584\n","Epoch 37/100, Loss: 1.624130408814613\n","Epoch 38/100, Loss: 1.6238712300645544\n","Epoch 39/100, Loss: 1.6226378501729761\n","Epoch 40/100, Loss: 1.624163235755677\n","Epoch 41/100, Loss: 1.6203855834108718\n","Epoch 42/100, Loss: 1.621371162698624\n","Epoch 43/100, Loss: 1.620238073328708\n","Epoch 44/100, Loss: 1.6213470164765702\n","Epoch 45/100, Loss: 1.6190784078963258\n","Epoch 46/100, Loss: 1.6191754062125023\n","Epoch 47/100, Loss: 1.6184645231733932\n","Epoch 48/100, Loss: 1.616248698944741\n","Epoch 49/100, Loss: 1.6156869010722383\n","Epoch 50/100, Loss: 1.614910362882817\n","Epoch 51/100, Loss: 1.6146075586055189\n","Epoch 52/100, Loss: 1.6149338280900996\n","Epoch 53/100, Loss: 1.6152235751456403\n","Epoch 54/100, Loss: 1.613760790926345\n","Epoch 55/100, Loss: 1.6124138172636642\n","Epoch 56/100, Loss: 1.6111703553098313\n","Epoch 57/100, Loss: 1.6119747022365003\n","Epoch 58/100, Loss: 1.611654688703253\n","Epoch 59/100, Loss: 1.6099778033317405\n","Epoch 60/100, Loss: 1.6109437321094757\n","Epoch 61/100, Loss: 1.6096502022540315\n","Epoch 62/100, Loss: 1.6070101882549042\n","Epoch 63/100, Loss: 1.6071234484936328\n","Epoch 64/100, Loss: 1.609298917841404\n","Epoch 65/100, Loss: 1.6072721138913582\n","Epoch 66/100, Loss: 1.6070855513532112\n","Epoch 67/100, Loss: 1.6052898688519255\n","Epoch 68/100, Loss: 1.6050202821163422\n","Epoch 69/100, Loss: 1.6055660996031254\n","Epoch 70/100, Loss: 1.6054910497462496\n","Epoch 71/100, Loss: 1.6038054709738874\n","Epoch 72/100, Loss: 1.60331078412685\n","Epoch 73/100, Loss: 1.60400294496658\n","Epoch 74/100, Loss: 1.60321403310654\n","Epoch 75/100, Loss: 1.6016219537308876\n","Epoch 76/100, Loss: 1.602683185262883\n","Epoch 77/100, Loss: 1.6036557238152687\n","Epoch 78/100, Loss: 1.6022010268049036\n","Epoch 79/100, Loss: 1.6016679431529754\n","Epoch 80/100, Loss: 1.6007904613271673\n","Epoch 81/100, Loss: 1.6015806578575296\n","Epoch 82/100, Loss: 1.5998799788190963\n","Epoch 83/100, Loss: 1.5991943909766826\n","Epoch 84/100, Loss: 1.5994605878566175\n","Epoch 85/100, Loss: 1.6003051336775436\n","Epoch 86/100, Loss: 1.59859122240797\n","Epoch 87/100, Loss: 1.599093977441179\n","Epoch 88/100, Loss: 1.5978965328094807\n","Epoch 89/100, Loss: 1.5979109533289646\n","Epoch 90/100, Loss: 1.59870434441465\n","Epoch 91/100, Loss: 1.5979397487133107\n","Epoch 92/100, Loss: 1.599205641036338\n","Epoch 93/100, Loss: 1.598476215879968\n","Epoch 94/100, Loss: 1.596946064462053\n","Epoch 95/100, Loss: 1.596971461113463\n","Epoch 96/100, Loss: 1.5966382407127542\n","Epoch 97/100, Loss: 1.5972616342788046\n","Epoch 98/100, Loss: 1.597148910481879\n","Epoch 99/100, Loss: 1.597006436358107\n","Epoch 100/100, Loss: 1.5957006748686446\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","  accuracy=0\n","  for batch_features,batch_labels in test_loader:\n","    flattend_features=flatten_image(batch_features)\n","\n","    y_pred=model(flattend_features)\n","    y_pred=torch.argmax(y_pred,dim=1)\n","    correct = (y_pred == batch_labels).sum().item()\n","    accuracy=accuracy+correct\n","\n","print(\"Testing Accuracy:\",(accuracy/len(test_dataset))*100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ztDvznkMCn9","executionInfo":{"status":"ok","timestamp":1752003934369,"user_tz":-330,"elapsed":227,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"96158389-c549-4446-caa5-4df96aec88ac"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Accuracy: 85.8\n"]}]},{"cell_type":"markdown","source":["**Now let us check, how our trained model performes on the Training data instead of the test Data.**"],"metadata":{"id":"_2pePP_Sf9N8"}},{"cell_type":"code","source":["with torch.no_grad():\n","  accuracy=0\n","  for batch_features,batch_labels in train_loader:\n","    flattend_features=flatten_image(batch_features)\n","\n","    y_pred=model(flattend_features)\n","    y_pred=torch.argmax(y_pred,dim=1)\n","    correct = (y_pred == batch_labels).sum().item()\n","    accuracy=accuracy+correct\n","\n","print(\"Training Model Accuracy:\",(accuracy/len(train_dataset))*100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pzi1NqmAgGpU","executionInfo":{"status":"ok","timestamp":1752003935990,"user_tz":-330,"elapsed":1623,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"1e5be4d8-734a-44ab-cbdc-dcb158d6f670"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model Accuracy: 86.8\n"]}]},{"cell_type":"markdown","source":["It can be seen this model is underfitting a little. Now if Say the training accuracy is 10% more than test accuracy then the Model will be a overfitted Model."],"metadata":{"id":"-Gt522-ghyBg"}},{"cell_type":"markdown","source":["| **Strategies for Underfitting**                         | **Strategies for Overfitting**                           |\n","|----------------------------------------------------------|-----------------------------------------------------------|\n","| Increase model complexity (more layers/neurons)          | Add regularization (L2 weight decay)                      |\n","| Train for more epochs                                     | Use dropout layers                                        |\n","| Reduce regularization (lower dropout, weight decay)       | Apply early stopping                                      |\n","| Use better feature engineering or preprocessing           | Use data augmentation                                     |\n","| Use better activation functions (e.g., ReLU)              | Simplify model architecture                               |\n","| Reduce learning rate if diverging                         | Reduce training epochs                                    |\n","| Improve data quality or add relevant features             | Add noise to inputs or use label smoothing                |\n","|                                                           | Increase training data or use transfer learning           |\n"],"metadata":{"id":"EaAszPc5i43S"}},{"cell_type":"markdown","source":["**Important Strategies in Overfitting**"],"metadata":{"id":"82abGvVSAEqg"}},{"cell_type":"markdown","source":["### 🧠 Dropout in Neural Network Training\n","\n","**What is Dropout?**  \n","Dropout is a regularization technique used to prevent overfitting in neural networks.  \n","During training, **random neurons are \"dropped out\" (i.e., temporarily deactivated)** with a certain probability.\n","\n","---\n","\n","### 🎯 Why Use Dropout?\n","- Neural networks with many parameters can easily **overfit** to the training data.\n","- Dropout forces the network to **not rely on any single neuron**, promoting **redundancy and robustness**.\n","- It acts like training an **ensemble of different subnetworks** and averaging their predictions.\n","\n","---\n","\n","### 🔢 How Dropout Works (Mathematically)\n","\n","Let:\n","- `h` = activation of a neuron\n","- `r ~ Bernoulli(p)`, where `p` is the probability of **keeping** a neuron\n","\n","Then during **training**:\n","- `h̃ = h * r`  \n","  If `r = 1`, neuron is kept  \n","  If `r = 0`, neuron is dropped\n","\n","During **inference (testing)**:  \n","All neurons are used, but their outputs are **scaled by `p`** to maintain the expected value:\n","\n","- `h_inference = p * h`\n","\n","\n","\n","---\n","\n","### 🔧 Example in PyTorch\n","\n","```python\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Linear(128, 64),\n","    nn.ReLU(),\n","    nn.Dropout(p=0.5),  # 50% dropout\n","    nn.Linear(64, 10)\n",")\n"],"metadata":{"id":"Ms8sTEsSAFRx"}},{"cell_type":"markdown","source":["### ==== Batch Normalization ====\n","\n","**What is Batch Normalization?**  \n","Batch Normalization (often called **BatchNorm**) is a technique used in deep learning to make training **faster, more stable, and reliable**.\n","\n","It works by **normalizing the inputs to each layer** so that they have a consistent distribution (mean ~0, variance ~1) during training.\n","\n","---\n","\n","### Why is Batch Normalization Needed?\n","\n","- In deep networks, the inputs to each layer can change during training as earlier layers update. This is known as **internal covariate shift**.\n","- Such shifts can make training slow and unstable.\n","- BatchNorm fixes this by **standardizing the input to each layer**, reducing the sensitivity to weight initialization and allowing for **faster convergence**.\n","- It also acts as a kind of **regularizer**, reducing the need for dropout in some cases.\n","\n","---\n","\n","### ✅ Benefits of Batch Normalization\n","\n","- **Faster training** (you can use higher learning rates)\n","- **Improved stability** during training\n","- **Better performance** on validation/test sets\n","- **Less dependency** on careful initialization\n","- Often **reduces overfitting**\n","\n","---\n","\n","### 🔧 PyTorch Example\n","\n","```python\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Linear(128, 64),\n","    nn.BatchNorm1d(64),  # Normalizes the 64 features, applied before activation fucntion\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n",")\n"],"metadata":{"id":"zbb_ik7WDIJX"}},{"cell_type":"markdown","source":["\n","\n","### ✅ Regularization\n","\n","**What is Regularization?**  \n","Regularization is a technique used in machine learning to **prevent overfitting** by **penalizing complex models**. It helps the model generalize better to unseen data. Mainly used to reduce the gap between training and testing.\n","\n","---\n","\n","### 🎯 Why is Regularization Needed? (Loss +Penalty)\n","- A model may learn noise or irrelevant patterns from training data.\n","- Regularization discourages the model from being too flexible or overly complex.\n","\n","---\n","\n","### 🔧 Regularization Strategies\n","\n","| **Strategy**            | **Explanation (Simple)**                                                                   |\n","|-------------------------|---------------------------------------------------------------------------------------------|\n","| **L1 Regularization**   | Adds penalty equal to the **absolute value** of weights → encourages sparsity (zeros out weights) |\n","| **L2 Regularization**   | Adds penalty equal to the **square** of weights → keeps weights small but non-zero         |\n","| **Dropout**             | Randomly disables some neurons during training → prevents co-adaptation                    |\n","| **Early Stopping**      | Stops training when validation loss increases → avoids overfitting                         |\n","| **Data Augmentation**   | Creates variations of training data → improves generalization                              |\n","| **Weight Constraint**   | Limits how large weights can grow (e.g., max norm)                                         |\n","\n","---\n","\n","### 🧠 PyTorch Example of L2 Regularization\n","\n","```python\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n"],"metadata":{"id":"_bGbgybfoGHZ"}},{"cell_type":"markdown","source":["# 🔍 Hyperparameter Tuning Approaches in Deep Learning\n","\n","Hyperparameter tuning is essential to improve the performance of deep learning models. Here, we discuss three important methods: **Grid Search**, **Random Search with Cross Validation**, and **Bayesian Optimization**, with a special focus on **Optuna**.\n","\n","---\n","\n","## 🔢 Grid Search\n","\n","**Definition**: Grid Search performs an exhaustive search over a manually specified subset of the hyperparameter space. Every combination is trained and evaluated.\n","\n","**Pros**:\n","- Simple and interpretable.\n","- Guarantees that all combinations are tested.\n","\n","**Cons**:\n","- Computationally very expensive.\n","- Doesn’t scale well with dimensionality.\n","- May evaluate many irrelevant combinations.\n","\n","**Example**:\n","```python\n","param_grid = {\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'batch_size': [16, 32, 64]\n","}\n"],"metadata":{"id":"ZOaC2WhCTLTO"}},{"cell_type":"markdown","source":["🎲 **Random Search with Cross Validation (Randomized CV)**\n","Definition: Random Search randomly samples combinations from specified ranges or distributions for a fixed number of iterations. When combined with Cross-Validation (CV), it gives a reliable estimate of model performance.\n","\n","**Pros:**\n","\n","a. More efficient than Grid Search.\n","\n","b. Can find good parameters quickly.\n","\n","c. Effective when only a few hyperparameters dominate performance.\n","\n","**Cons:**\n","\n","a. Results may vary run-to-run due to randomness.\n","\n","b. Can miss good combinations if not enough samples.\n","\n"],"metadata":{"id":"FF6U0NkCT9Fd"}},{"cell_type":"markdown","source":["📈 **Bayesian Optimization** <br/>\n","**Definition:** Bayesian Optimization builds a probabilistic model (like a Gaussian Process) to estimate the objective function and uses this model to choose the most promising hyperparameter combinations.\n","\n","**📌 Key Steps:** <br/>\n","a. Surrogate Model: Approximates the performance function (e.g., validation loss).\n","\n","b. Acquisition Function: Selects next best hyperparameter set to try (e.g., Expected Improvement).\n","\n","c. Model Update: Incorporates new trial results into the surrogate.\n","\n","d. Iteration: Repeat to converge towards the optimal values.\n","\n","**Pros:**\n","\n","a. Very sample-efficient.\n","\n","b. Intelligent exploration and exploitation.\n","\n","c. Suitable for expensive models where each run is costly.\n","\n","**Cons:**\n","\n","a. More complex to implement.\n","\n","b. Slower per iteration than random/grid search.\n","\n"],"metadata":{"id":"GvwNdb-mUkTt"}},{"cell_type":"markdown","source":["## ⚙️ Optuna: A Powerful Framework for Bayesian Optimization\n","\n","Optuna is an open-source hyperparameter optimization framework built for efficiency and flexibility.\n","\n","### 🔧 Features:\n","- **Define-by-Run API**: Hyperparameters are defined dynamically during execution.\n","- **Efficient Sampling**: Uses Tree-structured Parzen Estimator (TPE), a form of Bayesian Optimization.\n","- **Pruning**: Stops poorly performing trials early based on intermediate results.\n","- **Visualization**: Provides plots for optimization history, parameter importance, parallel coordinates, etc.\n","- **Multi-objective optimization**: Supports optimizing multiple metrics simultaneously.\n","\n"],"metadata":{"id":"WaAY0SJTVXqN"}},{"cell_type":"markdown","source":["## 📌 Key Questions During Hyperparameter Tuning in Deep Learning\n","\n","While tuning a deep learning model, several critical hyperparameters must be evaluated and optimized. The following are the most commonly tuned elements:\n","\n","1. **Number of Hidden Layers**  \n","   - How many layers should the model have?\n","   - Deeper networks can capture more complex features but risk overfitting.\n","\n","2. **Neurons per Layer**  \n","   - How many neurons should be in each hidden layer?\n","   - More neurons allow higher capacity but increase computation and overfitting risk.\n","\n","3. **Number of Epochs**  \n","   - For how many iterations should the model be trained?\n","   - Too few may underfit; too many may overfit. Early stopping can help.\n","\n","4. **Optimizer**  \n","   - Which optimization algorithm to use? (e.g., SGD, Adam, RMSprop)\n","   - Different optimizers may converge faster or handle sparse gradients better.\n","\n","5. **Learning Rate**  \n","   - The step size for weight updates during training.\n","   - A crucial hyperparameter — too high can overshoot, too low can slow down learning.\n","\n","6. **Batch Size**  \n","   - Number of samples processed before the model is updated.\n","   - Affects training stability, speed, and generalization.\n","\n","7. **Dropout Rate**  \n","   - Fraction of neurons randomly dropped during training to prevent overfitting.\n","   - Commonly set between 0.2 to 0.5.\n","\n","8. **Weight Decay (L2 Regularization / λ)**  \n","   - Penalizes large weights by adding a regularization term to the loss.\n","   - Helps reduce overfitting; finding the optimal λ value is critical.\n","\n","---\n","\n","Each of these parameters influences the training dynamics and final model performance. Hyperparameter tuning aims to find the optimal combination that minimizes validation loss or maximizes validation accuracy.\n"],"metadata":{"id":"8k2cdhDGW96V"}},{"cell_type":"markdown","source":["**==========OPTUNA Hyperparmeter Tuning==========**"],"metadata":{"id":"tvyGnTisXS0d"}},{"cell_type":"markdown","source":["# 🎯 Hyperparameter Tuning with Optuna – Step-by-Step Guide\n","\n","Optuna is a modern hyperparameter optimization framework that automates the process of finding the best set of hyperparameters. It uses Bayesian Optimization (TPE) to search the space efficiently.\n","\n","---\n","\n","## 🔄 How Optuna Works\n","\n","1. **Define Objective Function**  \n","   This function contains:\n","   - Hyperparameter definitions using `trial.suggest_*`\n","   - Model definition and training\n","   - Performance evaluation\n","   - Returns the metric to optimize (accuracy, loss, etc.)\n","\n","2. **Search Space**  \n","   Define the space of possible values for each hyperparameter dynamically.\n","\n","3. **Model Initialization**  \n","   Use the sampled hyperparameters to create your model (e.g., hidden size, dropout).\n","\n","4. **Training Loop**  \n","   Train your model using PyTorch (or another framework) with standard loops.\n","\n","5. **Evaluation Loop**  \n","   Evaluate the model on a validation set using accuracy or loss.\n","\n","6. **Create and Run Study**  \n","   Use `optuna.create_study()` and `study.optimize()` to start optimization.\n","\n","---\n","\n","## ✅ What We Tune\n","\n","- **Hidden Size**: Number of neurons in hidden layer.\n","- **Dropout Rate**: Randomly zero out neurons to prevent overfitting.\n","- **Learning Rate**: Step size for optimizer.\n","- **Batch Size**: Number of samples per batch.\n","- **Weight Decay**: L2 regularization (λ).\n","\n","---\n","\n","## 📋 Summary Table\n","\n","| Step | Description |\n","|------|-------------|\n","| 🎛️ Search Space | Defined using `trial.suggest_int`, `suggest_float`, etc. |\n","| 🧠 Model Init | Model is built dynamically inside the `objective()` |\n","| 🔧 Param Init | Hyperparameters sampled from user-defined ranges |\n","| 🔁 Training | Typical PyTorch loop with `optimizer.step()` and `loss.backward()` |\n","| 🧪 Evaluation | Use validation set accuracy/loss as optimization goal |\n","| 🚀 Optimization | Optuna runs the above process over multiple trials |\n","\n","---\n","\n","## 📌 Why Use Optuna?\n","\n","- Define-by-run API (more flexible than static grid)\n","- Fast and efficient with Tree-structured Parzen Estimator (TPE)\n","- Built-in pruning to stop bad trials early\n","- Easy integration with PyTorch, TensorFlow, XGBoost, etc.\n","- Great visualizations and reporting tools\n","\n","---\n","\n","### 🧪 Sample Code Structure Overview:\n","\n","```python\n","def objective(trial):\n","    # 1. Sample hyperparameters  <--- The function at the input gets a  trail object.\n","    # We define multiple things, we define the lowest and the highest range.\n","    # hidden layer size will vary from 16-128, same for others.\n","    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n","    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n","    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n","    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n","\n","    # 2. Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # 3. Define model < --- Model Initialization\n","    model = nn.Sequential(\n","        nn.Linear(input_dim, hidden_size),\n","        nn.ReLU(),\n","        nn.Dropout(dropout_rate),\n","        nn.Linear(hidden_size, 2)\n","    )\n","\n","    # 4. Define loss and optimizer  <-- Parameter Initialization\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    # 5. Training loop   <--- The iterative training loop\n","    for epoch in range(20):\n","        for xb, yb in train_loader:\n","            optimizer.zero_grad()\n","            out = model(xb)\n","            loss = criterion(out, yb)\n","            loss.backward()\n","            optimizer.step()\n","\n","    # 6. Evaluation   <--- Evaluation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            preds = model(xb)\n","            predicted = torch.argmax(preds, dim=1)\n","            correct += (predicted == yb).sum().item()\n","            total += yb.size(0)\n","\n","    accuracy = correct / total\n","    return accuracy   #<-- at last it is returning the trail accuracy.\n","\n","# So this function or method is getting optuna trail object, and for the particular trail what is the accuracy we got.\n","\n","\n"],"metadata":{"id":"2v1RkcwcdEQ8"}},{"cell_type":"markdown","source":["## 🎯 What is a `study` in Optuna?\n","\n","In Optuna, a **`study`** is the main object that manages the entire hyperparameter optimization process.\n","\n","### 🔧 Key Roles of `study`:\n","- Defines the **direction** of optimization (maximize or minimize)\n","- Manages all **trials** (each trial is one evaluation of hyperparameters)\n","- Tracks the **best result** and **best parameters**\n","- Controls how new hyperparameters are sampled\n","\n","---\n","\n","### ✅ Create a Study\n","```python\n","study = optuna.create_study(direction=\"maximize\")\n","\n","---\n","\n","### 🔁 Run Optimization\n","```python\n","study.optimize(objective, n_trials=50)\n","\n","---\n","study.best_value      # Best score (e.g., highest accuracy)\n","study.best_params     # Best hyperparameter combination\n","study.best_trial      # Full details of the best trial (value, params, duration, etc.)\n","\n","### 🧠 Summary Table\n","\n","| Term               | Description                                        |\n","|--------------------|----------------------------------------------------|\n","| `study`            | Manages the entire hyperparameter tuning process   |\n","| `trial`            | A single run using one set of hyperparameters      |\n","| `study.optimize()` | Runs multiple trials to find the best parameters   |\n","| `study.best_value` | Best score (e.g., highest accuracy or lowest loss) |\n","| `study.best_params`| Hyperparameters that gave the best score           |\n","| `study.best_trial` | Complete info of the best trial (value + params)   |\n"],"metadata":{"id":"arFiigPeq1JC"}},{"cell_type":"markdown","source":["<hr style=\"height:4px; background-color:#000000; border:none;\" />\n","<hr style=\"height:4px; background-color:#000000; border:none;\" />\n","<h1 style=\"color:#1E90FF;\">🔵 Neural Network Optimization Coding using Optuna</h2>\n","\n"],"metadata":{"id":"8EPHPIwyd_0r"}},{"cell_type":"code","source":["# create model class\n","import torch\n","import torch.nn as nn\n","\n","class MyNN(nn.Module):\n","  def __init__(self, feature_size, num_classes, num_hidden_layers,neurons_per_layer):\n","    super().__init__()\n","    layers=[]\n","    input_layer_count=feature_size\n","    for i in range(num_hidden_layers):\n","\n","        layers.append(nn.Linear(input_layer_count,neurons_per_layer))\n","        layers.append(nn.BatchNorm1d(neurons_per_layer))\n","        layers.append(nn.ReLU())\n","        layers.append(nn.Dropout(0.3))\n","        input_layer_count=neurons_per_layer\n","\n","\n","    layers.append(nn.Linear(neurons_per_layer,num_classes))\n","    #layers.append(nn.Softmax(dim=1))\n","\n","    self.model=nn.Sequential(*layers) #unpacking the list\n","                    #nn.Sequential(*[l1, l2, l3])  ≡  nn.Sequential(l1, l2, l3)\n","\n","  def forward(self,x):\n","    x=self.model(x)\n","    return x"],"metadata":{"id":"0Cajr6lpyydh","executionInfo":{"status":"ok","timestamp":1752007026818,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def objective(trail):\n","\n","  #next hyperparameter values from the search space\n","  num_hidden_layers= trail.suggest_int(\"num_hidden_layers\",1,5)\n","  neurons_per_layer=trail.suggest_int(\"num_per_layers\",8,128,step=8)\n","\n","  #model init\n","  input_dim=784\n","  output_dim=10\n","\n","  model=MyNN(input_dim, output_dim, num_hidden_layers,neurons_per_layer)\n","  #Initialization rest of the parameters\n","  learning_rate=0.01\n","  epochs=50\n","  #Select the optimizer\n","  criterion=nn.CrossEntropyLoss()\n","  optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate, weight_decay=1e-4)\n","  #Run the Training Loop\n","\n","  for epoch in range(epochs):\n","    #total_epoch_loss=0\n","\n","    for batch_features, batch_labels in train_loader:\n","      flatten_data=flatten_image(batch_features)\n","      output=model(flatten_data)\n","      loss=criterion(output,batch_labels)\n","      optimizer.zero_grad()\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","\n","\n","  #Run Evaluation\n","\n","  with torch.no_grad():\n","    accuracy=0\n","    for batch_features,batch_labels in test_loader:\n","      flattend_features=flatten_image(batch_features)\n","\n","      y_pred=model(flattend_features)\n","      y_pred=torch.argmax(y_pred,dim=1)\n","      correct = (y_pred == batch_labels).sum().item()\n","      accuracy=accuracy+correct\n","\n","  accuracy=(accuracy/len(test_dataset))*100\n","\n","  return accuracy\n"],"metadata":{"id":"1sXQkENGv9Qh","executionInfo":{"status":"ok","timestamp":1752007325616,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"CIE0Rf3krS4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creating a study to use the objective\n","\n","import optuna\n","\n","study=optuna.create_study(direction='maximize')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVHqmrZbrJKt","executionInfo":{"status":"ok","timestamp":1752007330968,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"7af5937e-9e12-46fe-b085-729c63cdbe93"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-07-08 20:42:18,140] A new study created in memory with name: no-name-b60a013c-6ff8-41f0-9b69-4d1a1d65b871\n"]}]},{"cell_type":"code","source":["study.optimize(objective, n_trials=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0BDksnnrtwY","executionInfo":{"status":"ok","timestamp":1752007843899,"user_tz":-330,"elapsed":508262,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"113338ec-48e3-480f-bd8e-6485483d0807"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-07-08 20:44:11,971] Trial 0 finished with value: 89.8 and parameters: {'num_hidden_layers': 4, 'num_per_layers': 120}. Best is trial 0 with value: 89.8.\n","[I 2025-07-08 20:45:54,697] Trial 1 finished with value: 77.60000000000001 and parameters: {'num_hidden_layers': 4, 'num_per_layers': 40}. Best is trial 0 with value: 89.8.\n","[I 2025-07-08 20:47:32,461] Trial 2 finished with value: 93.4 and parameters: {'num_hidden_layers': 1, 'num_per_layers': 88}. Best is trial 2 with value: 93.4.\n","[I 2025-07-08 20:49:06,924] Trial 3 finished with value: 90.2 and parameters: {'num_hidden_layers': 1, 'num_per_layers': 40}. Best is trial 2 with value: 93.4.\n","[I 2025-07-08 20:50:51,020] Trial 4 finished with value: 89.60000000000001 and parameters: {'num_hidden_layers': 3, 'num_per_layers': 88}. Best is trial 2 with value: 93.4.\n"]}]},{"cell_type":"code","source":["study.best_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voBwE_rB0Yj0","executionInfo":{"status":"ok","timestamp":1752008080284,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"de765d74-7ca2-4701-995e-eb208bc9f428"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["93.4"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["study.best_params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWgcUTgq0cNI","executionInfo":{"status":"ok","timestamp":1752008089169,"user_tz":-330,"elapsed":36,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"01e77b8e-b151-4b98-8025-dfcce3a1c37f"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'num_hidden_layers': 1, 'num_per_layers': 88}"]},"metadata":{},"execution_count":52}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}