{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"vscode":{"languageId":"plaintext"},"id":"MP_aRgC1Xsl3","executionInfo":{"status":"ok","timestamp":1753264396443,"user_tz":-330,"elapsed":6964,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","source":["# XOR dataset\n","X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n","y = torch.tensor([[0],[1],[1],[0]], dtype=torch.float32)\n","\n","class XORNetwork:\n","    def __init__(self, input_dim=2, hidden_dim=2, output_dim=1):\n","        self.w1 = torch.randn(input_dim, hidden_dim, requires_grad=True, dtype=torch.float32) * 0.1\n","        self.b1 = torch.zeros(hidden_dim, requires_grad=True, dtype=torch.float32)\n","        self.w2 = torch.randn(hidden_dim, output_dim, requires_grad=True, dtype=torch.float32) * 0.1\n","        self.b2 = torch.zeros(output_dim, requires_grad=True, dtype=torch.float32)\n","\n","    def forward(self, X):\n","        self.z1 = torch.matmul(X, self.w1) + self.b1\n","        self.a1 = torch.sigmoid(self.z1)\n","        self.z2 = torch.matmul(self.a1, self.w2) + self.b2\n","        return torch.sigmoid(self.z2)\n","\n","    def loss(self, y_pred, y):\n","        return torch.mean((y_pred - y) ** 2)"],"metadata":{"id":"bmL6Leo2qK_R","executionInfo":{"status":"ok","timestamp":1753265929202,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["epochs = 10000\n","lr = 1.0\n","model = XORNetwork()\n","\n","for epoch in range(epochs):\n","    y_pred = model.forward(X)\n","    loss = model.loss(y_pred, y)\n","    loss.backward()\n","\n","    with torch.no_grad():\n","        model.w1 -= lr * model.w1.grad\n","        model.b1 -= lr * model.b1.grad\n","        model.w2 -= lr * model.w2.grad\n","        model.b2 -= lr * model.b2.grad\n","\n","    model.w1.grad.zero_()\n","    model.b1.grad.zero_()\n","    model.w2.grad.zero_()\n","    model.b2.grad.zero_()\n","\n","    if (epoch+1) % 1000 == 0:\n","        print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}')\n","\n","print(\"\\nPredictions after training:\")\n","print(model.forward(X))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"MW9PbbkVsnz5","executionInfo":{"status":"error","timestamp":1753265931855,"user_tz":-330,"elapsed":59,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"c238e551-f52c-43b5-f667-85a5fa5a91be"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-41-1598292094.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n","  model.w1 -= lr * model.w1.grad\n"]},{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for *: 'float' and 'NoneType'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-41-1598292094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"]}]},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"fG9MzDhOYKwo","executionInfo":{"status":"ok","timestamp":1749992934738,"user_tz":-480,"elapsed":303,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"59197d5b-2ee9-42d1-f452-6caba8d568c5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0    842302         M        17.99         10.38          122.80     1001.0   \n","1    842517         M        20.57         17.77          132.90     1326.0   \n","2  84300903         M        19.69         21.25          130.00     1203.0   \n","3  84348301         M        11.42         20.38           77.58      386.1   \n","4  84358402         M        20.29         14.34          135.10     1297.0   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0          0.11840           0.27760          0.3001              0.14710   \n","1          0.08474           0.07864          0.0869              0.07017   \n","2          0.10960           0.15990          0.1974              0.12790   \n","3          0.14250           0.28390          0.2414              0.10520   \n","4          0.10030           0.13280          0.1980              0.10430   \n","\n","   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n","0  ...          17.33           184.60      2019.0            0.1622   \n","1  ...          23.41           158.80      1956.0            0.1238   \n","2  ...          25.53           152.50      1709.0            0.1444   \n","3  ...          26.50            98.87       567.7            0.2098   \n","4  ...          16.67           152.20      1575.0            0.1374   \n","\n","   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n","0             0.6656           0.7119                0.2654          0.4601   \n","1             0.1866           0.2416                0.1860          0.2750   \n","2             0.4245           0.4504                0.2430          0.3613   \n","3             0.8663           0.6869                0.2575          0.6638   \n","4             0.2050           0.4000                0.1625          0.2364   \n","\n","   fractal_dimension_worst  Unnamed: 32  \n","0                  0.11890          NaN  \n","1                  0.08902          NaN  \n","2                  0.08758          NaN  \n","3                  0.17300          NaN  \n","4                  0.07678          NaN  \n","\n","[5 rows x 33 columns]"],"text/html":["\n","  <div id=\"df-cb6b2512-5c19-43bf-bcad-928a50a5fdbf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>...</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","      <th>Unnamed: 32</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>842302</td>\n","      <td>M</td>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>...</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>842517</td>\n","      <td>M</td>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>...</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>84300903</td>\n","      <td>M</td>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>...</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>84348301</td>\n","      <td>M</td>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>...</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>84358402</td>\n","      <td>M</td>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>...</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 33 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb6b2512-5c19-43bf-bcad-928a50a5fdbf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cb6b2512-5c19-43bf-bcad-928a50a5fdbf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cb6b2512-5c19-43bf-bcad-928a50a5fdbf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-0467d767-5fe1-4838-a915-ddc6998168f5\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0467d767-5fe1-4838-a915-ddc6998168f5')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-0467d767-5fe1-4838-a915-ddc6998168f5 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["df. shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLr3Zx43YZGn","executionInfo":{"status":"ok","timestamp":1749992934762,"user_tz":-480,"elapsed":20,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"55681704-9982-4294-d92d-a581b39edbdd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(569, 33)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df.drop(columns=['id', 'Unnamed: 32'], inplace= True)"],"metadata":{"id":"ucQFXmQYYbtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"jPgggWC3Yfkn","executionInfo":{"status":"ok","timestamp":1749992934906,"user_tz":-480,"elapsed":126,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"f58929bb-759a-4a8f-c4d0-c33759b9141a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0         M        17.99         10.38          122.80     1001.0   \n","1         M        20.57         17.77          132.90     1326.0   \n","2         M        19.69         21.25          130.00     1203.0   \n","3         M        11.42         20.38           77.58      386.1   \n","4         M        20.29         14.34          135.10     1297.0   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0          0.11840           0.27760          0.3001              0.14710   \n","1          0.08474           0.07864          0.0869              0.07017   \n","2          0.10960           0.15990          0.1974              0.12790   \n","3          0.14250           0.28390          0.2414              0.10520   \n","4          0.10030           0.13280          0.1980              0.10430   \n","\n","   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","0         0.2419  ...         25.38          17.33           184.60   \n","1         0.1812  ...         24.99          23.41           158.80   \n","2         0.2069  ...         23.57          25.53           152.50   \n","3         0.2597  ...         14.91          26.50            98.87   \n","4         0.1809  ...         22.54          16.67           152.20   \n","\n","   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","0      2019.0            0.1622             0.6656           0.7119   \n","1      1956.0            0.1238             0.1866           0.2416   \n","2      1709.0            0.1444             0.4245           0.4504   \n","3       567.7            0.2098             0.8663           0.6869   \n","4      1575.0            0.1374             0.2050           0.4000   \n","\n","   concave points_worst  symmetry_worst  fractal_dimension_worst  \n","0                0.2654          0.4601                  0.11890  \n","1                0.1860          0.2750                  0.08902  \n","2                0.2430          0.3613                  0.08758  \n","3                0.2575          0.6638                  0.17300  \n","4                0.1625          0.2364                  0.07678  \n","\n","[5 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-279a16c0-1334-4c40-a684-0a33cb05960f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>M</td>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>...</td>\n","      <td>25.38</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>M</td>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>...</td>\n","      <td>24.99</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>...</td>\n","      <td>23.57</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>...</td>\n","      <td>14.91</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>M</td>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>...</td>\n","      <td>22.54</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-279a16c0-1334-4c40-a684-0a33cb05960f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-279a16c0-1334-4c40-a684-0a33cb05960f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-279a16c0-1334-4c40-a684-0a33cb05960f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-559e2dca-1e13-45b7-94de-8cb9bed2fcc0\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-559e2dca-1e13-45b7-94de-8cb9bed2fcc0')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-559e2dca-1e13-45b7-94de-8cb9bed2fcc0 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)"],"metadata":{"id":"LG8paa0TYhdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"wLifgqMZYx0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzc_1R99Yzuf","executionInfo":{"status":"ok","timestamp":1749992934984,"user_tz":-480,"elapsed":24,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"f168129f-76d9-4cff-a2e6-e1a5373feed6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.90810768,  1.13448886, -0.91876617, ..., -1.1846073 ,\n","         0.27764974, -0.42149146],\n","       [-1.45002726, -1.07646735, -1.33040643, ..., -0.95613438,\n","        -1.01658972,  1.32724805],\n","       [-1.08201562, -0.42034241, -1.07527098, ..., -1.13852242,\n","        -0.02943068, -0.32192104],\n","       ...,\n","       [-0.78468915,  0.12257787, -0.8205431 , ..., -1.24945639,\n","        -0.5294412 , -0.42856477],\n","       [-1.49546773, -0.80154176, -1.45063799, ..., -0.09838516,\n","         0.06381888,  0.96270061],\n","       [ 3.27409777, -0.42496301,  3.3732967 , ...,  2.46255318,\n","         1.29214055,  0.22544422]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"XyPp9EIlY2z_","executionInfo":{"status":"ok","timestamp":1749992934999,"user_tz":-480,"elapsed":13,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"def7ddc6-02e3-43fc-ea6f-38d94dbfd328"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["549    B\n","71     B\n","217    B\n","360    B\n","253    M\n","      ..\n","186    M\n","433    M\n","522    B\n","114    B\n","352    M\n","Name: diagnosis, Length: 455, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>549</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>360</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>253</th>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>186</th>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>433</th>\n","      <td>M</td>\n","    </tr>\n","    <tr>\n","      <th>522</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>352</th>\n","      <td>M</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>455 rows × 1 columns</p>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["encoder = LabelEncoder()\n","y_train = encoder.fit_transform(y_train)\n","y_test = encoder.transform(y_test)"],"metadata":{"id":"dYkECp3aY7Mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPuvnLUAY-0v","executionInfo":{"status":"ok","timestamp":1749992935033,"user_tz":-480,"elapsed":17,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"c61017d6-a06c-4924-df35-1df1dc2f26b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n","       1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n","       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n","       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n","       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n","       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n","       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n","       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n","       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n","       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n","       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n","       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Converting Numpy Arrays to Pytorch Tensors"],"metadata":{"id":"4LJ8uTSLZDQY"}},{"cell_type":"code","source":["X_train_tensor = torch.from_numpy(X_train)\n","X_test_tensor = torch.from_numpy(X_test)\n","y_train_tensor = torch.from_numpy(y_train)\n","y_test_tensor = torch.from_numpy(y_test)"],"metadata":{"id":"2cAtVPVkZBH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_tensor.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qd5WIQvZIvP","executionInfo":{"status":"ok","timestamp":1749992935090,"user_tz":-480,"elapsed":27,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"095175ab-c608-4c0c-df2a-c722d874f1c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["455"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["class Simple_NeuralNetwork():\n","  def __init__(self,X):\n","    self.weights=torch.rand(X.shape[1],1,dtype=torch.float64,requires_grad=True)\n","    self.bias=torch.rand(1,dtype=torch.float64,requires_grad=True)\n","\n","  def forward(self,X):\n","    #z=wx+b  equation of linear regression, or for fitting\n","\n","    z = torch.matmul(X, self.weights) + self.bias\n","    #Applying Sigmoid Function: In neural networks, the sigmoid function is a\n","    #mathematical function that squashes values between 0 and 1, making it\n","    #useful as an activation function, particularly in binary classification.\n","    #It transforms any real-valued number into a probability between 0 and 1.\n","\n","    y_pred=torch.sigmoid(z)\n","    return y_pred\n","\n","  def loss(self,y_pred,y):\n","    # Clamp predictions to avoid log(0)\n","    epsilon = 1e-7\n","    y_pred=torch.clamp(y_pred,epsilon,1-epsilon)\n","    #Calculating Loss\n","    # The BCE loss : (y * log(p) + (1 - y) * log(1 - p)) evalues this for all samples then does a mean\n","    #Binary Cross Entropy (BCE) loss is a loss function used in\n","    #binary classification tasks to quantify the difference between\n","    #predicted probabilities and actual binary labels. It's also known as log loss.\n","    loss=-(y*torch.log(y_pred)+(1-y)*torch.log(1-y_pred)).mean()\n","    return loss\n","\n","  def acuuracy_score():\n","    temp=0\n","    for i in range(len(y_pred)):\n","      if(y_pred[i]>0.5):\n","        y_pred[i]=1\n","      else:\n","        y_pred[i]=0\n","\n","      if(y_pred[i]==y_test_tensor[i]):\n","        temp=temp+1\n","\n","\n","\n"],"metadata":{"id":"DmYsRqgeZPTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parameter Settings"],"metadata":{"id":"3PFExnhBiD-B"}},{"cell_type":"code","source":["lr=0.001\n","epochs=1000\n","# create model\n","model = Simple_NeuralNetwork(X_train_tensor)\n","\n","# define loop\n","for epoch in range(epochs):\n","\n","  # forward pass\n","  y_pred = model.forward(X_train_tensor)\n","\n","  # loss calculate\n","  loss = model.loss(y_pred, y_train_tensor)\n","\n","  # backward pass\n","  loss.backward()\n","\n","  # parameters update\n","  #model.weights -= learning_rate * model.weights.grad\n","  #✔ In-place update — modifies weights directly, but can interfere with autograd if misused.\n","\n","  #model.weights = model.weights - (learning_rate * model.weights.grad)\n","  #✘ Out-of-place — reassigns a new tensor, which may break autograd tracking unless properly wrapped as a torch.nn.Parameter.\n","\n","  with torch.no_grad():\n","    model.weights -= lr * model.weights.grad\n","    model.bias -= lr * model.bias.grad\n","\n","  # zero gradients\n","  model.weights.grad.zero_()\n","  model.bias.grad.zero_()\n","\n","  # print loss in each epoch\n","  print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-OwCyaqiDsJ","executionInfo":{"status":"ok","timestamp":1749992937851,"user_tz":-480,"elapsed":2768,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"d1f83eef-ab61-446c-9733-89dd095a2549"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 3.4112290393703484\n","Epoch: 2, Loss: 3.410112650885825\n","Epoch: 3, Loss: 3.408996406355902\n","Epoch: 4, Loss: 3.4078803059258544\n","Epoch: 5, Loss: 3.406764349748392\n","Epoch: 6, Loss: 3.405648537966154\n","Epoch: 7, Loss: 3.4045328707302227\n","Epoch: 8, Loss: 3.4034173481858327\n","Epoch: 9, Loss: 3.402301970485193\n","Epoch: 10, Loss: 3.401186737775178\n","Epoch: 11, Loss: 3.40007165020341\n","Epoch: 12, Loss: 3.3989567079148184\n","Epoch: 13, Loss: 3.3978419110643845\n","Epoch: 14, Loss: 3.3967272598001443\n","Epoch: 15, Loss: 3.395608062034751\n","Epoch: 16, Loss: 3.394471576522996\n","Epoch: 17, Loss: 3.3933210448594187\n","Epoch: 18, Loss: 3.392170664096201\n","Epoch: 19, Loss: 3.3910204343943615\n","Epoch: 20, Loss: 3.389870355914104\n","Epoch: 21, Loss: 3.3887204288159167\n","Epoch: 22, Loss: 3.38757065325389\n","Epoch: 23, Loss: 3.386421029383511\n","Epoch: 24, Loss: 3.3852715573723255\n","Epoch: 25, Loss: 3.3841222373657573\n","Epoch: 26, Loss: 3.3829730695415683\n","Epoch: 27, Loss: 3.381824054043012\n","Epoch: 28, Loss: 3.380675191036535\n","Epoch: 29, Loss: 3.37952648068457\n","Epoch: 30, Loss: 3.378377923139159\n","Epoch: 31, Loss: 3.3772295185700076\n","Epoch: 32, Loss: 3.376081267126687\n","Epoch: 33, Loss: 3.374933168973037\n","Epoch: 34, Loss: 3.37378522427453\n","Epoch: 35, Loss: 3.372637433190527\n","Epoch: 36, Loss: 3.3714897958739996\n","Epoch: 37, Loss: 3.370342312502931\n","Epoch: 38, Loss: 3.369194983220974\n","Epoch: 39, Loss: 3.368047808195232\n","Epoch: 40, Loss: 3.3669007875925523\n","Epoch: 41, Loss: 3.3657539215702226\n","Epoch: 42, Loss: 3.364607210294\n","Epoch: 43, Loss: 3.3634606539192577\n","Epoch: 44, Loss: 3.362314252617757\n","Epoch: 45, Loss: 3.361168006546421\n","Epoch: 46, Loss: 3.3600219158645217\n","Epoch: 47, Loss: 3.3588759807433197\n","Epoch: 48, Loss: 3.3577302013401864\n","Epoch: 49, Loss: 3.356584577821618\n","Epoch: 50, Loss: 3.355439110353364\n","Epoch: 51, Loss: 3.354293799093201\n","Epoch: 52, Loss: 3.3531486442120664\n","Epoch: 53, Loss: 3.352003645862089\n","Epoch: 54, Loss: 3.350858804221772\n","Epoch: 55, Loss: 3.34971411944979\n","Epoch: 56, Loss: 3.348569591706686\n","Epoch: 57, Loss: 3.3474252211658024\n","Epoch: 58, Loss: 3.346281007986085\n","Epoch: 59, Loss: 3.3451369523292875\n","Epoch: 60, Loss: 3.3439930543700527\n","Epoch: 61, Loss: 3.3428493142709557\n","Epoch: 62, Loss: 3.3417057321955763\n","Epoch: 63, Loss: 3.3405623083083626\n","Epoch: 64, Loss: 3.339419042782928\n","Epoch: 65, Loss: 3.338275935777468\n","Epoch: 66, Loss: 3.3371257268551866\n","Epoch: 67, Loss: 3.3359602799459465\n","Epoch: 68, Loss: 3.3347949965759827\n","Epoch: 69, Loss: 3.3336298769110986\n","Epoch: 70, Loss: 3.3324649211282065\n","Epoch: 71, Loss: 3.331300129408022\n","Epoch: 72, Loss: 3.3301355019146297\n","Epoch: 73, Loss: 3.3289710388335947\n","Epoch: 74, Loss: 3.3278067403291156\n","Epoch: 75, Loss: 3.326642606584193\n","Epoch: 76, Loss: 3.325478637771661\n","Epoch: 77, Loss: 3.324314834069003\n","Epoch: 78, Loss: 3.3231511956443294\n","Epoch: 79, Loss: 3.321987722679709\n","Epoch: 80, Loss: 3.3208244153479725\n","Epoch: 81, Loss: 3.3196612738250524\n","Epoch: 82, Loss: 3.318498298292441\n","Epoch: 83, Loss: 3.317335488920304\n","Epoch: 84, Loss: 3.31617284588606\n","Epoch: 85, Loss: 3.3150103693702087\n","Epoch: 86, Loss: 3.3138480595487922\n","Epoch: 87, Loss: 3.312685916597533\n","Epoch: 88, Loss: 3.311523940693561\n","Epoch: 89, Loss: 3.310362132016696\n","Epoch: 90, Loss: 3.3091915142810775\n","Epoch: 91, Loss: 3.3080058050501124\n","Epoch: 92, Loss: 3.306820269069941\n","Epoch: 93, Loss: 3.3056349065260866\n","Epoch: 94, Loss: 3.3044497176089784\n","Epoch: 95, Loss: 3.3032647024958868\n","Epoch: 96, Loss: 3.3020798613772446\n","Epoch: 97, Loss: 3.3008951944497182\n","Epoch: 98, Loss: 3.2997107018842673\n","Epoch: 99, Loss: 3.2985263838802794\n","Epoch: 100, Loss: 3.2973422406252872\n","Epoch: 101, Loss: 3.2961582723004685\n","Epoch: 102, Loss: 3.2949744790978053\n","Epoch: 103, Loss: 3.293790861204881\n","Epoch: 104, Loss: 3.2926074188083416\n","Epoch: 105, Loss: 3.291415766844368\n","Epoch: 106, Loss: 3.290210253752428\n","Epoch: 107, Loss: 3.2889958502962036\n","Epoch: 108, Loss: 3.287771668004521\n","Epoch: 109, Loss: 3.286547669090129\n","Epoch: 110, Loss: 3.285323853760421\n","Epoch: 111, Loss: 3.2841002222073605\n","Epoch: 112, Loss: 3.282876774645294\n","Epoch: 113, Loss: 3.281653511274206\n","Epoch: 114, Loss: 3.280430432285994\n","Epoch: 115, Loss: 3.2792075378901018\n","Epoch: 116, Loss: 3.277984828286925\n","Epoch: 117, Loss: 3.2767623036765845\n","Epoch: 118, Loss: 3.2755399642697287\n","Epoch: 119, Loss: 3.274317810262738\n","Epoch: 120, Loss: 3.273095841864075\n","Epoch: 121, Loss: 3.271874059272545\n","Epoch: 122, Loss: 3.27065246269029\n","Epoch: 123, Loss: 3.269431052328887\n","Epoch: 124, Loss: 3.268209828382325\n","Epoch: 125, Loss: 3.2669887910633784\n","Epoch: 126, Loss: 3.265767940574089\n","Epoch: 127, Loss: 3.2645472771153794\n","Epoch: 128, Loss: 3.2633268008959266\n","Epoch: 129, Loss: 3.2621065121253765\n","Epoch: 130, Loss: 3.2608864109986997\n","Epoch: 131, Loss: 3.259666497725696\n","Epoch: 132, Loss: 3.258446772515034\n","Epoch: 133, Loss: 3.257227235571988\n","Epoch: 134, Loss: 3.2560078871019438\n","Epoch: 135, Loss: 3.254788727304969\n","Epoch: 136, Loss: 3.2535697563984414\n","Epoch: 137, Loss: 3.252350974581152\n","Epoch: 138, Loss: 3.251132382062999\n","Epoch: 139, Loss: 3.249913979053576\n","Epoch: 140, Loss: 3.2486957657593596\n","Epoch: 141, Loss: 3.247477742377633\n","Epoch: 142, Loss: 3.2462599091317723\n","Epoch: 143, Loss: 3.2450422662225322\n","Epoch: 144, Loss: 3.2438248138522483\n","Epoch: 145, Loss: 3.242607552243298\n","Epoch: 146, Loss: 3.2413904815923256\n","Epoch: 147, Loss: 3.2401736021115326\n","Epoch: 148, Loss: 3.238956914014751\n","Epoch: 149, Loss: 3.2377404175037654\n","Epoch: 150, Loss: 3.236524112792003\n","Epoch: 151, Loss: 3.2353080000882017\n","Epoch: 152, Loss: 3.23409207960332\n","Epoch: 153, Loss: 3.232876351542123\n","Epoch: 154, Loss: 3.231660816121266\n","Epoch: 155, Loss: 3.2304454735496084\n","Epoch: 156, Loss: 3.229230324034566\n","Epoch: 157, Loss: 3.228015367792461\n","Epoch: 158, Loss: 3.2268006050318534\n","Epoch: 159, Loss: 3.2255860359582944\n","Epoch: 160, Loss: 3.224371660789073\n","Epoch: 161, Loss: 3.2231574797374964\n","Epoch: 162, Loss: 3.221943493007318\n","Epoch: 163, Loss: 3.2207297008212326\n","Epoch: 164, Loss: 3.219516103383124\n","Epoch: 165, Loss: 3.218302334512292\n","Epoch: 166, Loss: 3.2170721097484805\n","Epoch: 167, Loss: 3.2158420819439932\n","Epoch: 168, Loss: 3.214612251320831\n","Epoch: 169, Loss: 3.2133826180936778\n","Epoch: 170, Loss: 3.2121531824738736\n","Epoch: 171, Loss: 3.210923944690843\n","Epoch: 172, Loss: 3.209694904949911\n","Epoch: 173, Loss: 3.2084660634733857\n","Epoch: 174, Loss: 3.2072374204786103\n","Epoch: 175, Loss: 3.2060089761863524\n","Epoch: 176, Loss: 3.2047807308106036\n","Epoch: 177, Loss: 3.2035526845692512\n","Epoch: 178, Loss: 3.2023248376860822\n","Epoch: 179, Loss: 3.2010971903738343\n","Epoch: 180, Loss: 3.1998697428542053\n","Epoch: 181, Loss: 3.198642495350464\n","Epoch: 182, Loss: 3.197415448075581\n","Epoch: 183, Loss: 3.196188601247007\n","Epoch: 184, Loss: 3.194961955090104\n","Epoch: 185, Loss: 3.1937355098255105\n","Epoch: 186, Loss: 3.192509265670993\n","Epoch: 187, Loss: 3.191283222848122\n","Epoch: 188, Loss: 3.190057381571154\n","Epoch: 189, Loss: 3.1888317420692145\n","Epoch: 190, Loss: 3.187606304559942\n","Epoch: 191, Loss: 3.186381069257753\n","Epoch: 192, Loss: 3.1851560363964593\n","Epoch: 193, Loss: 3.1839312061867364\n","Epoch: 194, Loss: 3.182706578860961\n","Epoch: 195, Loss: 3.181482154630193\n","Epoch: 196, Loss: 3.180257933720495\n","Epoch: 197, Loss: 3.1790339163526116\n","Epoch: 198, Loss: 3.177810102755761\n","Epoch: 199, Loss: 3.1765864931420067\n","Epoch: 200, Loss: 3.1753630877411627\n","Epoch: 201, Loss: 3.1741398867758917\n","Epoch: 202, Loss: 3.172916890466291\n","Epoch: 203, Loss: 3.1716940990361344\n","Epoch: 204, Loss: 3.170471512710261\n","Epoch: 205, Loss: 3.1692491317155906\n","Epoch: 206, Loss: 3.1680269562693684\n","Epoch: 207, Loss: 3.16680498659958\n","Epoch: 208, Loss: 3.1655832229290697\n","Epoch: 209, Loss: 3.1643616654835163\n","Epoch: 210, Loss: 3.1631403144885355\n","Epoch: 211, Loss: 3.1619191701648606\n","Epoch: 212, Loss: 3.160698232740892\n","Epoch: 213, Loss: 3.1594775024388984\n","Epoch: 214, Loss: 3.1582569794885043\n","Epoch: 215, Loss: 3.1570366641110303\n","Epoch: 216, Loss: 3.155816556534335\n","Epoch: 217, Loss: 3.1545966569833226\n","Epoch: 218, Loss: 3.153376965684147\n","Epoch: 219, Loss: 3.1521574828643772\n","Epoch: 220, Loss: 3.1509382087498508\n","Epoch: 221, Loss: 3.1497191435681846\n","Epoch: 222, Loss: 3.1485002875436114\n","Epoch: 223, Loss: 3.147281640900473\n","Epoch: 224, Loss: 3.1460632038752707\n","Epoch: 225, Loss: 3.144844976686612\n","Epoch: 226, Loss: 3.1436269595689716\n","Epoch: 227, Loss: 3.1424091527456275\n","Epoch: 228, Loss: 3.141191556442021\n","Epoch: 229, Loss: 3.139974170890968\n","Epoch: 230, Loss: 3.138756996318069\n","Epoch: 231, Loss: 3.137540032955145\n","Epoch: 232, Loss: 3.1363232810277726\n","Epoch: 233, Loss: 3.135106740763353\n","Epoch: 234, Loss: 3.1338904123924856\n","Epoch: 235, Loss: 3.13267429614618\n","Epoch: 236, Loss: 3.1314583922517905\n","Epoch: 237, Loss: 3.1302427009371807\n","Epoch: 238, Loss: 3.129027222434766\n","Epoch: 239, Loss: 3.1278119569750396\n","Epoch: 240, Loss: 3.126596904780658\n","Epoch: 241, Loss: 3.1253820660934015\n","Epoch: 242, Loss: 3.124163965261695\n","Epoch: 243, Loss: 3.1229266705718377\n","Epoch: 244, Loss: 3.1216895940115146\n","Epoch: 245, Loss: 3.1204527358264773\n","Epoch: 246, Loss: 3.1192160962455353\n","Epoch: 247, Loss: 3.117979675509405\n","Epoch: 248, Loss: 3.1167434738589375\n","Epoch: 249, Loss: 3.1155074915337337\n","Epoch: 250, Loss: 3.114271728772862\n","Epoch: 251, Loss: 3.1130361858145394\n","Epoch: 252, Loss: 3.111800862904638\n","Epoch: 253, Loss: 3.1105657602743384\n","Epoch: 254, Loss: 3.1093308781689\n","Epoch: 255, Loss: 3.10809621682927\n","Epoch: 256, Loss: 3.1068617764937003\n","Epoch: 257, Loss: 3.105627557402836\n","Epoch: 258, Loss: 3.104393559801729\n","Epoch: 259, Loss: 3.1031597839227554\n","Epoch: 260, Loss: 3.101926230014875\n","Epoch: 261, Loss: 3.100692898319675\n","Epoch: 262, Loss: 3.0994597890706848\n","Epoch: 263, Loss: 3.098226902516565\n","Epoch: 264, Loss: 3.096994238900427\n","Epoch: 265, Loss: 3.0957617984599097\n","Epoch: 266, Loss: 3.094529581439393\n","Epoch: 267, Loss: 3.093292106664956\n","Epoch: 268, Loss: 3.0920331056430497\n","Epoch: 269, Loss: 3.0907743369988383\n","Epoch: 270, Loss: 3.089515800997419\n","Epoch: 271, Loss: 3.0882574978819344\n","Epoch: 272, Loss: 3.086999427918104\n","Epoch: 273, Loss: 3.0857415913487167\n","Epoch: 274, Loss: 3.084483988442952\n","Epoch: 275, Loss: 3.0832266194443143\n","Epoch: 276, Loss: 3.081969484618203\n","Epoch: 277, Loss: 3.080712584214211\n","Epoch: 278, Loss: 3.0794559184898005\n","Epoch: 279, Loss: 3.0781994877054117\n","Epoch: 280, Loss: 3.0769432921155557\n","Epoch: 281, Loss: 3.0756873319735956\n","Epoch: 282, Loss: 3.074431607537605\n","Epoch: 283, Loss: 3.0731761190682536\n","Epoch: 284, Loss: 3.0719208668212508\n","Epoch: 285, Loss: 3.0706658510514964\n","Epoch: 286, Loss: 3.0694110720195633\n","Epoch: 287, Loss: 3.068156529981783\n","Epoch: 288, Loss: 3.0669022251935525\n","Epoch: 289, Loss: 3.065648157918344\n","Epoch: 290, Loss: 3.0643910520594764\n","Epoch: 291, Loss: 3.06311310486063\n","Epoch: 292, Loss: 3.0618353990643334\n","Epoch: 293, Loss: 3.0605579349342413\n","Epoch: 294, Loss: 3.0592807127337407\n","Epoch: 295, Loss: 3.0580037327370877\n","Epoch: 296, Loss: 3.0567269952033214\n","Epoch: 297, Loss: 3.055450500397913\n","Epoch: 298, Loss: 3.054174248590868\n","Epoch: 299, Loss: 3.0528982400524525\n","Epoch: 300, Loss: 3.0516224750479677\n","Epoch: 301, Loss: 3.050346953840002\n","Epoch: 302, Loss: 3.049071676696846\n","Epoch: 303, Loss: 3.0477966438906128\n","Epoch: 304, Loss: 3.046521855686971\n","Epoch: 305, Loss: 3.0452473123507446\n","Epoch: 306, Loss: 3.043973014150419\n","Epoch: 307, Loss: 3.042698961361342\n","Epoch: 308, Loss: 3.041425154245182\n","Epoch: 309, Loss: 3.0401515930685084\n","Epoch: 310, Loss: 3.0388782781070067\n","Epoch: 311, Loss: 3.037605209623647\n","Epoch: 312, Loss: 3.036332387895637\n","Epoch: 313, Loss: 3.035059813180231\n","Epoch: 314, Loss: 3.033787485752656\n","Epoch: 315, Loss: 3.032515405887929\n","Epoch: 316, Loss: 3.031243573850138\n","Epoch: 317, Loss: 3.029971989907135\n","Epoch: 318, Loss: 3.0287006543301893\n","Epoch: 319, Loss: 3.0274295673940568\n","Epoch: 320, Loss: 3.0261587293651364\n","Epoch: 321, Loss: 3.0248881405143973\n","Epoch: 322, Loss: 3.0236178011148485\n","Epoch: 323, Loss: 3.0223477114315807\n","Epoch: 324, Loss: 3.0210778717433358\n","Epoch: 325, Loss: 3.0198082823180754\n","Epoch: 326, Loss: 3.0185389434264542\n","Epoch: 327, Loss: 3.0172698553397423\n","Epoch: 328, Loss: 3.0160010183257002\n","Epoch: 329, Loss: 3.014732432665229\n","Epoch: 330, Loss: 3.013464098623663\n","Epoch: 331, Loss: 3.0121960164780326\n","Epoch: 332, Loss: 3.0109281864924755\n","Epoch: 333, Loss: 3.00966060895086\n","Epoch: 334, Loss: 3.0083932841167256\n","Epoch: 335, Loss: 3.007126212264374\n","Epoch: 336, Loss: 3.005859393669916\n","Epoch: 337, Loss: 3.004592828604812\n","Epoch: 338, Loss: 3.003326517341822\n","Epoch: 339, Loss: 3.0020604601553518\n","Epoch: 340, Loss: 3.0007946573135285\n","Epoch: 341, Loss: 2.99952910910403\n","Epoch: 342, Loss: 2.99826381578447\n","Epoch: 343, Loss: 2.996998777638928\n","Epoch: 344, Loss: 2.995733994937241\n","Epoch: 345, Loss: 2.994469467955111\n","Epoch: 346, Loss: 2.993205196965602\n","Epoch: 347, Loss: 2.9919411822479454\n","Epoch: 348, Loss: 2.9906774240700345\n","Epoch: 349, Loss: 2.9894139227130627\n","Epoch: 350, Loss: 2.988150678448308\n","Epoch: 351, Loss: 2.9868876915525133\n","Epoch: 352, Loss: 2.9856249622991213\n","Epoch: 353, Loss: 2.984362490969305\n","Epoch: 354, Loss: 2.983100277831739\n","Epoch: 355, Loss: 2.9818383231655345\n","Epoch: 356, Loss: 2.980576627247434\n","Epoch: 357, Loss: 2.9793151903493182\n","Epoch: 358, Loss: 2.9780540127532578\n","Epoch: 359, Loss: 2.9767930947329755\n","Epoch: 360, Loss: 2.975532436561946\n","Epoch: 361, Loss: 2.9742720385201578\n","Epoch: 362, Loss: 2.9730119008863616\n","Epoch: 363, Loss: 2.971752023933707\n","Epoch: 364, Loss: 2.970492407943411\n","Epoch: 365, Loss: 2.9692330531879025\n","Epoch: 366, Loss: 2.9679739599468133\n","Epoch: 367, Loss: 2.9667151284976927\n","Epoch: 368, Loss: 2.965456559119637\n","Epoch: 369, Loss: 2.9641870247388824\n","Epoch: 370, Loss: 2.9629001306250884\n","Epoch: 371, Loss: 2.961613507066807\n","Epoch: 372, Loss: 2.960327154349473\n","Epoch: 373, Loss: 2.959041072771555\n","Epoch: 374, Loss: 2.957755262617118\n","Epoch: 375, Loss: 2.956469724180263\n","Epoch: 376, Loss: 2.9551844577525537\n","Epoch: 377, Loss: 2.953899463621382\n","Epoch: 378, Loss: 2.952614742083962\n","Epoch: 379, Loss: 2.951330293429723\n","Epoch: 380, Loss: 2.9500461179513096\n","Epoch: 381, Loss: 2.9487622159369993\n","Epoch: 382, Loss: 2.947478587684112\n","Epoch: 383, Loss: 2.9461952334816592\n","Epoch: 384, Loss: 2.9449121536213765\n","Epoch: 385, Loss: 2.9436293483975846\n","Epoch: 386, Loss: 2.9423468181055097\n","Epoch: 387, Loss: 2.941064563030528\n","Epoch: 388, Loss: 2.9397825834740665\n","Epoch: 389, Loss: 2.9385008797228336\n","Epoch: 390, Loss: 2.937219452072013\n","Epoch: 391, Loss: 2.9359383008175395\n","Epoch: 392, Loss: 2.9346445236038425\n","Epoch: 393, Loss: 2.933336576392596\n","Epoch: 394, Loss: 2.932028913532644\n","Epoch: 395, Loss: 2.930721535335427\n","Epoch: 396, Loss: 2.9294144421018573\n","Epoch: 397, Loss: 2.928107634140342\n","Epoch: 398, Loss: 2.9268011117558896\n","Epoch: 399, Loss: 2.9254948752558523\n","Epoch: 400, Loss: 2.9241889249503887\n","Epoch: 401, Loss: 2.922883261140329\n","Epoch: 402, Loss: 2.9215778841351074\n","Epoch: 403, Loss: 2.920272794242434\n","Epoch: 404, Loss: 2.9189679917672744\n","Epoch: 405, Loss: 2.917663477023401\n","Epoch: 406, Loss: 2.916359250310183\n","Epoch: 407, Loss: 2.9150553119422282\n","Epoch: 408, Loss: 2.9137516622261703\n","Epoch: 409, Loss: 2.9124483014650857\n","Epoch: 410, Loss: 2.9111452299745775\n","Epoch: 411, Loss: 2.909842448055894\n","Epoch: 412, Loss: 2.9085399560220386\n","Epoch: 413, Loss: 2.907237754180413\n","Epoch: 414, Loss: 2.9059358428356186\n","Epoch: 415, Loss: 2.904634222307846\n","Epoch: 416, Loss: 2.90333289289686\n","Epoch: 417, Loss: 2.902031854910163\n","Epoch: 418, Loss: 2.9007311086638783\n","Epoch: 419, Loss: 2.899430654463023\n","Epoch: 420, Loss: 2.8981304926214655\n","Epoch: 421, Loss: 2.8968306234402648\n","Epoch: 422, Loss: 2.8955310472395057\n","Epoch: 423, Loss: 2.8942317643241924\n","Epoch: 424, Loss: 2.8929327750068516\n","Epoch: 425, Loss: 2.8916340795944375\n","Epoch: 426, Loss: 2.890335678398397\n","Epoch: 427, Loss: 2.889037571730346\n","Epoch: 428, Loss: 2.8877397599007146\n","Epoch: 429, Loss: 2.8864422432221555\n","Epoch: 430, Loss: 2.8851450220018475\n","Epoch: 431, Loss: 2.883848096552809\n","Epoch: 432, Loss: 2.8825514671845736\n","Epoch: 433, Loss: 2.8812551342144794\n","Epoch: 434, Loss: 2.8799590979455427\n","Epoch: 435, Loss: 2.8786633586962873\n","Epoch: 436, Loss: 2.877367916774939\n","Epoch: 437, Loss: 2.876072772493215\n","Epoch: 438, Loss: 2.8747779261651454\n","Epoch: 439, Loss: 2.8734833781037756\n","Epoch: 440, Loss: 2.872189128619293\n","Epoch: 441, Loss: 2.870895178022751\n","Epoch: 442, Loss: 2.8695984266059087\n","Epoch: 443, Loss: 2.868287420965139\n","Epoch: 444, Loss: 2.8669767164690176\n","Epoch: 445, Loss: 2.8656663134340246\n","Epoch: 446, Loss: 2.864356212178508\n","Epoch: 447, Loss: 2.8630464130287665\n","Epoch: 448, Loss: 2.86173691629852\n","Epoch: 449, Loss: 2.8604277223055297\n","Epoch: 450, Loss: 2.8591188313692824\n","Epoch: 451, Loss: 2.8578102438144244\n","Epoch: 452, Loss: 2.856501959953772\n","Epoch: 453, Loss: 2.8551939801132855\n","Epoch: 454, Loss: 2.8538863046064247\n","Epoch: 455, Loss: 2.8525789337600407\n","Epoch: 456, Loss: 2.8512718678861795\n","Epoch: 457, Loss: 2.849965107310997\n","Epoch: 458, Loss: 2.8486586523521145\n","Epoch: 459, Loss: 2.8473525033326106\n","Epoch: 460, Loss: 2.8460394914242513\n","Epoch: 461, Loss: 2.844701634433155\n","Epoch: 462, Loss: 2.8433640961474094\n","Epoch: 463, Loss: 2.842026876908258\n","Epoch: 464, Loss: 2.8406899770478606\n","Epoch: 465, Loss: 2.8393533969073776\n","Epoch: 466, Loss: 2.8380171368251847\n","Epoch: 467, Loss: 2.8366811971434696\n","Epoch: 468, Loss: 2.83534557820051\n","Epoch: 469, Loss: 2.8340102803322513\n","Epoch: 470, Loss: 2.8326685671893093\n","Epoch: 471, Loss: 2.8313104410554355\n","Epoch: 472, Loss: 2.829952642491501\n","Epoch: 473, Loss: 2.8285951718547615\n","Epoch: 474, Loss: 2.8272380294855703\n","Epoch: 475, Loss: 2.825881215739591\n","Epoch: 476, Loss: 2.824524730971941\n","Epoch: 477, Loss: 2.8231685755309774\n","Epoch: 478, Loss: 2.821812749763209\n","Epoch: 479, Loss: 2.8204572540230326\n","Epoch: 480, Loss: 2.8191020886635645\n","Epoch: 481, Loss: 2.817747254037521\n","Epoch: 482, Loss: 2.816392750492554\n","Epoch: 483, Loss: 2.815038578385521\n","Epoch: 484, Loss: 2.813684738064332\n","Epoch: 485, Loss: 2.8123312298810523\n","Epoch: 486, Loss: 2.8109780541896834\n","Epoch: 487, Loss: 2.8096252113441516\n","Epoch: 488, Loss: 2.8082727016981797\n","Epoch: 489, Loss: 2.806920525602597\n","Epoch: 490, Loss: 2.805568683404741\n","Epoch: 491, Loss: 2.804217175469889\n","Epoch: 492, Loss: 2.8028660021401484\n","Epoch: 493, Loss: 2.8015151637749467\n","Epoch: 494, Loss: 2.800164660724994\n","Epoch: 495, Loss: 2.79881449334925\n","Epoch: 496, Loss: 2.7974646619969166\n","Epoch: 497, Loss: 2.7961151670165774\n","Epoch: 498, Loss: 2.79476600877532\n","Epoch: 499, Loss: 2.793417187618839\n","Epoch: 500, Loss: 2.7920687039052017\n","Epoch: 501, Loss: 2.7907205579837084\n","Epoch: 502, Loss: 2.7893727502157133\n","Epoch: 503, Loss: 2.7880252809524224\n","Epoch: 504, Loss: 2.786678150550213\n","Epoch: 505, Loss: 2.7853313593617433\n","Epoch: 506, Loss: 2.7839849077440766\n","Epoch: 507, Loss: 2.7826387960537518\n","Epoch: 508, Loss: 2.781293024644809\n","Epoch: 509, Loss: 2.7799475938720715\n","Epoch: 510, Loss: 2.778602504093247\n","Epoch: 511, Loss: 2.777257755668783\n","Epoch: 512, Loss: 2.775913348944355\n","Epoch: 513, Loss: 2.7745692842825043\n","Epoch: 514, Loss: 2.773225562040924\n","Epoch: 515, Loss: 2.771882182571268\n","Epoch: 516, Loss: 2.7705391462353806\n","Epoch: 517, Loss: 2.7691964533886355\n","Epoch: 518, Loss: 2.7678541043838387\n","Epoch: 519, Loss: 2.766512099583389\n","Epoch: 520, Loss: 2.765170439343303\n","Epoch: 521, Loss: 2.7638291240213526\n","Epoch: 522, Loss: 2.762488153972652\n","Epoch: 523, Loss: 2.7611475295583787\n","Epoch: 524, Loss: 2.7598072511297453\n","Epoch: 525, Loss: 2.758467319051956\n","Epoch: 526, Loss: 2.75712773368458\n","Epoch: 527, Loss: 2.7557884953757226\n","Epoch: 528, Loss: 2.7544496044957687\n","Epoch: 529, Loss: 2.753111061392002\n","Epoch: 530, Loss: 2.751772866430819\n","Epoch: 531, Loss: 2.750435019968115\n","Epoch: 532, Loss: 2.7490975223664083\n","Epoch: 533, Loss: 2.747760373978604\n","Epoch: 534, Loss: 2.7464235751671744\n","Epoch: 535, Loss: 2.7450871262928342\n","Epoch: 536, Loss: 2.743751027714362\n","Epoch: 537, Loss: 2.7424152797918895\n","Epoch: 538, Loss: 2.741079882880268\n","Epoch: 539, Loss: 2.739744837347046\n","Epoch: 540, Loss: 2.7384101435454045\n","Epoch: 541, Loss: 2.737075801842626\n","Epoch: 542, Loss: 2.735741812592276\n","Epoch: 543, Loss: 2.73440817616046\n","Epoch: 544, Loss: 2.733074892905424\n","Epoch: 545, Loss: 2.7317419631852338\n","Epoch: 546, Loss: 2.730409387365108\n","Epoch: 547, Loss: 2.7290771658035013\n","Epoch: 548, Loss: 2.727745298862601\n","Epoch: 549, Loss: 2.7264137869057117\n","Epoch: 550, Loss: 2.7250826302889575\n","Epoch: 551, Loss: 2.723751829381011\n","Epoch: 552, Loss: 2.722421384538807\n","Epoch: 553, Loss: 2.7210912961243516\n","Epoch: 554, Loss: 2.7197615645027104\n","Epoch: 555, Loss: 2.7184321900316384\n","Epoch: 556, Loss: 2.717103173078426\n","Epoch: 557, Loss: 2.7157745140016467\n","Epoch: 558, Loss: 2.7144462131653992\n","Epoch: 559, Loss: 2.713118270932389\n","Epoch: 560, Loss: 2.7117906876678273\n","Epoch: 561, Loss: 2.710463463729883\n","Epoch: 562, Loss: 2.7091365994841903\n","Epoch: 563, Loss: 2.707810095298114\n","Epoch: 564, Loss: 2.706483951528585\n","Epoch: 565, Loss: 2.7051581685405304\n","Epoch: 566, Loss: 2.7038327467013707\n","Epoch: 567, Loss: 2.70250768637285\n","Epoch: 568, Loss: 2.70118298791983\n","Epoch: 569, Loss: 2.6998586517033853\n","Epoch: 570, Loss: 2.6985346780922477\n","Epoch: 571, Loss: 2.6972110674474914\n","Epoch: 572, Loss: 2.695887820134847\n","Epoch: 573, Loss: 2.694564936519918\n","Epoch: 574, Loss: 2.6932424169666147\n","Epoch: 575, Loss: 2.691920261838929\n","Epoch: 576, Loss: 2.690598471506122\n","Epoch: 577, Loss: 2.689277046331372\n","Epoch: 578, Loss: 2.6879559866771845\n","Epoch: 579, Loss: 2.686635292912939\n","Epoch: 580, Loss: 2.6853149654026325\n","Epoch: 581, Loss: 2.6839950045125\n","Epoch: 582, Loss: 2.6826754106090207\n","Epoch: 583, Loss: 2.68135618405725\n","Epoch: 584, Loss: 2.6800373252252796\n","Epoch: 585, Loss: 2.6787188344780697\n","Epoch: 586, Loss: 2.6774007121843786\n","Epoch: 587, Loss: 2.676082958709804\n","Epoch: 588, Loss: 2.674765574420134\n","Epoch: 589, Loss: 2.6734485596826056\n","Epoch: 590, Loss: 2.672131914866728\n","Epoch: 591, Loss: 2.670815640336251\n","Epoch: 592, Loss: 2.6694997364624116\n","Epoch: 593, Loss: 2.668184203612425\n","Epoch: 594, Loss: 2.666869042150591\n","Epoch: 595, Loss: 2.6655502676547984\n","Epoch: 596, Loss: 2.664213143873457\n","Epoch: 597, Loss: 2.6628763972329432\n","Epoch: 598, Loss: 2.661540028095375\n","Epoch: 599, Loss: 2.6602040368581177\n","Epoch: 600, Loss: 2.658868423883221\n","Epoch: 601, Loss: 2.657533189561567\n","Epoch: 602, Loss: 2.6561983342643294\n","Epoch: 603, Loss: 2.6548638583777886\n","Epoch: 604, Loss: 2.6535297622758107\n","Epoch: 605, Loss: 2.6521960463396566\n","Epoch: 606, Loss: 2.6508627109503973\n","Epoch: 607, Loss: 2.649514288927171\n","Epoch: 608, Loss: 2.6481479401577155\n","Epoch: 609, Loss: 2.6467819859533264\n","Epoch: 610, Loss: 2.6454164267076323\n","Epoch: 611, Loss: 2.6440512628223316\n","Epoch: 612, Loss: 2.6426864947039306\n","Epoch: 613, Loss: 2.641322122750602\n","Epoch: 614, Loss: 2.639958147355711\n","Epoch: 615, Loss: 2.6385945689327657\n","Epoch: 616, Loss: 2.637231387872596\n","Epoch: 617, Loss: 2.635868604582232\n","Epoch: 618, Loss: 2.634506219465856\n","Epoch: 619, Loss: 2.6331442329192787\n","Epoch: 620, Loss: 2.631782645347154\n","Epoch: 621, Loss: 2.6304214571519937\n","Epoch: 622, Loss: 2.6290606687362748\n","Epoch: 623, Loss: 2.6277002804990963\n","Epoch: 624, Loss: 2.6263402928511277\n","Epoch: 625, Loss: 2.6249807061919266\n","Epoch: 626, Loss: 2.623621520919522\n","Epoch: 627, Loss: 2.622262737441561\n","Epoch: 628, Loss: 2.6209043561626695\n","Epoch: 629, Loss: 2.6195463774880747\n","Epoch: 630, Loss: 2.61818880181626\n","Epoch: 631, Loss: 2.616831629549703\n","Epoch: 632, Loss: 2.615474861101894\n","Epoch: 633, Loss: 2.6141184968680315\n","Epoch: 634, Loss: 2.6127625372623653\n","Epoch: 635, Loss: 2.611406982680557\n","Epoch: 636, Loss: 2.610051833531483\n","Epoch: 637, Loss: 2.608697090220695\n","Epoch: 638, Loss: 2.607342753153329\n","Epoch: 639, Loss: 2.6059888227317303\n","Epoch: 640, Loss: 2.6046352993672985\n","Epoch: 641, Loss: 2.603282183464502\n","Epoch: 642, Loss: 2.6019294754245297\n","Epoch: 643, Loss: 2.6005771756568\n","Epoch: 644, Loss: 2.5992252845700214\n","Epoch: 645, Loss: 2.5978738025664083\n","Epoch: 646, Loss: 2.596522730061113\n","Epoch: 647, Loss: 2.5951720674507723\n","Epoch: 648, Loss: 2.5938218151485586\n","Epoch: 649, Loss: 2.592471973561107\n","Epoch: 650, Loss: 2.5911225430936478\n","Epoch: 651, Loss: 2.5897735241572017\n","Epoch: 652, Loss: 2.5884249171589455\n","Epoch: 653, Loss: 2.587076722506121\n","Epoch: 654, Loss: 2.5857289406076105\n","Epoch: 655, Loss: 2.584381571871554\n","Epoch: 656, Loss: 2.5830346167082117\n","Epoch: 657, Loss: 2.581688075525317\n","Epoch: 658, Loss: 2.5803419487315677\n","Epoch: 659, Loss: 2.5789962367352675\n","Epoch: 660, Loss: 2.577650939948059\n","Epoch: 661, Loss: 2.5763060587807067\n","Epoch: 662, Loss: 2.574961593640332\n","Epoch: 663, Loss: 2.573617544940663\n","Epoch: 664, Loss: 2.5722739130849064\n","Epoch: 665, Loss: 2.5709306984920266\n","Epoch: 666, Loss: 2.5695879015682737\n","Epoch: 667, Loss: 2.5682455227251277\n","Epoch: 668, Loss: 2.5669035623728975\n","Epoch: 669, Loss: 2.565562020924854\n","Epoch: 670, Loss: 2.5642208987893733\n","Epoch: 671, Loss: 2.562880196381824\n","Epoch: 672, Loss: 2.561539914113596\n","Epoch: 673, Loss: 2.5602000523968713\n","Epoch: 674, Loss: 2.558860611639018\n","Epoch: 675, Loss: 2.5575215922551546\n","Epoch: 676, Loss: 2.5561829946637684\n","Epoch: 677, Loss: 2.55484481927152\n","Epoch: 678, Loss: 2.5535070664905253\n","Epoch: 679, Loss: 2.552169736737716\n","Epoch: 680, Loss: 2.5508328304271166\n","Epoch: 681, Loss: 2.5494963479695643\n","Epoch: 682, Loss: 2.5481602897807227\n","Epoch: 683, Loss: 2.5468246562738512\n","Epoch: 684, Loss: 2.5454894478622125\n","Epoch: 685, Loss: 2.544154664962254\n","Epoch: 686, Loss: 2.542820307987829\n","Epoch: 687, Loss: 2.5414863773545813\n","Epoch: 688, Loss: 2.540152873477316\n","Epoch: 689, Loss: 2.5388197967701243\n","Epoch: 690, Loss: 2.5374871476508742\n","Epoch: 691, Loss: 2.5361549265344303\n","Epoch: 692, Loss: 2.534823133834975\n","Epoch: 693, Loss: 2.5334917699703454\n","Epoch: 694, Loss: 2.5321608353582192\n","Epoch: 695, Loss: 2.5308303304109026\n","Epoch: 696, Loss: 2.529500255549051\n","Epoch: 697, Loss: 2.528170611189094\n","Epoch: 698, Loss: 2.5268413977458604\n","Epoch: 699, Loss: 2.5255126156397996\n","Epoch: 700, Loss: 2.524184265286715\n","Epoch: 701, Loss: 2.5228563471050522\n","Epoch: 702, Loss: 2.5215288615135023\n","Epoch: 703, Loss: 2.5202018089292983\n","Epoch: 704, Loss: 2.518875189772026\n","Epoch: 705, Loss: 2.5175490044566464\n","Epoch: 706, Loss: 2.516223253407291\n","Epoch: 707, Loss: 2.5148979370395734\n","Epoch: 708, Loss: 2.5135730557752685\n","Epoch: 709, Loss: 2.512248610030554\n","Epoch: 710, Loss: 2.510924600229807\n","Epoch: 711, Loss: 2.5096010267891558\n","Epoch: 712, Loss: 2.508277890130563\n","Epoch: 713, Loss: 2.5069551906733323\n","Epoch: 714, Loss: 2.5056329288388794\n","Epoch: 715, Loss: 2.5043111050479734\n","Epoch: 716, Loss: 2.502989719720736\n","Epoch: 717, Loss: 2.501668773279544\n","Epoch: 718, Loss: 2.50034826614697\n","Epoch: 719, Loss: 2.4990281987404908\n","Epoch: 720, Loss: 2.4977085714871916\n","Epoch: 721, Loss: 2.496389384805339\n","Epoch: 722, Loss: 2.4950706391203874\n","Epoch: 723, Loss: 2.4937523348515986\n","Epoch: 724, Loss: 2.492434472425163\n","Epoch: 725, Loss: 2.4911170522614188\n","Epoch: 726, Loss: 2.4898000747853377\n","Epoch: 727, Loss: 2.4884835404191947\n","Epoch: 728, Loss: 2.4871674495869525\n","Epoch: 729, Loss: 2.485851802712511\n","Epoch: 730, Loss: 2.4845366002210794\n","Epoch: 731, Loss: 2.4832218425360755\n","Epoch: 732, Loss: 2.4819075300812177\n","Epoch: 733, Loss: 2.480593663282869\n","Epoch: 734, Loss: 2.479280242564761\n","Epoch: 735, Loss: 2.477967268352449\n","Epoch: 736, Loss: 2.4766547410729562\n","Epoch: 737, Loss: 2.47534266114976\n","Epoch: 738, Loss: 2.4740310290101455\n","Epoch: 739, Loss: 2.472719845079602\n","Epoch: 740, Loss: 2.471409109785269\n","Epoch: 741, Loss: 2.470098823552005\n","Epoch: 742, Loss: 2.4687889868088657\n","Epoch: 743, Loss: 2.467479599980732\n","Epoch: 744, Loss: 2.466170663496538\n","Epoch: 745, Loss: 2.4648621777837936\n","Epoch: 746, Loss: 2.463554143269568\n","Epoch: 747, Loss: 2.4622465603811503\n","Epoch: 748, Loss: 2.4609394295476847\n","Epoch: 749, Loss: 2.459632751197967\n","Epoch: 750, Loss: 2.458326525759068\n","Epoch: 751, Loss: 2.457020753661832\n","Epoch: 752, Loss: 2.455715435334153\n","Epoch: 753, Loss: 2.454410571204626\n","Epoch: 754, Loss: 2.4531061617046332\n","Epoch: 755, Loss: 2.4518022072632037\n","Epoch: 756, Loss: 2.4504987083098593\n","Epoch: 757, Loss: 2.449195665275786\n","Epoch: 758, Loss: 2.4478930785897566\n","Epoch: 759, Loss: 2.4465909486847925\n","Epoch: 760, Loss: 2.445289275990594\n","Epoch: 761, Loss: 2.443988060937811\n","Epoch: 762, Loss: 2.4426873039582633\n","Epoch: 763, Loss: 2.44138700548414\n","Epoch: 764, Loss: 2.4400871659472863\n","Epoch: 765, Loss: 2.438787785779013\n","Epoch: 766, Loss: 2.437488865411669\n","Epoch: 767, Loss: 2.4361904052790706\n","Epoch: 768, Loss: 2.434892405813252\n","Epoch: 769, Loss: 2.433594867445991\n","Epoch: 770, Loss: 2.4322977906123264\n","Epoch: 771, Loss: 2.431001175746513\n","Epoch: 772, Loss: 2.4297050232787276\n","Epoch: 773, Loss: 2.4284093336466404\n","Epoch: 774, Loss: 2.4271141072833946\n","Epoch: 775, Loss: 2.425819344622062\n","Epoch: 776, Loss: 2.4245250460981596\n","Epoch: 777, Loss: 2.4232312121472765\n","Epoch: 778, Loss: 2.4219378432037173\n","Epoch: 779, Loss: 2.4206449397035583\n","Epoch: 780, Loss: 2.4193525020816686\n","Epoch: 781, Loss: 2.418060530773948\n","Epoch: 782, Loss: 2.4167690262167643\n","Epoch: 783, Loss: 2.4154779888474365\n","Epoch: 784, Loss: 2.414187419101115\n","Epoch: 785, Loss: 2.4128973174150783\n","Epoch: 786, Loss: 2.411607684226495\n","Epoch: 787, Loss: 2.410318519971841\n","Epoch: 788, Loss: 2.4090298250894\n","Epoch: 789, Loss: 2.407741600016495\n","Epoch: 790, Loss: 2.4064538451915154\n","Epoch: 791, Loss: 2.40516656105306\n","Epoch: 792, Loss: 2.4038797480384315\n","Epoch: 793, Loss: 2.402593406587184\n","Epoch: 794, Loss: 2.401307537137278\n","Epoch: 795, Loss: 2.4000221401294617\n","Epoch: 796, Loss: 2.3987372160022082\n","Epoch: 797, Loss: 2.3974527651950788\n","Epoch: 798, Loss: 2.3961687881477287\n","Epoch: 799, Loss: 2.394885285300272\n","Epoch: 800, Loss: 2.3936022570938027\n","Epoch: 801, Loss: 2.392319703968822\n","Epoch: 802, Loss: 2.391037626365369\n","Epoch: 803, Loss: 2.389756024725493\n","Epoch: 804, Loss: 2.388474899489564\n","Epoch: 805, Loss: 2.3871942510997504\n","Epoch: 806, Loss: 2.3859140799966054\n","Epoch: 807, Loss: 2.3846343866241995\n","Epoch: 808, Loss: 2.3833551714231995\n","Epoch: 809, Loss: 2.3820764348363386\n","Epoch: 810, Loss: 2.380798177306904\n","Epoch: 811, Loss: 2.379520399277736\n","Epoch: 812, Loss: 2.378243101190956\n","Epoch: 813, Loss: 2.376966283491843\n","Epoch: 814, Loss: 2.3756899466228565\n","Epoch: 815, Loss: 2.3744140910273157\n","Epoch: 816, Loss: 2.373138717151039\n","Epoch: 817, Loss: 2.3718638254372992\n","Epoch: 818, Loss: 2.3705894163305423\n","Epoch: 819, Loss: 2.3693154902765867\n","Epoch: 820, Loss: 2.3680420477193853\n","Epoch: 821, Loss: 2.366769089104945\n","Epoch: 822, Loss: 2.3654966148785967\n","Epoch: 823, Loss: 2.364224625487166\n","Epoch: 824, Loss: 2.3629531213746446\n","Epoch: 825, Loss: 2.3616821029887034\n","Epoch: 826, Loss: 2.3604115707759923\n","Epoch: 827, Loss: 2.3591415251823773\n","Epoch: 828, Loss: 2.3578719666553503\n","Epoch: 829, Loss: 2.3566028956418967\n","Epoch: 830, Loss: 2.355334312589741\n","Epoch: 831, Loss: 2.354066217946315\n","Epoch: 832, Loss: 2.3527898416654667\n","Epoch: 833, Loss: 2.3514886746892465\n","Epoch: 834, Loss: 2.3501880120010608\n","Epoch: 835, Loss: 2.348887854070151\n","Epoch: 836, Loss: 2.34758820137238\n","Epoch: 837, Loss: 2.3462890543819745\n","Epoch: 838, Loss: 2.3449904135674067\n","Epoch: 839, Loss: 2.343692279406821\n","Epoch: 840, Loss: 2.3423946523724757\n","Epoch: 841, Loss: 2.341097532933799\n","Epoch: 842, Loss: 2.3398009215721003\n","Epoch: 843, Loss: 2.338504818756702\n","Epoch: 844, Loss: 2.3372092249641296\n","Epoch: 845, Loss: 2.3359141406716084\n","Epoch: 846, Loss: 2.3346195663502636\n","Epoch: 847, Loss: 2.333324738240966\n","Epoch: 848, Loss: 2.3320013177021397\n","Epoch: 849, Loss: 2.3306784202384945\n","Epoch: 850, Loss: 2.3293560463504766\n","Epoch: 851, Loss: 2.3280341965394253\n","Epoch: 852, Loss: 2.3267128712964773\n","Epoch: 853, Loss: 2.3253920711267613\n","Epoch: 854, Loss: 2.324071796524567\n","Epoch: 855, Loss: 2.3227520479883568\n","Epoch: 856, Loss: 2.321432826013436\n","Epoch: 857, Loss: 2.320114131109886\n","Epoch: 858, Loss: 2.3187959637683604\n","Epoch: 859, Loss: 2.317478324488991\n","Epoch: 860, Loss: 2.316149675223591\n","Epoch: 861, Loss: 2.3148034651109954\n","Epoch: 862, Loss: 2.3134577921646695\n","Epoch: 863, Loss: 2.3121126569018533\n","Epoch: 864, Loss: 2.3107680598400173\n","Epoch: 865, Loss: 2.30942400150183\n","Epoch: 866, Loss: 2.308080482413865\n","Epoch: 867, Loss: 2.3067375030907544\n","Epoch: 868, Loss: 2.305395064060096\n","Epoch: 869, Loss: 2.3040531658409713\n","Epoch: 870, Loss: 2.302711808955952\n","Epoch: 871, Loss: 2.3013709939265117\n","Epoch: 872, Loss: 2.300030721281809\n","Epoch: 873, Loss: 2.298690991541406\n","Epoch: 874, Loss: 2.2973518052240767\n","Epoch: 875, Loss: 2.2960131628641998\n","Epoch: 876, Loss: 2.2946750649753875\n","Epoch: 877, Loss: 2.293337512090752\n","Epoch: 878, Loss: 2.292000504732914\n","Epoch: 879, Loss: 2.290664043424391\n","Epoch: 880, Loss: 2.2893281286931337\n","Epoch: 881, Loss: 2.2879927610656052\n","Epoch: 882, Loss: 2.2866579410683863\n","Epoch: 883, Loss: 2.285323669225496\n","Epoch: 884, Loss: 2.2839899460622033\n","Epoch: 885, Loss: 2.282656772110709\n","Epoch: 886, Loss: 2.2813241478952557\n","Epoch: 887, Loss: 2.279992073944586\n","Epoch: 888, Loss: 2.278660550785044\n","Epoch: 889, Loss: 2.277329578948367\n","Epoch: 890, Loss: 2.275999158958143\n","Epoch: 891, Loss: 2.2746692913468975\n","Epoch: 892, Loss: 2.273339976645009\n","Epoch: 893, Loss: 2.272011215379191\n","Epoch: 894, Loss: 2.2706830080794744\n","Epoch: 895, Loss: 2.269355355275951\n","Epoch: 896, Loss: 2.2680282574984\n","Epoch: 897, Loss: 2.2667017152766578\n","Epoch: 898, Loss: 2.265375729145181\n","Epoch: 899, Loss: 2.2640502996331233\n","Epoch: 900, Loss: 2.262725427272682\n","Epoch: 901, Loss: 2.2614011125933238\n","Epoch: 902, Loss: 2.2600773561281238\n","Epoch: 903, Loss: 2.258754158413158\n","Epoch: 904, Loss: 2.2574315199715502\n","Epoch: 905, Loss: 2.2561094413478155\n","Epoch: 906, Loss: 2.2547879230666568\n","Epoch: 907, Loss: 2.2534669656646007\n","Epoch: 908, Loss: 2.252136083087789\n","Epoch: 909, Loss: 2.2507875727259106\n","Epoch: 910, Loss: 2.249439634058665\n","Epoch: 911, Loss: 2.2480922676305526\n","Epoch: 912, Loss: 2.246745474005559\n","Epoch: 913, Loss: 2.2453992537340204\n","Epoch: 914, Loss: 2.244053607370613\n","Epoch: 915, Loss: 2.2427085354719103\n","Epoch: 916, Loss: 2.2413640385939324\n","Epoch: 917, Loss: 2.2400201172942085\n","Epoch: 918, Loss: 2.238676772129619\n","Epoch: 919, Loss: 2.2373340036543676\n","Epoch: 920, Loss: 2.2359918124248073\n","Epoch: 921, Loss: 2.234650198997661\n","Epoch: 922, Loss: 2.233309163936832\n","Epoch: 923, Loss: 2.231968707792741\n","Epoch: 924, Loss: 2.230628831130433\n","Epoch: 925, Loss: 2.2292895345013366\n","Epoch: 926, Loss: 2.2279508184671806\n","Epoch: 927, Loss: 2.22661268359263\n","Epoch: 928, Loss: 2.2252751304248584\n","Epoch: 929, Loss: 2.2239381595329806\n","Epoch: 930, Loss: 2.2226017714701856\n","Epoch: 931, Loss: 2.2212659668057704\n","Epoch: 932, Loss: 2.219930746093854\n","Epoch: 933, Loss: 2.218596109892734\n","Epoch: 934, Loss: 2.2172620587682297\n","Epoch: 935, Loss: 2.21592859327783\n","Epoch: 936, Loss: 2.214595713982989\n","Epoch: 937, Loss: 2.213263421449754\n","Epoch: 938, Loss: 2.211931716235051\n","Epoch: 939, Loss: 2.2106005989064625\n","Epoch: 940, Loss: 2.2092700700203545\n","Epoch: 941, Loss: 2.2079401301430583\n","Epoch: 942, Loss: 2.2066107798356667\n","Epoch: 943, Loss: 2.2052820196616207\n","Epoch: 944, Loss: 2.20395385018766\n","Epoch: 945, Loss: 2.2026262719713525\n","Epoch: 946, Loss: 2.2012992855823508\n","Epoch: 947, Loss: 2.199972891580314\n","Epoch: 948, Loss: 2.1986470905315016\n","Epoch: 949, Loss: 2.1973218830014773\n","Epoch: 950, Loss: 2.1959972695570835\n","Epoch: 951, Loss: 2.1946732507587474\n","Epoch: 952, Loss: 2.1933498271726233\n","Epoch: 953, Loss: 2.192026999366628\n","Epoch: 954, Loss: 2.190704767903173\n","Epoch: 955, Loss: 2.1893831333532536\n","Epoch: 956, Loss: 2.188062096278738\n","Epoch: 957, Loss: 2.186741657249611\n","Epoch: 958, Loss: 2.1854218168305803\n","Epoch: 959, Loss: 2.1841025755871595\n","Epoch: 960, Loss: 2.1827717750338467\n","Epoch: 961, Loss: 2.1814266804705587\n","Epoch: 962, Loss: 2.1800821949861957\n","Epoch: 963, Loss: 2.17873831916402\n","Epoch: 964, Loss: 2.1773950535981057\n","Epoch: 965, Loss: 2.1760523988718354\n","Epoch: 966, Loss: 2.174710355575268\n","Epoch: 967, Loss: 2.173368924299158\n","Epoch: 968, Loss: 2.1720281056377235\n","Epoch: 969, Loss: 2.1706879001741815\n","Epoch: 970, Loss: 2.1693483084988543\n","Epoch: 971, Loss: 2.1680093312074793\n","Epoch: 972, Loss: 2.166670968888658\n","Epoch: 973, Loss: 2.1653332221329245\n","Epoch: 974, Loss: 2.163996091528209\n","Epoch: 975, Loss: 2.1626595776723816\n","Epoch: 976, Loss: 2.161323681151418\n","Epoch: 977, Loss: 2.1599884025586\n","Epoch: 978, Loss: 2.1586537424871395\n","Epoch: 979, Loss: 2.1573197015304277\n","Epoch: 980, Loss: 2.15598628027909\n","Epoch: 981, Loss: 2.1546534793221874\n","Epoch: 982, Loss: 2.1533212992607225\n","Epoch: 983, Loss: 2.1519897406792357\n","Epoch: 984, Loss: 2.1506588041771684\n","Epoch: 985, Loss: 2.1493284903468948\n","Epoch: 986, Loss: 2.147998799779331\n","Epoch: 987, Loss: 2.146669733069182\n","Epoch: 988, Loss: 2.1453412908121736\n","Epoch: 989, Loss: 2.1440134736025955\n","Epoch: 990, Loss: 2.142686282033194\n","Epoch: 991, Loss: 2.14135971669768\n","Epoch: 992, Loss: 2.1400337781936845\n","Epoch: 993, Loss: 2.138708467116118\n","Epoch: 994, Loss: 2.1373837840586036\n","Epoch: 995, Loss: 2.1360597296157415\n","Epoch: 996, Loss: 2.1347363043844845\n","Epoch: 997, Loss: 2.1334135089606874\n","Epoch: 998, Loss: 2.1320913439399147\n","Epoch: 999, Loss: 2.130769809916739\n","Epoch: 1000, Loss: 2.129448907488588\n"]}]},{"cell_type":"code","source":["#Checking the performance of the trained model\n","temp=0\n","with torch.no_grad():\n","  y_pred = model.forward(X_test_tensor)\n","\n","for i in range(len(y_pred)):\n","  if(y_pred[i]>0.5):\n","    y_pred[i]=1\n","  else:\n","    y_pred[i]=0\n","\n","  if(y_pred[i]==y_test_tensor[i]):\n","    temp=temp+1\n","\n","\n","print(f' Accuracy is {(temp/y_test_tensor.shape[0])*100}')"],"metadata":{"id":"zTLtpIDeiHYR","executionInfo":{"status":"ok","timestamp":1749992937886,"user_tz":-480,"elapsed":33,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4ff4716-e465-48a1-b869-120ef956552f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Accuracy is 80.7017543859649\n"]}]},{"cell_type":"markdown","source":["Torch with NN Module - Constructing the Neural Network Module using NN.\n","\n","A custom module in PyTorch is a user-defined module that is built using the PyTorch library's built-in neural network module, torch.nn.Module. It's a way of creating new modules by combining and extending the functionality provided by existing PyTorch modules.\n","\n","The torch.nn.Module class provides a convenient way to create custom modules because it includes some key features that are important for building neural networks, such as the ability to keep track of learnable parameters and the ability to perform automatic differentiation (for computing gradients during training).\n","\n","By creating a new class that inherits from torch.nn.Module, and defining an __init__ method to initialize the module's parameters, and forward method that perform the computation, we can create our own custom module. These custom modules can be used just like any of the built-in PyTorch modules, such as torch.nn.Module or torch.nn.Conv2d, and can be included in a larger model architecture.\n","\n","LINK: https://chatgpt.com/share/68493a42-2d18-800e-99f9-19d07332fb4d"],"metadata":{"id":"RtYRZDpHaFi0"}},{"cell_type":"code","source":["X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n","X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n","y_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n","y_test_tensor = torch.from_numpy(y_test.astype(np.float32))\n","\n","\n","\n","# create model class\n","import torch\n","import torch.nn as nn\n","\n","class MySimpleNN(nn.Module):\n","\n","  def __init__(self, num_features):\n","\n","    super().__init__()\n","    self.linear = nn.Linear(num_features, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, features):\n","\n","    out = self.linear(features)\n","    out = self.sigmoid(out)\n","\n","    return out"],"metadata":{"id":"Ia4ndLkDaZZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.001\n","epochs = 1000"],"metadata":{"id":"TGu0Ed--RQZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define loss function\n","loss_function = nn.BCELoss()"],"metadata":{"id":"jbp5e4mMDdfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create model\n","model = MySimpleNN(X_train_tensor.shape[1])\n","\n","# define optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# define loop\n","for epoch in range(epochs):\n","\n","  # forward pass\n","  y_pred = model(X_train_tensor) #when you do this the forward pass is automatically triggered. this is a magic function.\n","  #Link: https://www.geeksforgeeks.org/dunder-magic-methods-python/\n","\n","  # loss calculate\n","  loss = loss_function(y_pred, y_train_tensor.view(-1,1))\n","\n","  #y_train_tensor.view(-1, 1) is a PyTorch operation that reshapes the tensor y_train_tensor into a 2-dimensional tensor with 1 column, and the number of rows is inferred automatically.\n","  #-1: PyTorch will infer the correct size for this dimension based on the total number of elements and the other dimensions specified. It's a placeholder for \"whatever size fits\". 1: This is the number of columns — you're explicitly saying that you want each target value to be a separate row, and each row should have one value.\n","\n","  # clear gradients\n","  optimizer.zero_grad()\n","\n","  # backward pass\n","  loss.backward()\n","\n","  # parameters update\n","  optimizer.step()\n","\n","  # print loss in each epoch\n","  print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04Xs02sOD5Wn","executionInfo":{"status":"ok","timestamp":1749992943654,"user_tz":-480,"elapsed":5722,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"0dbba651-efd5-4f97-a001-3c69a840c60c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 0.6175392270088196\n","Epoch: 2, Loss: 0.6160126328468323\n","Epoch: 3, Loss: 0.6144943833351135\n","Epoch: 4, Loss: 0.6129844188690186\n","Epoch: 5, Loss: 0.6114826202392578\n","Epoch: 6, Loss: 0.6099890470504761\n","Epoch: 7, Loss: 0.6085036993026733\n","Epoch: 8, Loss: 0.6070261597633362\n","Epoch: 9, Loss: 0.6055569052696228\n","Epoch: 10, Loss: 0.6040953993797302\n","Epoch: 11, Loss: 0.6026418805122375\n","Epoch: 12, Loss: 0.6011962890625\n","Epoch: 13, Loss: 0.5997583866119385\n","Epoch: 14, Loss: 0.5983282327651978\n","Epoch: 15, Loss: 0.5969058275222778\n","Epoch: 16, Loss: 0.5954910516738892\n","Epoch: 17, Loss: 0.5940837860107422\n","Epoch: 18, Loss: 0.5926841497421265\n","Epoch: 19, Loss: 0.5912920236587524\n","Epoch: 20, Loss: 0.5899072885513306\n","Epoch: 21, Loss: 0.5885298848152161\n","Epoch: 22, Loss: 0.5871598720550537\n","Epoch: 23, Loss: 0.5857971906661987\n","Epoch: 24, Loss: 0.5844416618347168\n","Epoch: 25, Loss: 0.5830933451652527\n","Epoch: 26, Loss: 0.5817521810531616\n","Epoch: 27, Loss: 0.5804181098937988\n","Epoch: 28, Loss: 0.5790910720825195\n","Epoch: 29, Loss: 0.5777710676193237\n","Epoch: 30, Loss: 0.5764579176902771\n","Epoch: 31, Loss: 0.575151801109314\n","Epoch: 32, Loss: 0.5738524198532104\n","Epoch: 33, Loss: 0.5725598931312561\n","Epoch: 34, Loss: 0.5712741017341614\n","Epoch: 35, Loss: 0.5699950456619263\n","Epoch: 36, Loss: 0.5687227249145508\n","Epoch: 37, Loss: 0.5674569010734558\n","Epoch: 38, Loss: 0.5661978125572205\n","Epoch: 39, Loss: 0.5649451613426208\n","Epoch: 40, Loss: 0.563698947429657\n","Epoch: 41, Loss: 0.5624593496322632\n","Epoch: 42, Loss: 0.5612261295318604\n","Epoch: 43, Loss: 0.5599991083145142\n","Epoch: 44, Loss: 0.5587785840034485\n","Epoch: 45, Loss: 0.5575642585754395\n","Epoch: 46, Loss: 0.5563561916351318\n","Epoch: 47, Loss: 0.5551542639732361\n","Epoch: 48, Loss: 0.5539586544036865\n","Epoch: 49, Loss: 0.5527690052986145\n","Epoch: 50, Loss: 0.5515853762626648\n","Epoch: 51, Loss: 0.5504079461097717\n","Epoch: 52, Loss: 0.5492364168167114\n","Epoch: 53, Loss: 0.5480708479881287\n","Epoch: 54, Loss: 0.5469111204147339\n","Epoch: 55, Loss: 0.5457573533058167\n","Epoch: 56, Loss: 0.5446093678474426\n","Epoch: 57, Loss: 0.5434671640396118\n","Epoch: 58, Loss: 0.5423307418823242\n","Epoch: 59, Loss: 0.5412001013755798\n","Epoch: 60, Loss: 0.5400750637054443\n","Epoch: 61, Loss: 0.5389556884765625\n","Epoch: 62, Loss: 0.5378418564796448\n","Epoch: 63, Loss: 0.5367336273193359\n","Epoch: 64, Loss: 0.5356308817863464\n","Epoch: 65, Loss: 0.5345337390899658\n","Epoch: 66, Loss: 0.533441960811615\n","Epoch: 67, Loss: 0.5323556661605835\n","Epoch: 68, Loss: 0.531274676322937\n","Epoch: 69, Loss: 0.5301991105079651\n","Epoch: 70, Loss: 0.5291287302970886\n","Epoch: 71, Loss: 0.5280637741088867\n","Epoch: 72, Loss: 0.5270039439201355\n","Epoch: 73, Loss: 0.5259494185447693\n","Epoch: 74, Loss: 0.5249000191688538\n","Epoch: 75, Loss: 0.5238557457923889\n","Epoch: 76, Loss: 0.52281653881073\n","Epoch: 77, Loss: 0.521782398223877\n","Epoch: 78, Loss: 0.5207534432411194\n","Epoch: 79, Loss: 0.5197293758392334\n","Epoch: 80, Loss: 0.5187102556228638\n","Epoch: 81, Loss: 0.5176960825920105\n","Epoch: 82, Loss: 0.5166868567466736\n","Epoch: 83, Loss: 0.5156825184822083\n","Epoch: 84, Loss: 0.5146830081939697\n","Epoch: 85, Loss: 0.5136882662773132\n","Epoch: 86, Loss: 0.5126982927322388\n","Epoch: 87, Loss: 0.5117130875587463\n","Epoch: 88, Loss: 0.5107326507568359\n","Epoch: 89, Loss: 0.509756863117218\n","Epoch: 90, Loss: 0.5087857246398926\n","Epoch: 91, Loss: 0.5078192353248596\n","Epoch: 92, Loss: 0.5068572163581848\n","Epoch: 93, Loss: 0.5058999061584473\n","Epoch: 94, Loss: 0.5049470663070679\n","Epoch: 95, Loss: 0.5039987564086914\n","Epoch: 96, Loss: 0.5030549764633179\n","Epoch: 97, Loss: 0.5021156072616577\n","Epoch: 98, Loss: 0.5011806488037109\n","Epoch: 99, Loss: 0.5002500414848328\n","Epoch: 100, Loss: 0.49932384490966797\n","Epoch: 101, Loss: 0.498401939868927\n","Epoch: 102, Loss: 0.49748438596725464\n","Epoch: 103, Loss: 0.49657103419303894\n","Epoch: 104, Loss: 0.49566203355789185\n","Epoch: 105, Loss: 0.49475720524787903\n","Epoch: 106, Loss: 0.4938565790653229\n","Epoch: 107, Loss: 0.4929600656032562\n","Epoch: 108, Loss: 0.49206778407096863\n","Epoch: 109, Loss: 0.49117955565452576\n","Epoch: 110, Loss: 0.49029555916786194\n","Epoch: 111, Loss: 0.4894154667854309\n","Epoch: 112, Loss: 0.4885394275188446\n","Epoch: 113, Loss: 0.487667441368103\n","Epoch: 114, Loss: 0.4867994785308838\n","Epoch: 115, Loss: 0.48593541979789734\n","Epoch: 116, Loss: 0.4850753843784332\n","Epoch: 117, Loss: 0.48421916365623474\n","Epoch: 118, Loss: 0.48336684703826904\n","Epoch: 119, Loss: 0.48251840472221375\n","Epoch: 120, Loss: 0.48167383670806885\n","Epoch: 121, Loss: 0.4808330237865448\n","Epoch: 122, Loss: 0.4799960255622864\n","Epoch: 123, Loss: 0.4791628122329712\n","Epoch: 124, Loss: 0.47833332419395447\n","Epoch: 125, Loss: 0.4775075316429138\n","Epoch: 126, Loss: 0.47668543457984924\n","Epoch: 127, Loss: 0.47586706280708313\n","Epoch: 128, Loss: 0.4750523269176483\n","Epoch: 129, Loss: 0.4742411971092224\n","Epoch: 130, Loss: 0.4734336733818054\n","Epoch: 131, Loss: 0.47262975573539734\n","Epoch: 132, Loss: 0.4718293845653534\n","Epoch: 133, Loss: 0.47103258967399597\n","Epoch: 134, Loss: 0.47023922204971313\n","Epoch: 135, Loss: 0.4694494307041168\n","Epoch: 136, Loss: 0.4686630964279175\n","Epoch: 137, Loss: 0.4678802192211151\n","Epoch: 138, Loss: 0.46710067987442017\n","Epoch: 139, Loss: 0.46632468700408936\n","Epoch: 140, Loss: 0.4655520021915436\n","Epoch: 141, Loss: 0.46478271484375\n","Epoch: 142, Loss: 0.46401676535606384\n","Epoch: 143, Loss: 0.46325406432151794\n","Epoch: 144, Loss: 0.46249476075172424\n","Epoch: 145, Loss: 0.4617387354373932\n","Epoch: 146, Loss: 0.4609859585762024\n","Epoch: 147, Loss: 0.46023643016815186\n","Epoch: 148, Loss: 0.4594900906085968\n","Epoch: 149, Loss: 0.45874708890914917\n","Epoch: 150, Loss: 0.4580071270465851\n","Epoch: 151, Loss: 0.45727038383483887\n","Epoch: 152, Loss: 0.45653679966926575\n","Epoch: 153, Loss: 0.45580628514289856\n","Epoch: 154, Loss: 0.45507895946502686\n","Epoch: 155, Loss: 0.45435476303100586\n","Epoch: 156, Loss: 0.45363351702690125\n","Epoch: 157, Loss: 0.45291540026664734\n","Epoch: 158, Loss: 0.4522002637386322\n","Epoch: 159, Loss: 0.451488196849823\n","Epoch: 160, Loss: 0.45077911019325256\n","Epoch: 161, Loss: 0.4500729739665985\n","Epoch: 162, Loss: 0.4493699073791504\n","Epoch: 163, Loss: 0.4486696422100067\n","Epoch: 164, Loss: 0.4479724168777466\n","Epoch: 165, Loss: 0.4472780227661133\n","Epoch: 166, Loss: 0.44658657908439636\n","Epoch: 167, Loss: 0.4458979666233063\n","Epoch: 168, Loss: 0.4452122747898102\n","Epoch: 169, Loss: 0.4445294141769409\n","Epoch: 170, Loss: 0.4438494145870209\n","Epoch: 171, Loss: 0.44317206740379333\n","Epoch: 172, Loss: 0.4424976408481598\n","Epoch: 173, Loss: 0.4418259561061859\n","Epoch: 174, Loss: 0.4411570429801941\n","Epoch: 175, Loss: 0.44049087166786194\n","Epoch: 176, Loss: 0.43982744216918945\n","Epoch: 177, Loss: 0.43916669487953186\n","Epoch: 178, Loss: 0.43850868940353394\n","Epoch: 179, Loss: 0.43785330653190613\n","Epoch: 180, Loss: 0.4372006058692932\n","Epoch: 181, Loss: 0.4365505874156952\n","Epoch: 182, Loss: 0.4359031319618225\n","Epoch: 183, Loss: 0.43525832891464233\n","Epoch: 184, Loss: 0.4346161484718323\n","Epoch: 185, Loss: 0.43397653102874756\n","Epoch: 186, Loss: 0.43333950638771057\n","Epoch: 187, Loss: 0.4327050447463989\n","Epoch: 188, Loss: 0.4320731461048126\n","Epoch: 189, Loss: 0.4314436912536621\n","Epoch: 190, Loss: 0.4308168292045593\n","Epoch: 191, Loss: 0.43019241094589233\n","Epoch: 192, Loss: 0.4295705258846283\n","Epoch: 193, Loss: 0.42895105481147766\n","Epoch: 194, Loss: 0.4283340871334076\n","Epoch: 195, Loss: 0.4277195930480957\n","Epoch: 196, Loss: 0.42710745334625244\n","Epoch: 197, Loss: 0.42649778723716736\n","Epoch: 198, Loss: 0.4258904457092285\n","Epoch: 199, Loss: 0.42528557777404785\n","Epoch: 200, Loss: 0.4246830344200134\n","Epoch: 201, Loss: 0.4240829050540924\n","Epoch: 202, Loss: 0.4234851002693176\n","Epoch: 203, Loss: 0.4228895902633667\n","Epoch: 204, Loss: 0.422296404838562\n","Epoch: 205, Loss: 0.4217055141925812\n","Epoch: 206, Loss: 0.42111703753471375\n","Epoch: 207, Loss: 0.4205307066440582\n","Epoch: 208, Loss: 0.41994673013687134\n","Epoch: 209, Loss: 0.41936489939689636\n","Epoch: 210, Loss: 0.41878542304039\n","Epoch: 211, Loss: 0.41820812225341797\n","Epoch: 212, Loss: 0.417633056640625\n","Epoch: 213, Loss: 0.4170602262020111\n","Epoch: 214, Loss: 0.41648951172828674\n","Epoch: 215, Loss: 0.41592106223106384\n","Epoch: 216, Loss: 0.41535472869873047\n","Epoch: 217, Loss: 0.4147906005382538\n","Epoch: 218, Loss: 0.41422855854034424\n","Epoch: 219, Loss: 0.4136687219142914\n","Epoch: 220, Loss: 0.4131109416484833\n","Epoch: 221, Loss: 0.4125553071498871\n","Epoch: 222, Loss: 0.41200172901153564\n","Epoch: 223, Loss: 0.4114502966403961\n","Epoch: 224, Loss: 0.41090095043182373\n","Epoch: 225, Loss: 0.41035357117652893\n","Epoch: 226, Loss: 0.40980830788612366\n","Epoch: 227, Loss: 0.40926507115364075\n","Epoch: 228, Loss: 0.4087238609790802\n","Epoch: 229, Loss: 0.4081847369670868\n","Epoch: 230, Loss: 0.4076475501060486\n","Epoch: 231, Loss: 0.40711236000061035\n","Epoch: 232, Loss: 0.4065791964530945\n","Epoch: 233, Loss: 0.4060479402542114\n","Epoch: 234, Loss: 0.4055187404155731\n","Epoch: 235, Loss: 0.40499147772789\n","Epoch: 236, Loss: 0.40446609258651733\n","Epoch: 237, Loss: 0.403942734003067\n","Epoch: 238, Loss: 0.40342119336128235\n","Epoch: 239, Loss: 0.40290164947509766\n","Epoch: 240, Loss: 0.4023839831352234\n","Epoch: 241, Loss: 0.40186813473701477\n","Epoch: 242, Loss: 0.40135428309440613\n","Epoch: 243, Loss: 0.40084224939346313\n","Epoch: 244, Loss: 0.4003320634365082\n","Epoch: 245, Loss: 0.39982375502586365\n","Epoch: 246, Loss: 0.39931726455688477\n","Epoch: 247, Loss: 0.39881259202957153\n","Epoch: 248, Loss: 0.3983098566532135\n","Epoch: 249, Loss: 0.39780884981155396\n","Epoch: 250, Loss: 0.39730963110923767\n","Epoch: 251, Loss: 0.39681223034858704\n","Epoch: 252, Loss: 0.3963165879249573\n","Epoch: 253, Loss: 0.39582279324531555\n","Epoch: 254, Loss: 0.39533066749572754\n","Epoch: 255, Loss: 0.3948403596878052\n","Epoch: 256, Loss: 0.3943517804145813\n","Epoch: 257, Loss: 0.3938649296760559\n","Epoch: 258, Loss: 0.3933798372745514\n","Epoch: 259, Loss: 0.3928964138031006\n","Epoch: 260, Loss: 0.39241477847099304\n","Epoch: 261, Loss: 0.3919347822666168\n","Epoch: 262, Loss: 0.3914564847946167\n","Epoch: 263, Loss: 0.39097991585731506\n","Epoch: 264, Loss: 0.39050495624542236\n","Epoch: 265, Loss: 0.39003169536590576\n","Epoch: 266, Loss: 0.38956010341644287\n","Epoch: 267, Loss: 0.3890901803970337\n","Epoch: 268, Loss: 0.38862183690071106\n","Epoch: 269, Loss: 0.38815516233444214\n","Epoch: 270, Loss: 0.38769012689590454\n","Epoch: 271, Loss: 0.3872266709804535\n","Epoch: 272, Loss: 0.3867648243904114\n","Epoch: 273, Loss: 0.3863045871257782\n","Epoch: 274, Loss: 0.38584598898887634\n","Epoch: 275, Loss: 0.38538888096809387\n","Epoch: 276, Loss: 0.3849334120750427\n","Epoch: 277, Loss: 0.3844795525074005\n","Epoch: 278, Loss: 0.3840271532535553\n","Epoch: 279, Loss: 0.3835763931274414\n","Epoch: 280, Loss: 0.3831270933151245\n","Epoch: 281, Loss: 0.3826793432235718\n","Epoch: 282, Loss: 0.3822331726551056\n","Epoch: 283, Loss: 0.3817884922027588\n","Epoch: 284, Loss: 0.38134530186653137\n","Epoch: 285, Loss: 0.3809036314487457\n","Epoch: 286, Loss: 0.38046348094940186\n","Epoch: 287, Loss: 0.380024790763855\n","Epoch: 288, Loss: 0.3795875608921051\n","Epoch: 289, Loss: 0.3791518807411194\n","Epoch: 290, Loss: 0.3787176311016083\n","Epoch: 291, Loss: 0.37828484177589417\n","Epoch: 292, Loss: 0.37785348296165466\n","Epoch: 293, Loss: 0.37742355465888977\n","Epoch: 294, Loss: 0.37699511647224426\n","Epoch: 295, Loss: 0.376568078994751\n","Epoch: 296, Loss: 0.3761424720287323\n","Epoch: 297, Loss: 0.37571826577186584\n","Epoch: 298, Loss: 0.3752954602241516\n","Epoch: 299, Loss: 0.3748741149902344\n","Epoch: 300, Loss: 0.3744541108608246\n","Epoch: 301, Loss: 0.374035507440567\n","Epoch: 302, Loss: 0.37361833453178406\n","Epoch: 303, Loss: 0.37320247292518616\n","Epoch: 304, Loss: 0.3727879822254181\n","Epoch: 305, Loss: 0.37237486243247986\n","Epoch: 306, Loss: 0.3719630837440491\n","Epoch: 307, Loss: 0.3715527355670929\n","Epoch: 308, Loss: 0.371143639087677\n","Epoch: 309, Loss: 0.37073585391044617\n","Epoch: 310, Loss: 0.37032946944236755\n","Epoch: 311, Loss: 0.3699244260787964\n","Epoch: 312, Loss: 0.3695206642150879\n","Epoch: 313, Loss: 0.3691181540489197\n","Epoch: 314, Loss: 0.3687170147895813\n","Epoch: 315, Loss: 0.368317186832428\n","Epoch: 316, Loss: 0.36791858077049255\n","Epoch: 317, Loss: 0.3675212860107422\n","Epoch: 318, Loss: 0.3671253025531769\n","Epoch: 319, Loss: 0.36673054099082947\n","Epoch: 320, Loss: 0.3663370907306671\n","Epoch: 321, Loss: 0.36594489216804504\n","Epoch: 322, Loss: 0.36555394530296326\n","Epoch: 323, Loss: 0.36516422033309937\n","Epoch: 324, Loss: 0.36477574706077576\n","Epoch: 325, Loss: 0.36438849568367004\n","Epoch: 326, Loss: 0.3640024662017822\n","Epoch: 327, Loss: 0.3636177182197571\n","Epoch: 328, Loss: 0.36323413252830505\n","Epoch: 329, Loss: 0.3628517985343933\n","Epoch: 330, Loss: 0.3624706566333771\n","Epoch: 331, Loss: 0.36209070682525635\n","Epoch: 332, Loss: 0.36171194911003113\n","Epoch: 333, Loss: 0.36133435368537903\n","Epoch: 334, Loss: 0.3609580099582672\n","Epoch: 335, Loss: 0.3605828285217285\n","Epoch: 336, Loss: 0.36020880937576294\n","Epoch: 337, Loss: 0.3598359227180481\n","Epoch: 338, Loss: 0.35946419835090637\n","Epoch: 339, Loss: 0.35909363627433777\n","Epoch: 340, Loss: 0.3587242662906647\n","Epoch: 341, Loss: 0.3583560585975647\n","Epoch: 342, Loss: 0.3579889237880707\n","Epoch: 343, Loss: 0.3576229512691498\n","Epoch: 344, Loss: 0.3572580814361572\n","Epoch: 345, Loss: 0.3568943440914154\n","Epoch: 346, Loss: 0.3565317690372467\n","Epoch: 347, Loss: 0.3561702370643616\n","Epoch: 348, Loss: 0.35580989718437195\n","Epoch: 349, Loss: 0.3554505705833435\n","Epoch: 350, Loss: 0.35509243607521057\n","Epoch: 351, Loss: 0.3547353148460388\n","Epoch: 352, Loss: 0.3543793261051178\n","Epoch: 353, Loss: 0.3540244400501251\n","Epoch: 354, Loss: 0.353670597076416\n","Epoch: 355, Loss: 0.35331782698631287\n","Epoch: 356, Loss: 0.3529660999774933\n","Epoch: 357, Loss: 0.35261544585227966\n","Epoch: 358, Loss: 0.3522658944129944\n","Epoch: 359, Loss: 0.3519173860549927\n","Epoch: 360, Loss: 0.35156992077827454\n","Epoch: 361, Loss: 0.3512234389781952\n","Epoch: 362, Loss: 0.3508780598640442\n","Epoch: 363, Loss: 0.35053369402885437\n","Epoch: 364, Loss: 0.3501904308795929\n","Epoch: 365, Loss: 0.34984809160232544\n","Epoch: 366, Loss: 0.34950679540634155\n","Epoch: 367, Loss: 0.3491665720939636\n","Epoch: 368, Loss: 0.3488272428512573\n","Epoch: 369, Loss: 0.34848904609680176\n","Epoch: 370, Loss: 0.3481517434120178\n","Epoch: 371, Loss: 0.34781554341316223\n","Epoch: 372, Loss: 0.34748026728630066\n","Epoch: 373, Loss: 0.34714600443840027\n","Epoch: 374, Loss: 0.3468126952648163\n","Epoch: 375, Loss: 0.3464803695678711\n","Epoch: 376, Loss: 0.3461490571498871\n","Epoch: 377, Loss: 0.34581872820854187\n","Epoch: 378, Loss: 0.3454892933368683\n","Epoch: 379, Loss: 0.3451608419418335\n","Epoch: 380, Loss: 0.3448334336280823\n","Epoch: 381, Loss: 0.3445068895816803\n","Epoch: 382, Loss: 0.3441813290119171\n","Epoch: 383, Loss: 0.34385666251182556\n","Epoch: 384, Loss: 0.3435329794883728\n","Epoch: 385, Loss: 0.34321025013923645\n","Epoch: 386, Loss: 0.3428884446620941\n","Epoch: 387, Loss: 0.3425675630569458\n","Epoch: 388, Loss: 0.3422476053237915\n","Epoch: 389, Loss: 0.3419285714626312\n","Epoch: 390, Loss: 0.34161049127578735\n","Epoch: 391, Loss: 0.34129324555397034\n","Epoch: 392, Loss: 0.34097692370414734\n","Epoch: 393, Loss: 0.34066152572631836\n","Epoch: 394, Loss: 0.340347021818161\n","Epoch: 395, Loss: 0.34003347158432007\n","Epoch: 396, Loss: 0.3397207260131836\n","Epoch: 397, Loss: 0.3394089341163635\n","Epoch: 398, Loss: 0.3390979766845703\n","Epoch: 399, Loss: 0.3387879431247711\n","Epoch: 400, Loss: 0.33847877383232117\n","Epoch: 401, Loss: 0.33817043900489807\n","Epoch: 402, Loss: 0.3378629982471466\n","Epoch: 403, Loss: 0.3375564515590668\n","Epoch: 404, Loss: 0.3372507393360138\n","Epoch: 405, Loss: 0.33694589138031006\n","Epoch: 406, Loss: 0.3366418778896332\n","Epoch: 407, Loss: 0.3363387882709503\n","Epoch: 408, Loss: 0.33603647351264954\n","Epoch: 409, Loss: 0.3357350528240204\n","Epoch: 410, Loss: 0.3354344069957733\n","Epoch: 411, Loss: 0.3351346552371979\n","Epoch: 412, Loss: 0.3348356783390045\n","Epoch: 413, Loss: 0.3345375955104828\n","Epoch: 414, Loss: 0.3342403173446655\n","Epoch: 415, Loss: 0.33394384384155273\n","Epoch: 416, Loss: 0.3336482048034668\n","Epoch: 417, Loss: 0.3333533704280853\n","Epoch: 418, Loss: 0.3330593705177307\n","Epoch: 419, Loss: 0.3327661454677582\n","Epoch: 420, Loss: 0.3324736952781677\n","Epoch: 421, Loss: 0.3321821689605713\n","Epoch: 422, Loss: 0.3318912982940674\n","Epoch: 423, Loss: 0.3316013216972351\n","Epoch: 424, Loss: 0.3313121199607849\n","Epoch: 425, Loss: 0.3310236632823944\n","Epoch: 426, Loss: 0.33073604106903076\n","Epoch: 427, Loss: 0.3304491639137268\n","Epoch: 428, Loss: 0.3301630914211273\n","Epoch: 429, Loss: 0.3298778235912323\n","Epoch: 430, Loss: 0.3295932710170746\n","Epoch: 431, Loss: 0.32930952310562134\n","Epoch: 432, Loss: 0.3290265202522278\n","Epoch: 433, Loss: 0.3287442624568939\n","Epoch: 434, Loss: 0.32846277952194214\n","Epoch: 435, Loss: 0.32818204164505005\n","Epoch: 436, Loss: 0.32790204882621765\n","Epoch: 437, Loss: 0.3276228904724121\n","Epoch: 438, Loss: 0.3273444175720215\n","Epoch: 439, Loss: 0.32706668972969055\n","Epoch: 440, Loss: 0.3267896771430969\n","Epoch: 441, Loss: 0.3265134394168854\n","Epoch: 442, Loss: 0.32623791694641113\n","Epoch: 443, Loss: 0.3259631395339966\n","Epoch: 444, Loss: 0.32568904757499695\n","Epoch: 445, Loss: 0.325415700674057\n","Epoch: 446, Loss: 0.32514312863349915\n","Epoch: 447, Loss: 0.3248712122440338\n","Epoch: 448, Loss: 0.3246000409126282\n","Epoch: 449, Loss: 0.3243296146392822\n","Epoch: 450, Loss: 0.3240598440170288\n","Epoch: 451, Loss: 0.3237907886505127\n","Epoch: 452, Loss: 0.3235224783420563\n","Epoch: 453, Loss: 0.32325479388237\n","Epoch: 454, Loss: 0.3229879140853882\n","Epoch: 455, Loss: 0.3227216899394989\n","Epoch: 456, Loss: 0.32245612144470215\n","Epoch: 457, Loss: 0.3221912980079651\n","Epoch: 458, Loss: 0.32192710041999817\n","Epoch: 459, Loss: 0.32166358828544617\n","Epoch: 460, Loss: 0.32140079140663147\n","Epoch: 461, Loss: 0.3211386799812317\n","Epoch: 462, Loss: 0.3208772540092468\n","Epoch: 463, Loss: 0.3206164538860321\n","Epoch: 464, Loss: 0.3203563988208771\n","Epoch: 465, Loss: 0.3200969398021698\n","Epoch: 466, Loss: 0.31983816623687744\n","Epoch: 467, Loss: 0.319580078125\n","Epoch: 468, Loss: 0.3193226456642151\n","Epoch: 469, Loss: 0.3190658688545227\n","Epoch: 470, Loss: 0.31880974769592285\n","Epoch: 471, Loss: 0.3185543119907379\n","Epoch: 472, Loss: 0.31829947233200073\n","Epoch: 473, Loss: 0.3180452883243561\n","Epoch: 474, Loss: 0.31779178977012634\n","Epoch: 475, Loss: 0.31753888726234436\n","Epoch: 476, Loss: 0.3172866404056549\n","Epoch: 477, Loss: 0.3170350193977356\n","Epoch: 478, Loss: 0.3167840540409088\n","Epoch: 479, Loss: 0.3165336847305298\n","Epoch: 480, Loss: 0.3162840008735657\n","Epoch: 481, Loss: 0.3160349428653717\n","Epoch: 482, Loss: 0.3157864511013031\n","Epoch: 483, Loss: 0.315538614988327\n","Epoch: 484, Loss: 0.3152914047241211\n","Epoch: 485, Loss: 0.3150447905063629\n","Epoch: 486, Loss: 0.31479883193969727\n","Epoch: 487, Loss: 0.31455346941947937\n","Epoch: 488, Loss: 0.31430867314338684\n","Epoch: 489, Loss: 0.3140645921230316\n","Epoch: 490, Loss: 0.31382104754447937\n","Epoch: 491, Loss: 0.3135780990123749\n","Epoch: 492, Loss: 0.3133357763290405\n","Epoch: 493, Loss: 0.31309401988983154\n","Epoch: 494, Loss: 0.3128528892993927\n","Epoch: 495, Loss: 0.3126123547554016\n","Epoch: 496, Loss: 0.3123723864555359\n","Epoch: 497, Loss: 0.3121330142021179\n","Epoch: 498, Loss: 0.3118942379951477\n","Epoch: 499, Loss: 0.31165602803230286\n","Epoch: 500, Loss: 0.31141841411590576\n","Epoch: 501, Loss: 0.3111814260482788\n","Epoch: 502, Loss: 0.31094497442245483\n","Epoch: 503, Loss: 0.3107091188430786\n","Epoch: 504, Loss: 0.31047379970550537\n","Epoch: 505, Loss: 0.3102390468120575\n","Epoch: 506, Loss: 0.3100048899650574\n","Epoch: 507, Loss: 0.309771329164505\n","Epoch: 508, Loss: 0.3095382750034332\n","Epoch: 509, Loss: 0.3093058466911316\n","Epoch: 510, Loss: 0.30907395482063293\n","Epoch: 511, Loss: 0.30884259939193726\n","Epoch: 512, Loss: 0.30861184000968933\n","Epoch: 513, Loss: 0.3083815574645996\n","Epoch: 514, Loss: 0.3081519305706024\n","Epoch: 515, Loss: 0.3079228103160858\n","Epoch: 516, Loss: 0.3076942563056946\n","Epoch: 517, Loss: 0.30746620893478394\n","Epoch: 518, Loss: 0.30723872780799866\n","Epoch: 519, Loss: 0.30701175332069397\n","Epoch: 520, Loss: 0.30678537487983704\n","Epoch: 521, Loss: 0.3065595030784607\n","Epoch: 522, Loss: 0.3063341975212097\n","Epoch: 523, Loss: 0.30610933899879456\n","Epoch: 524, Loss: 0.3058851361274719\n","Epoch: 525, Loss: 0.3056614100933075\n","Epoch: 526, Loss: 0.30543819069862366\n","Epoch: 527, Loss: 0.3052155077457428\n","Epoch: 528, Loss: 0.3049933910369873\n","Epoch: 529, Loss: 0.3047717809677124\n","Epoch: 530, Loss: 0.3045506477355957\n","Epoch: 531, Loss: 0.304330050945282\n","Epoch: 532, Loss: 0.30410999059677124\n","Epoch: 533, Loss: 0.3038904070854187\n","Epoch: 534, Loss: 0.30367136001586914\n","Epoch: 535, Loss: 0.30345284938812256\n","Epoch: 536, Loss: 0.3032348155975342\n","Epoch: 537, Loss: 0.3030173182487488\n","Epoch: 538, Loss: 0.3028002679347992\n","Epoch: 539, Loss: 0.30258381366729736\n","Epoch: 540, Loss: 0.30236780643463135\n","Epoch: 541, Loss: 0.30215224623680115\n","Epoch: 542, Loss: 0.3019372522830963\n","Epoch: 543, Loss: 0.30172276496887207\n","Epoch: 544, Loss: 0.30150875449180603\n","Epoch: 545, Loss: 0.3012952208518982\n","Epoch: 546, Loss: 0.30108219385147095\n","Epoch: 547, Loss: 0.3008696436882019\n","Epoch: 548, Loss: 0.30065757036209106\n","Epoch: 549, Loss: 0.3004460334777832\n","Epoch: 550, Loss: 0.30023494362831116\n","Epoch: 551, Loss: 0.3000243604183197\n","Epoch: 552, Loss: 0.29981422424316406\n","Epoch: 553, Loss: 0.29960453510284424\n","Epoch: 554, Loss: 0.2993953824043274\n","Epoch: 555, Loss: 0.29918667674064636\n","Epoch: 556, Loss: 0.2989785075187683\n","Epoch: 557, Loss: 0.2987707555294037\n","Epoch: 558, Loss: 0.2985634505748749\n","Epoch: 559, Loss: 0.29835665225982666\n","Epoch: 560, Loss: 0.29815033078193665\n","Epoch: 561, Loss: 0.29794442653656006\n","Epoch: 562, Loss: 0.29773902893066406\n","Epoch: 563, Loss: 0.29753410816192627\n","Epoch: 564, Loss: 0.2973295748233795\n","Epoch: 565, Loss: 0.29712551832199097\n","Epoch: 566, Loss: 0.2969219982624054\n","Epoch: 567, Loss: 0.2967188358306885\n","Epoch: 568, Loss: 0.29651618003845215\n","Epoch: 569, Loss: 0.29631397128105164\n","Epoch: 570, Loss: 0.29611220955848694\n","Epoch: 571, Loss: 0.29591086506843567\n","Epoch: 572, Loss: 0.295710027217865\n","Epoch: 573, Loss: 0.29550957679748535\n","Epoch: 574, Loss: 0.2953096032142639\n","Epoch: 575, Loss: 0.2951100766658783\n","Epoch: 576, Loss: 0.2949109375476837\n","Epoch: 577, Loss: 0.2947123050689697\n","Epoch: 578, Loss: 0.29451411962509155\n","Epoch: 579, Loss: 0.2943163216114044\n","Epoch: 580, Loss: 0.2941189706325531\n","Epoch: 581, Loss: 0.2939220368862152\n","Epoch: 582, Loss: 0.2937255799770355\n","Epoch: 583, Loss: 0.29352954030036926\n","Epoch: 584, Loss: 0.29333391785621643\n","Epoch: 585, Loss: 0.293138712644577\n","Epoch: 586, Loss: 0.29294392466545105\n","Epoch: 587, Loss: 0.2927495837211609\n","Epoch: 588, Loss: 0.29255566000938416\n","Epoch: 589, Loss: 0.29236215353012085\n","Epoch: 590, Loss: 0.29216909408569336\n","Epoch: 591, Loss: 0.2919763922691345\n","Epoch: 592, Loss: 0.2917841970920563\n","Epoch: 593, Loss: 0.2915923297405243\n","Epoch: 594, Loss: 0.2914009094238281\n","Epoch: 595, Loss: 0.2912099361419678\n","Epoch: 596, Loss: 0.2910193204879761\n","Epoch: 597, Loss: 0.2908291518688202\n","Epoch: 598, Loss: 0.29063940048217773\n","Epoch: 599, Loss: 0.29045000672340393\n","Epoch: 600, Loss: 0.29026103019714355\n","Epoch: 601, Loss: 0.2900725305080414\n","Epoch: 602, Loss: 0.2898843586444855\n","Epoch: 603, Loss: 0.289696604013443\n","Epoch: 604, Loss: 0.28950926661491394\n","Epoch: 605, Loss: 0.2893223166465759\n","Epoch: 606, Loss: 0.28913575410842896\n","Epoch: 607, Loss: 0.288949579000473\n","Epoch: 608, Loss: 0.2887638211250305\n","Epoch: 609, Loss: 0.28857848048210144\n","Epoch: 610, Loss: 0.288393497467041\n","Epoch: 611, Loss: 0.28820887207984924\n","Epoch: 612, Loss: 0.2880246937274933\n","Epoch: 613, Loss: 0.28784090280532837\n","Epoch: 614, Loss: 0.2876574695110321\n","Epoch: 615, Loss: 0.2874744236469269\n","Epoch: 616, Loss: 0.2872917652130127\n","Epoch: 617, Loss: 0.28710946440696716\n","Epoch: 618, Loss: 0.28692761063575745\n","Epoch: 619, Loss: 0.286746084690094\n","Epoch: 620, Loss: 0.28656497597694397\n","Epoch: 621, Loss: 0.2863842248916626\n","Epoch: 622, Loss: 0.2862038314342499\n","Epoch: 623, Loss: 0.2860238552093506\n","Epoch: 624, Loss: 0.28584420680999756\n","Epoch: 625, Loss: 0.28566494584083557\n","Epoch: 626, Loss: 0.285486102104187\n","Epoch: 627, Loss: 0.28530755639076233\n","Epoch: 628, Loss: 0.28512945771217346\n","Epoch: 629, Loss: 0.28495168685913086\n","Epoch: 630, Loss: 0.2847742736339569\n","Epoch: 631, Loss: 0.284597247838974\n","Epoch: 632, Loss: 0.28442057967185974\n","Epoch: 633, Loss: 0.28424423933029175\n","Epoch: 634, Loss: 0.2840682864189148\n","Epoch: 635, Loss: 0.2838927209377289\n","Epoch: 636, Loss: 0.28371748328208923\n","Epoch: 637, Loss: 0.28354260325431824\n","Epoch: 638, Loss: 0.2833680808544159\n","Epoch: 639, Loss: 0.2831939458847046\n","Epoch: 640, Loss: 0.28302016854286194\n","Epoch: 641, Loss: 0.2828466594219208\n","Epoch: 642, Loss: 0.28267359733581543\n","Epoch: 643, Loss: 0.28250089287757874\n","Epoch: 644, Loss: 0.28232845664024353\n","Epoch: 645, Loss: 0.28215643763542175\n","Epoch: 646, Loss: 0.28198474645614624\n","Epoch: 647, Loss: 0.2818133533000946\n","Epoch: 648, Loss: 0.2816423773765564\n","Epoch: 649, Loss: 0.28147169947624207\n","Epoch: 650, Loss: 0.2813014090061188\n","Epoch: 651, Loss: 0.28113141655921936\n","Epoch: 652, Loss: 0.2809617817401886\n","Epoch: 653, Loss: 0.2807925045490265\n","Epoch: 654, Loss: 0.28062355518341064\n","Epoch: 655, Loss: 0.2804549038410187\n","Epoch: 656, Loss: 0.28028663992881775\n","Epoch: 657, Loss: 0.2801187038421631\n","Epoch: 658, Loss: 0.2799510955810547\n","Epoch: 659, Loss: 0.27978381514549255\n","Epoch: 660, Loss: 0.2796168625354767\n","Epoch: 661, Loss: 0.27945026755332947\n","Epoch: 662, Loss: 0.2792840003967285\n","Epoch: 663, Loss: 0.27911806106567383\n","Epoch: 664, Loss: 0.278952419757843\n","Epoch: 665, Loss: 0.27878713607788086\n","Epoch: 666, Loss: 0.2786221504211426\n","Epoch: 667, Loss: 0.27845755219459534\n","Epoch: 668, Loss: 0.2782932221889496\n","Epoch: 669, Loss: 0.2781292200088501\n","Epoch: 670, Loss: 0.2779655456542969\n","Epoch: 671, Loss: 0.2778021991252899\n","Epoch: 672, Loss: 0.27763915061950684\n","Epoch: 673, Loss: 0.2774764895439148\n","Epoch: 674, Loss: 0.27731406688690186\n","Epoch: 675, Loss: 0.2771519720554352\n","Epoch: 676, Loss: 0.27699020504951477\n","Epoch: 677, Loss: 0.276828795671463\n","Epoch: 678, Loss: 0.27666765451431274\n","Epoch: 679, Loss: 0.27650684118270874\n","Epoch: 680, Loss: 0.2763463258743286\n","Epoch: 681, Loss: 0.27618613839149475\n","Epoch: 682, Loss: 0.27602624893188477\n","Epoch: 683, Loss: 0.27586665749549866\n","Epoch: 684, Loss: 0.2757073938846588\n","Epoch: 685, Loss: 0.27554842829704285\n","Epoch: 686, Loss: 0.27538979053497314\n","Epoch: 687, Loss: 0.27523142099380493\n","Epoch: 688, Loss: 0.275073379278183\n","Epoch: 689, Loss: 0.2749156653881073\n","Epoch: 690, Loss: 0.2747582197189331\n","Epoch: 691, Loss: 0.2746010720729828\n","Epoch: 692, Loss: 0.27444425225257874\n","Epoch: 693, Loss: 0.27428773045539856\n","Epoch: 694, Loss: 0.2741314768791199\n","Epoch: 695, Loss: 0.27397552132606506\n","Epoch: 696, Loss: 0.2738199234008789\n","Epoch: 697, Loss: 0.27366453409194946\n","Epoch: 698, Loss: 0.27350950241088867\n","Epoch: 699, Loss: 0.27335473895072937\n","Epoch: 700, Loss: 0.27320027351379395\n","Epoch: 701, Loss: 0.2730461061000824\n","Epoch: 702, Loss: 0.2728922367095947\n","Epoch: 703, Loss: 0.27273863554000854\n","Epoch: 704, Loss: 0.27258536219596863\n","Epoch: 705, Loss: 0.2724323570728302\n","Epoch: 706, Loss: 0.27227962017059326\n","Epoch: 707, Loss: 0.2721271812915802\n","Epoch: 708, Loss: 0.27197501063346863\n","Epoch: 709, Loss: 0.2718231976032257\n","Epoch: 710, Loss: 0.2716715931892395\n","Epoch: 711, Loss: 0.27152031660079956\n","Epoch: 712, Loss: 0.2713693082332611\n","Epoch: 713, Loss: 0.27121859788894653\n","Epoch: 714, Loss: 0.27106812596321106\n","Epoch: 715, Loss: 0.27091798186302185\n","Epoch: 716, Loss: 0.27076807618141174\n","Epoch: 717, Loss: 0.2706184685230255\n","Epoch: 718, Loss: 0.27046915888786316\n","Epoch: 719, Loss: 0.2703200876712799\n","Epoch: 720, Loss: 0.27017131447792053\n","Epoch: 721, Loss: 0.27002280950546265\n","Epoch: 722, Loss: 0.26987454295158386\n","Epoch: 723, Loss: 0.26972663402557373\n","Epoch: 724, Loss: 0.2695789337158203\n","Epoch: 725, Loss: 0.26943153142929077\n","Epoch: 726, Loss: 0.2692843973636627\n","Epoch: 727, Loss: 0.26913756132125854\n","Epoch: 728, Loss: 0.26899096369743347\n","Epoch: 729, Loss: 0.2688446044921875\n","Epoch: 730, Loss: 0.2686985433101654\n","Epoch: 731, Loss: 0.2685527503490448\n","Epoch: 732, Loss: 0.2684072256088257\n","Epoch: 733, Loss: 0.26826193928718567\n","Epoch: 734, Loss: 0.26811695098876953\n","Epoch: 735, Loss: 0.2679722309112549\n","Epoch: 736, Loss: 0.26782774925231934\n","Epoch: 737, Loss: 0.26768356561660767\n","Epoch: 738, Loss: 0.2675395905971527\n","Epoch: 739, Loss: 0.267395943403244\n","Epoch: 740, Loss: 0.26725250482559204\n","Epoch: 741, Loss: 0.26710933446884155\n","Epoch: 742, Loss: 0.26696643233299255\n","Epoch: 743, Loss: 0.26682379841804504\n","Epoch: 744, Loss: 0.26668140292167664\n","Epoch: 745, Loss: 0.26653924584388733\n","Epoch: 746, Loss: 0.2663973569869995\n","Epoch: 747, Loss: 0.2662557363510132\n","Epoch: 748, Loss: 0.26611438393592834\n","Epoch: 749, Loss: 0.2659732401371002\n","Epoch: 750, Loss: 0.26583239436149597\n","Epoch: 751, Loss: 0.26569175720214844\n","Epoch: 752, Loss: 0.2655514180660248\n","Epoch: 753, Loss: 0.26541128754615784\n","Epoch: 754, Loss: 0.2652714252471924\n","Epoch: 755, Loss: 0.2651318311691284\n","Epoch: 756, Loss: 0.26499247550964355\n","Epoch: 757, Loss: 0.2648533582687378\n","Epoch: 758, Loss: 0.26471447944641113\n","Epoch: 759, Loss: 0.2645758390426636\n","Epoch: 760, Loss: 0.2644374370574951\n","Epoch: 761, Loss: 0.26429930329322815\n","Epoch: 762, Loss: 0.26416143774986267\n","Epoch: 763, Loss: 0.2640238106250763\n","Epoch: 764, Loss: 0.26388639211654663\n","Epoch: 765, Loss: 0.26374921202659607\n","Epoch: 766, Loss: 0.2636123299598694\n","Epoch: 767, Loss: 0.263475626707077\n","Epoch: 768, Loss: 0.26333919167518616\n","Epoch: 769, Loss: 0.263202965259552\n","Epoch: 770, Loss: 0.26306697726249695\n","Epoch: 771, Loss: 0.26293131709098816\n","Epoch: 772, Loss: 0.2627957761287689\n","Epoch: 773, Loss: 0.26266056299209595\n","Epoch: 774, Loss: 0.2625255584716797\n","Epoch: 775, Loss: 0.26239073276519775\n","Epoch: 776, Loss: 0.2622562050819397\n","Epoch: 777, Loss: 0.26212188601493835\n","Epoch: 778, Loss: 0.2619878351688385\n","Epoch: 779, Loss: 0.261853963136673\n","Epoch: 780, Loss: 0.26172035932540894\n","Epoch: 781, Loss: 0.261586993932724\n","Epoch: 782, Loss: 0.2614538073539734\n","Epoch: 783, Loss: 0.26132088899612427\n","Epoch: 784, Loss: 0.26118820905685425\n","Epoch: 785, Loss: 0.26105573773384094\n","Epoch: 786, Loss: 0.26092350482940674\n","Epoch: 787, Loss: 0.26079148054122925\n","Epoch: 788, Loss: 0.26065969467163086\n","Epoch: 789, Loss: 0.2605281174182892\n","Epoch: 790, Loss: 0.260396808385849\n","Epoch: 791, Loss: 0.2602657079696655\n","Epoch: 792, Loss: 0.26013481616973877\n","Epoch: 793, Loss: 0.2600041627883911\n","Epoch: 794, Loss: 0.25987371802330017\n","Epoch: 795, Loss: 0.25974348187446594\n","Epoch: 796, Loss: 0.2596135139465332\n","Epoch: 797, Loss: 0.2594837546348572\n","Epoch: 798, Loss: 0.2593541741371155\n","Epoch: 799, Loss: 0.2592248320579529\n","Epoch: 800, Loss: 0.259095698595047\n","Epoch: 801, Loss: 0.2589668333530426\n","Epoch: 802, Loss: 0.25883811712265015\n","Epoch: 803, Loss: 0.25870969891548157\n","Epoch: 804, Loss: 0.2585814297199249\n","Epoch: 805, Loss: 0.2584533989429474\n","Epoch: 806, Loss: 0.25832560658454895\n","Epoch: 807, Loss: 0.25819799304008484\n","Epoch: 808, Loss: 0.25807061791419983\n","Epoch: 809, Loss: 0.25794345140457153\n","Epoch: 810, Loss: 0.25781649351119995\n","Epoch: 811, Loss: 0.2576897144317627\n","Epoch: 812, Loss: 0.25756320357322693\n","Epoch: 813, Loss: 0.2574368715286255\n","Epoch: 814, Loss: 0.25731077790260315\n","Epoch: 815, Loss: 0.25718486309051514\n","Epoch: 816, Loss: 0.25705915689468384\n","Epoch: 817, Loss: 0.25693368911743164\n","Epoch: 818, Loss: 0.25680842995643616\n","Epoch: 819, Loss: 0.2566833794116974\n","Epoch: 820, Loss: 0.25655850768089294\n","Epoch: 821, Loss: 0.2564338445663452\n","Epoch: 822, Loss: 0.2563094198703766\n","Epoch: 823, Loss: 0.2561851739883423\n","Epoch: 824, Loss: 0.2560611367225647\n","Epoch: 825, Loss: 0.2559373378753662\n","Epoch: 826, Loss: 0.25581368803977966\n","Epoch: 827, Loss: 0.25569024682044983\n","Epoch: 828, Loss: 0.2555670440196991\n","Epoch: 829, Loss: 0.2554440498352051\n","Epoch: 830, Loss: 0.2553212344646454\n","Epoch: 831, Loss: 0.2551986277103424\n","Epoch: 832, Loss: 0.25507619976997375\n","Epoch: 833, Loss: 0.2549539804458618\n","Epoch: 834, Loss: 0.2548319697380066\n","Epoch: 835, Loss: 0.2547101676464081\n","Epoch: 836, Loss: 0.2545885741710663\n","Epoch: 837, Loss: 0.2544671297073364\n","Epoch: 838, Loss: 0.2543458938598633\n","Epoch: 839, Loss: 0.25422489643096924\n","Epoch: 840, Loss: 0.2541040778160095\n","Epoch: 841, Loss: 0.25398343801498413\n","Epoch: 842, Loss: 0.25386297702789307\n","Epoch: 843, Loss: 0.2537427544593811\n","Epoch: 844, Loss: 0.25362271070480347\n","Epoch: 845, Loss: 0.25350287556648254\n","Epoch: 846, Loss: 0.25338318943977356\n","Epoch: 847, Loss: 0.2532637417316437\n","Epoch: 848, Loss: 0.2531444728374481\n","Epoch: 849, Loss: 0.2530254125595093\n","Epoch: 850, Loss: 0.25290647149086\n","Epoch: 851, Loss: 0.2527877986431122\n","Epoch: 852, Loss: 0.2526692748069763\n","Epoch: 853, Loss: 0.25255095958709717\n","Epoch: 854, Loss: 0.25243282318115234\n","Epoch: 855, Loss: 0.25231489539146423\n","Epoch: 856, Loss: 0.25219714641571045\n","Epoch: 857, Loss: 0.252079576253891\n","Epoch: 858, Loss: 0.25196218490600586\n","Epoch: 859, Loss: 0.25184500217437744\n","Epoch: 860, Loss: 0.25172799825668335\n","Epoch: 861, Loss: 0.2516111731529236\n","Epoch: 862, Loss: 0.25149455666542053\n","Epoch: 863, Loss: 0.2513781189918518\n","Epoch: 864, Loss: 0.2512618601322174\n","Epoch: 865, Loss: 0.25114578008651733\n","Epoch: 866, Loss: 0.2510298788547516\n","Epoch: 867, Loss: 0.25091415643692017\n","Epoch: 868, Loss: 0.25079861283302307\n","Epoch: 869, Loss: 0.2506832778453827\n","Epoch: 870, Loss: 0.250568151473999\n","Epoch: 871, Loss: 0.2504531443119049\n","Epoch: 872, Loss: 0.2503383457660675\n","Epoch: 873, Loss: 0.25022372603416443\n","Epoch: 874, Loss: 0.2501092851161957\n","Epoch: 875, Loss: 0.24999502301216125\n","Epoch: 876, Loss: 0.24988092482089996\n","Epoch: 877, Loss: 0.2497670203447342\n","Epoch: 878, Loss: 0.24965327978134155\n","Epoch: 879, Loss: 0.24953973293304443\n","Epoch: 880, Loss: 0.24942634999752045\n","Epoch: 881, Loss: 0.24931319057941437\n","Epoch: 882, Loss: 0.24920013546943665\n","Epoch: 883, Loss: 0.24908728897571564\n","Epoch: 884, Loss: 0.24897460639476776\n","Epoch: 885, Loss: 0.2488621324300766\n","Epoch: 886, Loss: 0.24874979257583618\n","Epoch: 887, Loss: 0.24863764643669128\n","Epoch: 888, Loss: 0.24852566421031952\n","Epoch: 889, Loss: 0.24841389060020447\n","Epoch: 890, Loss: 0.24830226600170135\n","Epoch: 891, Loss: 0.248190775513649\n","Epoch: 892, Loss: 0.24807949364185333\n","Epoch: 893, Loss: 0.247968390583992\n","Epoch: 894, Loss: 0.24785739183425903\n","Epoch: 895, Loss: 0.24774664640426636\n","Epoch: 896, Loss: 0.24763603508472443\n","Epoch: 897, Loss: 0.24752557277679443\n","Epoch: 898, Loss: 0.24741534888744354\n","Epoch: 899, Loss: 0.2473052442073822\n","Epoch: 900, Loss: 0.247195303440094\n","Epoch: 901, Loss: 0.2470855563879013\n","Epoch: 902, Loss: 0.24697597324848175\n","Epoch: 903, Loss: 0.24686653912067413\n","Epoch: 904, Loss: 0.24675725400447845\n","Epoch: 905, Loss: 0.2466481775045395\n","Epoch: 906, Loss: 0.24653925001621246\n","Epoch: 907, Loss: 0.24643048644065857\n","Epoch: 908, Loss: 0.2463218867778778\n","Epoch: 909, Loss: 0.24621346592903137\n","Epoch: 910, Loss: 0.2461051642894745\n","Epoch: 911, Loss: 0.2459970861673355\n","Epoch: 912, Loss: 0.24588914215564728\n","Epoch: 913, Loss: 0.24578134715557098\n","Epoch: 914, Loss: 0.24567371606826782\n","Epoch: 915, Loss: 0.245566263794899\n","Epoch: 916, Loss: 0.2454589605331421\n","Epoch: 917, Loss: 0.2453518509864807\n","Epoch: 918, Loss: 0.2452448606491089\n","Epoch: 919, Loss: 0.2451380491256714\n","Epoch: 920, Loss: 0.24503140151500702\n","Epoch: 921, Loss: 0.2449249029159546\n","Epoch: 922, Loss: 0.2448185682296753\n","Epoch: 923, Loss: 0.24471238255500793\n","Epoch: 924, Loss: 0.24460633099079132\n","Epoch: 925, Loss: 0.24450050294399261\n","Epoch: 926, Loss: 0.24439479410648346\n","Epoch: 927, Loss: 0.24428924918174744\n","Epoch: 928, Loss: 0.24418385326862335\n","Epoch: 929, Loss: 0.2440786212682724\n","Epoch: 930, Loss: 0.24397355318069458\n","Epoch: 931, Loss: 0.2438686490058899\n","Epoch: 932, Loss: 0.24376387894153595\n","Epoch: 933, Loss: 0.24365925788879395\n","Epoch: 934, Loss: 0.24355481564998627\n","Epoch: 935, Loss: 0.24345049262046814\n","Epoch: 936, Loss: 0.24334636330604553\n","Epoch: 937, Loss: 0.24324236810207367\n","Epoch: 938, Loss: 0.24313850700855255\n","Epoch: 939, Loss: 0.24303480982780457\n","Epoch: 940, Loss: 0.2429312914609909\n","Epoch: 941, Loss: 0.2428278923034668\n","Epoch: 942, Loss: 0.24272467195987701\n","Epoch: 943, Loss: 0.24262158572673798\n","Epoch: 944, Loss: 0.24251864850521088\n","Epoch: 945, Loss: 0.2424158900976181\n","Epoch: 946, Loss: 0.2423132359981537\n","Epoch: 947, Loss: 0.2422107756137848\n","Epoch: 948, Loss: 0.24210843443870544\n","Epoch: 949, Loss: 0.24200624227523804\n","Epoch: 950, Loss: 0.24190424382686615\n","Epoch: 951, Loss: 0.2418023645877838\n","Epoch: 952, Loss: 0.24170060455799103\n","Epoch: 953, Loss: 0.24159900844097137\n","Epoch: 954, Loss: 0.24149756133556366\n","Epoch: 955, Loss: 0.24139626324176788\n","Epoch: 956, Loss: 0.24129514396190643\n","Epoch: 957, Loss: 0.24119409918785095\n","Epoch: 958, Loss: 0.241093248128891\n","Epoch: 959, Loss: 0.24099254608154297\n","Epoch: 960, Loss: 0.24089199304580688\n","Epoch: 961, Loss: 0.24079157412052155\n","Epoch: 962, Loss: 0.24069125950336456\n","Epoch: 963, Loss: 0.2405911386013031\n","Epoch: 964, Loss: 0.24049115180969238\n","Epoch: 965, Loss: 0.24039128422737122\n","Epoch: 966, Loss: 0.24029162526130676\n","Epoch: 967, Loss: 0.24019204080104828\n","Epoch: 968, Loss: 0.24009262025356293\n","Epoch: 969, Loss: 0.23999333381652832\n","Epoch: 970, Loss: 0.23989421129226685\n","Epoch: 971, Loss: 0.23979520797729492\n","Epoch: 972, Loss: 0.23969638347625732\n","Epoch: 973, Loss: 0.23959766328334808\n","Epoch: 974, Loss: 0.23949910700321198\n","Epoch: 975, Loss: 0.23940065503120422\n","Epoch: 976, Loss: 0.2393023669719696\n","Epoch: 977, Loss: 0.23920422792434692\n","Epoch: 978, Loss: 0.2391061931848526\n","Epoch: 979, Loss: 0.23900830745697021\n","Epoch: 980, Loss: 0.23891058564186096\n","Epoch: 981, Loss: 0.23881298303604126\n","Epoch: 982, Loss: 0.2387154996395111\n","Epoch: 983, Loss: 0.23861819505691528\n","Epoch: 984, Loss: 0.238521009683609\n","Epoch: 985, Loss: 0.23842395842075348\n","Epoch: 986, Loss: 0.2383270561695099\n","Epoch: 987, Loss: 0.23823025822639465\n","Epoch: 988, Loss: 0.23813362419605255\n","Epoch: 989, Loss: 0.238037109375\n","Epoch: 990, Loss: 0.2379407435655594\n","Epoch: 991, Loss: 0.2378445267677307\n","Epoch: 992, Loss: 0.2377483993768692\n","Epoch: 993, Loss: 0.23765242099761963\n","Epoch: 994, Loss: 0.237556591629982\n","Epoch: 995, Loss: 0.2374608963727951\n","Epoch: 996, Loss: 0.23736535012722015\n","Epoch: 997, Loss: 0.23726992309093475\n","Epoch: 998, Loss: 0.2371746003627777\n","Epoch: 999, Loss: 0.2370794415473938\n","Epoch: 1000, Loss: 0.23698440194129944\n"]}]},{"cell_type":"code","source":["# model evaluation\n","with torch.no_grad():\n","  y_pred = model.forward(X_test_tensor)\n","  y_pred = (y_pred > 0.5).float()\n","  accuracy = (y_pred == y_test_tensor).float().mean()\n","  print(f'Accuracy: {accuracy.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ijQ3qDJFIbY","executionInfo":{"status":"ok","timestamp":1749992943687,"user_tz":-480,"elapsed":41,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"dbca988b-e00b-4237-9a88-b043ebd69097"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.5120037198066711\n"]}]},{"cell_type":"code","source":["#showcasing the weights.\n","model.linear.weight.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNQSigHIzv8R","executionInfo":{"status":"ok","timestamp":1749992943701,"user_tz":-480,"elapsed":11,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"fbdd5568-a52d-4422-f321-0b0475ee1759"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 30])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["model.linear.bias.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-QKM80N12Tq","executionInfo":{"status":"ok","timestamp":1749992943722,"user_tz":-480,"elapsed":17,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"3c23ab12-1102-4381-9181-e19626e3e0d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["!pip install torchinfo\n","from torchinfo import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rv2k5jLq2Ldi","executionInfo":{"status":"ok","timestamp":1749992951526,"user_tz":-480,"elapsed":7810,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"b31887d6-6642-407b-e970-23cba855d273"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["summary(model,input_size=(455,30))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raVy8ssl2iNS","executionInfo":{"status":"ok","timestamp":1749992951543,"user_tz":-480,"elapsed":14,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"b5d4407b-3879-4f3c-ba69-5cd0f91913ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","MySimpleNN                               [455, 1]                  --\n","├─Linear: 1-1                            [455, 1]                  31\n","├─Sigmoid: 1-2                           [455, 1]                  --\n","==========================================================================================\n","Total params: 31\n","Trainable params: 31\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 0.01\n","==========================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.06\n","=========================================================================================="]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["===Constructing a Neural Network with a Hidden Layer===<br>\n","Layer 1 (30x5)-->ReLU; Layer 2(5x1)-->Sigmoid; Output Layer (1)\n"],"metadata":{"id":"gr93u5M_3DgC"}},{"cell_type":"code","source":["# create model class\n","import torch\n","import torch.nn as nn\n","\n","# nn.Module, which is the base class for all neural networks in PyTorch.\n","#a. The parent class (nn.Module) does critical setup, including:\n","\n","#b. Registering layers (like nn.Linear, nn.Conv2d, etc.).\n","\n","#c. Making .parameters() work, so the optimizer knows what to update.\n","\n","#d. Enabling features like .cuda(), .eval(), .state_dict(), etc.\n","\n","class TwoLayerNN(nn.Module):\n","  def __init__(self,num_feature):\n","    #super() is a built-in function in Python that returns a temporary object of the parent class,\n","    #so you can call its methods (like __init__) without directly naming the parent class.\n","    #here when we are writing super().__init__() it is actually triggering super().__init__(self)\n","    super().__init__()\n","    #Creating one input layer and a hidden. Linear layer means a fully connected layer.\n","    self.input_layer=nn.Linear(num_feature,5)\n","    self.hidden_layer=nn.Linear(5,1)\n","    self.relu=nn.ReLU()\n","    self.sigmoid=nn.Sigmoid()\n","\n","  def forward(self,input_features):\n","    out=self.input_layer(input_features)\n","    out=self.relu(out)\n","    out=self.hidden_layer(out)\n","    out=self.sigmoid(out)\n","    return out\n","\n"],"metadata":{"id":"4ati1lFQ3C-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr=0.001\n","epochs=300\n","loss_function=nn.BCELoss()"],"metadata":{"id":"D_vxsl57Yg4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TwoLayerNN(X_train_tensor.shape[1])"],"metadata":{"id":"RnKeEu5OYkjW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for param in model.parameters():\n","    #if param.grad is None:\n","        #continue\n","    #param.data = param.data - lr * param.grad.data\n","\n","#===Updating parameters using stochastic gradient descent=====\n","\n","optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n","\n","for epoch in range(epochs):\n","  y_pred=model(X_train_tensor)\n","  loss=loss_function(y_pred,y_train_tensor.view(-1,1))\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  print(f'Epoch is {epoch+1} and loss is {loss}')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSoYdoUMZlgG","executionInfo":{"status":"ok","timestamp":1749992951845,"user_tz":-480,"elapsed":258,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"2d8ff881-d8bf-4f75-94d6-b692202b7041"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch is 1 and loss is 0.7289665341377258\n","Epoch is 2 and loss is 0.7288199663162231\n","Epoch is 3 and loss is 0.72867351770401\n","Epoch is 4 and loss is 0.7285270094871521\n","Epoch is 5 and loss is 0.7283806800842285\n","Epoch is 6 and loss is 0.7282345294952393\n","Epoch is 7 and loss is 0.7280884385108948\n","Epoch is 8 and loss is 0.7279424071311951\n","Epoch is 9 and loss is 0.7277966737747192\n","Epoch is 10 and loss is 0.7276511192321777\n","Epoch is 11 and loss is 0.7275057435035706\n","Epoch is 12 and loss is 0.7273603677749634\n","Epoch is 13 and loss is 0.7272149324417114\n","Epoch is 14 and loss is 0.7270695567131042\n","Epoch is 15 and loss is 0.7269248366355896\n","Epoch is 16 and loss is 0.726780354976654\n","Epoch is 17 and loss is 0.7266359329223633\n","Epoch is 18 and loss is 0.7264918088912964\n","Epoch is 19 and loss is 0.7263476252555847\n","Epoch is 20 and loss is 0.7262036204338074\n","Epoch is 21 and loss is 0.72605961561203\n","Epoch is 22 and loss is 0.7259157299995422\n","Epoch is 23 and loss is 0.7257719039916992\n","Epoch is 24 and loss is 0.7256283164024353\n","Epoch is 25 and loss is 0.7254845499992371\n","Epoch is 26 and loss is 0.7253408432006836\n","Epoch is 27 and loss is 0.725196897983551\n","Epoch is 28 and loss is 0.725053071975708\n","Epoch is 29 and loss is 0.7249091863632202\n","Epoch is 30 and loss is 0.7247655391693115\n","Epoch is 31 and loss is 0.7246218323707581\n","Epoch is 32 and loss is 0.7244781255722046\n","Epoch is 33 and loss is 0.7243345975875854\n","Epoch is 34 and loss is 0.7241911292076111\n","Epoch is 35 and loss is 0.7240477204322815\n","Epoch is 36 and loss is 0.7239041924476624\n","Epoch is 37 and loss is 0.7237606644630432\n","Epoch is 38 and loss is 0.7236171364784241\n","Epoch is 39 and loss is 0.7234737277030945\n","Epoch is 40 and loss is 0.7233309149742126\n","Epoch is 41 and loss is 0.7231884598731995\n","Epoch is 42 and loss is 0.7230461239814758\n","Epoch is 43 and loss is 0.7229037284851074\n","Epoch is 44 and loss is 0.7227615118026733\n","Epoch is 45 and loss is 0.7226192355155945\n","Epoch is 46 and loss is 0.7224770188331604\n","Epoch is 47 and loss is 0.7223349809646606\n","Epoch is 48 and loss is 0.7221930623054504\n","Epoch is 49 and loss is 0.7220513224601746\n","Epoch is 50 and loss is 0.7219095826148987\n","Epoch is 51 and loss is 0.7217680215835571\n","Epoch is 52 and loss is 0.7216266989707947\n","Epoch is 53 and loss is 0.7214855551719666\n","Epoch is 54 and loss is 0.7213445901870728\n","Epoch is 55 and loss is 0.721203625202179\n","Epoch is 56 and loss is 0.7210627198219299\n","Epoch is 57 and loss is 0.7209217548370361\n","Epoch is 58 and loss is 0.7207809090614319\n","Epoch is 59 and loss is 0.7206401228904724\n","Epoch is 60 and loss is 0.7204994559288025\n","Epoch is 61 and loss is 0.7203588485717773\n","Epoch is 62 and loss is 0.7202184796333313\n","Epoch is 63 and loss is 0.7200782895088196\n","Epoch is 64 and loss is 0.7199380397796631\n","Epoch is 65 and loss is 0.7197977900505066\n","Epoch is 66 and loss is 0.7196577191352844\n","Epoch is 67 and loss is 0.7195176482200623\n","Epoch is 68 and loss is 0.7193776369094849\n","Epoch is 69 and loss is 0.7192376255989075\n","Epoch is 70 and loss is 0.7190976738929749\n","Epoch is 71 and loss is 0.718957781791687\n","Epoch is 72 and loss is 0.7188178896903992\n","Epoch is 73 and loss is 0.7186777591705322\n","Epoch is 74 and loss is 0.7185370326042175\n","Epoch is 75 and loss is 0.7183963060379028\n","Epoch is 76 and loss is 0.7182558178901672\n","Epoch is 77 and loss is 0.7181151509284973\n","Epoch is 78 and loss is 0.7179746627807617\n","Epoch is 79 and loss is 0.7178341746330261\n","Epoch is 80 and loss is 0.7176939845085144\n","Epoch is 81 and loss is 0.7175541520118713\n","Epoch is 82 and loss is 0.717414379119873\n","Epoch is 83 and loss is 0.7172746658325195\n","Epoch is 84 and loss is 0.7171350717544556\n","Epoch is 85 and loss is 0.7169955372810364\n","Epoch is 86 and loss is 0.7168560028076172\n","Epoch is 87 and loss is 0.7167165279388428\n","Epoch is 88 and loss is 0.7165766358375549\n","Epoch is 89 and loss is 0.7164368629455566\n","Epoch is 90 and loss is 0.716297447681427\n","Epoch is 91 and loss is 0.7161581516265869\n","Epoch is 92 and loss is 0.7160189151763916\n","Epoch is 93 and loss is 0.7158796787261963\n","Epoch is 94 and loss is 0.7157405614852905\n","Epoch is 95 and loss is 0.7156015038490295\n","Epoch is 96 and loss is 0.7154625654220581\n","Epoch is 97 and loss is 0.7153235673904419\n","Epoch is 98 and loss is 0.7151845693588257\n","Epoch is 99 and loss is 0.7150456309318542\n","Epoch is 100 and loss is 0.7149067521095276\n","Epoch is 101 and loss is 0.7147679328918457\n","Epoch is 102 and loss is 0.7146294116973877\n","Epoch is 103 and loss is 0.7144910097122192\n","Epoch is 104 and loss is 0.7143527865409851\n","Epoch is 105 and loss is 0.7142146229743958\n","Epoch is 106 and loss is 0.714076578617096\n","Epoch is 107 and loss is 0.7139388918876648\n","Epoch is 108 and loss is 0.7138013243675232\n","Epoch is 109 and loss is 0.7136638164520264\n","Epoch is 110 and loss is 0.7135267853736877\n","Epoch is 111 and loss is 0.7133897542953491\n","Epoch is 112 and loss is 0.7132527232170105\n","Epoch is 113 and loss is 0.7131158113479614\n","Epoch is 114 and loss is 0.7129789590835571\n","Epoch is 115 and loss is 0.7128419876098633\n","Epoch is 116 and loss is 0.7127052545547485\n","Epoch is 117 and loss is 0.7125684022903442\n","Epoch is 118 and loss is 0.7124316096305847\n","Epoch is 119 and loss is 0.7122950553894043\n","Epoch is 120 and loss is 0.7121582627296448\n","Epoch is 121 and loss is 0.7120216488838196\n","Epoch is 122 and loss is 0.7118849754333496\n","Epoch is 123 and loss is 0.7117483019828796\n","Epoch is 124 and loss is 0.7116116881370544\n","Epoch is 125 and loss is 0.7114752531051636\n","Epoch is 126 and loss is 0.7113386988639832\n","Epoch is 127 and loss is 0.7112023234367371\n","Epoch is 128 and loss is 0.711065948009491\n","Epoch is 129 and loss is 0.7109296917915344\n","Epoch is 130 and loss is 0.7107941508293152\n","Epoch is 131 and loss is 0.7106586694717407\n","Epoch is 132 and loss is 0.710523247718811\n","Epoch is 133 and loss is 0.7103878855705261\n","Epoch is 134 and loss is 0.710252583026886\n","Epoch is 135 and loss is 0.7101172804832458\n","Epoch is 136 and loss is 0.7099819183349609\n","Epoch is 137 and loss is 0.7098466753959656\n","Epoch is 138 and loss is 0.7097115516662598\n","Epoch is 139 and loss is 0.7095763683319092\n","Epoch is 140 and loss is 0.7094412446022034\n","Epoch is 141 and loss is 0.7093061804771423\n","Epoch is 142 and loss is 0.7091711759567261\n","Epoch is 143 and loss is 0.7090362310409546\n","Epoch is 144 and loss is 0.7089012861251831\n","Epoch is 145 and loss is 0.7087664008140564\n","Epoch is 146 and loss is 0.7086312174797058\n","Epoch is 147 and loss is 0.7084961533546448\n","Epoch is 148 and loss is 0.7083610892295837\n","Epoch is 149 and loss is 0.7082262635231018\n","Epoch is 150 and loss is 0.7080914378166199\n","Epoch is 151 and loss is 0.7079567313194275\n","Epoch is 152 and loss is 0.7078219652175903\n","Epoch is 153 and loss is 0.7076871991157532\n","Epoch is 154 and loss is 0.7075521349906921\n","Epoch is 155 and loss is 0.7074167728424072\n","Epoch is 156 and loss is 0.7072814106941223\n","Epoch is 157 and loss is 0.7071458101272583\n","Epoch is 158 and loss is 0.7070103287696838\n","Epoch is 159 and loss is 0.7068748474121094\n","Epoch is 160 and loss is 0.7067392468452454\n","Epoch is 161 and loss is 0.7066034078598022\n","Epoch is 162 and loss is 0.7064678072929382\n","Epoch is 163 and loss is 0.7063322067260742\n","Epoch is 164 and loss is 0.7061967253684998\n","Epoch is 165 and loss is 0.7060612440109253\n","Epoch is 166 and loss is 0.7059258818626404\n","Epoch is 167 and loss is 0.7057904005050659\n","Epoch is 168 and loss is 0.7056550979614258\n","Epoch is 169 and loss is 0.7055197358131409\n","Epoch is 170 and loss is 0.7053844332695007\n","Epoch is 171 and loss is 0.7052491307258606\n","Epoch is 172 and loss is 0.7051138877868652\n","Epoch is 173 and loss is 0.7049786448478699\n","Epoch is 174 and loss is 0.7048435211181641\n","Epoch is 175 and loss is 0.7047082781791687\n","Epoch is 176 and loss is 0.7045731544494629\n","Epoch is 177 and loss is 0.7044381499290466\n","Epoch is 178 and loss is 0.7043032646179199\n","Epoch is 179 and loss is 0.7041683197021484\n","Epoch is 180 and loss is 0.7040334343910217\n","Epoch is 181 and loss is 0.703898549079895\n","Epoch is 182 and loss is 0.7037632465362549\n","Epoch is 183 and loss is 0.7036281228065491\n","Epoch is 184 and loss is 0.7034928798675537\n","Epoch is 185 and loss is 0.7033578753471375\n","Epoch is 186 and loss is 0.7032227516174316\n","Epoch is 187 and loss is 0.7030876874923706\n","Epoch is 188 and loss is 0.7029527425765991\n","Epoch is 189 and loss is 0.7028176784515381\n","Epoch is 190 and loss is 0.7026827931404114\n","Epoch is 191 and loss is 0.7025482058525085\n","Epoch is 192 and loss is 0.7024134397506714\n","Epoch is 193 and loss is 0.7022785544395447\n","Epoch is 194 and loss is 0.7021436095237732\n","Epoch is 195 and loss is 0.7020084857940674\n","Epoch is 196 and loss is 0.7018734216690063\n","Epoch is 197 and loss is 0.7017385363578796\n","Epoch is 198 and loss is 0.7016039490699768\n","Epoch is 199 and loss is 0.7014691233634949\n","Epoch is 200 and loss is 0.7013342976570129\n","Epoch is 201 and loss is 0.7011997103691101\n","Epoch is 202 and loss is 0.7010651230812073\n","Epoch is 203 and loss is 0.7009305357933044\n","Epoch is 204 and loss is 0.7007960677146912\n","Epoch is 205 and loss is 0.7006616592407227\n","Epoch is 206 and loss is 0.7005272507667542\n","Epoch is 207 and loss is 0.7003929018974304\n","Epoch is 208 and loss is 0.7002587914466858\n","Epoch is 209 and loss is 0.7001245617866516\n","Epoch is 210 and loss is 0.6999907493591309\n","Epoch is 211 and loss is 0.6998569369316101\n","Epoch is 212 and loss is 0.6997231245040894\n","Epoch is 213 and loss is 0.6995891332626343\n","Epoch is 214 and loss is 0.6994550824165344\n","Epoch is 215 and loss is 0.6993212103843689\n","Epoch is 216 and loss is 0.6991872191429138\n","Epoch is 217 and loss is 0.6990533471107483\n","Epoch is 218 and loss is 0.6989195346832275\n","Epoch is 219 and loss is 0.6987857222557068\n","Epoch is 220 and loss is 0.6986518502235413\n","Epoch is 221 and loss is 0.6985180974006653\n","Epoch is 222 and loss is 0.6983844041824341\n","Epoch is 223 and loss is 0.6982506513595581\n","Epoch is 224 and loss is 0.6981168985366821\n","Epoch is 225 and loss is 0.6979832053184509\n","Epoch is 226 and loss is 0.697849690914154\n","Epoch is 227 and loss is 0.6977164149284363\n","Epoch is 228 and loss is 0.6975831985473633\n","Epoch is 229 and loss is 0.6974506378173828\n","Epoch is 230 and loss is 0.6973179578781128\n","Epoch is 231 and loss is 0.6971853375434875\n","Epoch is 232 and loss is 0.6970528364181519\n","Epoch is 233 and loss is 0.6969207525253296\n","Epoch is 234 and loss is 0.6967881917953491\n","Epoch is 235 and loss is 0.6966554522514343\n","Epoch is 236 and loss is 0.6965227127075195\n","Epoch is 237 and loss is 0.6963900923728943\n","Epoch is 238 and loss is 0.6962573528289795\n","Epoch is 239 and loss is 0.696124792098999\n","Epoch is 240 and loss is 0.6959925889968872\n","Epoch is 241 and loss is 0.6958600878715515\n","Epoch is 242 and loss is 0.6957273483276367\n","Epoch is 243 and loss is 0.6955945491790771\n","Epoch is 244 and loss is 0.6954618096351624\n","Epoch is 245 and loss is 0.6953291296958923\n","Epoch is 246 and loss is 0.6951963901519775\n","Epoch is 247 and loss is 0.6950637698173523\n","Epoch is 248 and loss is 0.694931149482727\n","Epoch is 249 and loss is 0.6947985887527466\n","Epoch is 250 and loss is 0.6946659684181213\n","Epoch is 251 and loss is 0.6945331692695618\n","Epoch is 252 and loss is 0.6944003105163574\n","Epoch is 253 and loss is 0.6942675709724426\n","Epoch is 254 and loss is 0.6941348314285278\n","Epoch is 255 and loss is 0.6940023303031921\n","Epoch is 256 and loss is 0.693869948387146\n","Epoch is 257 and loss is 0.6937376856803894\n","Epoch is 258 and loss is 0.6936055421829224\n","Epoch is 259 and loss is 0.693473219871521\n","Epoch is 260 and loss is 0.6933408379554749\n","Epoch is 261 and loss is 0.6932083368301392\n","Epoch is 262 and loss is 0.6930761933326721\n","Epoch is 263 and loss is 0.6929441094398499\n","Epoch is 264 and loss is 0.6928120255470276\n","Epoch is 265 and loss is 0.692680299282074\n","Epoch is 266 and loss is 0.6925486326217651\n","Epoch is 267 and loss is 0.6924172639846802\n","Epoch is 268 and loss is 0.6922858953475952\n","Epoch is 269 and loss is 0.6921546459197998\n","Epoch is 270 and loss is 0.6920233368873596\n","Epoch is 271 and loss is 0.6918920278549194\n","Epoch is 272 and loss is 0.691760778427124\n","Epoch is 273 and loss is 0.6916294693946838\n","Epoch is 274 and loss is 0.6914981007575989\n","Epoch is 275 and loss is 0.6913667917251587\n","Epoch is 276 and loss is 0.6912355422973633\n","Epoch is 277 and loss is 0.6911042928695679\n","Epoch is 278 and loss is 0.6909732222557068\n","Epoch is 279 and loss is 0.6908425688743591\n","Epoch is 280 and loss is 0.6907119154930115\n","Epoch is 281 and loss is 0.6905816793441772\n","Epoch is 282 and loss is 0.6904513239860535\n","Epoch is 283 and loss is 0.6903207898139954\n","Epoch is 284 and loss is 0.6901901960372925\n","Epoch is 285 and loss is 0.6900596022605896\n","Epoch is 286 and loss is 0.6899290680885315\n","Epoch is 287 and loss is 0.6897985339164734\n","Epoch is 288 and loss is 0.6896679401397705\n","Epoch is 289 and loss is 0.689537525177002\n","Epoch is 290 and loss is 0.6894069314002991\n","Epoch is 291 and loss is 0.6892764568328857\n","Epoch is 292 and loss is 0.6891459822654724\n","Epoch is 293 and loss is 0.6890156865119934\n","Epoch is 294 and loss is 0.6888854503631592\n","Epoch is 295 and loss is 0.6887550354003906\n","Epoch is 296 and loss is 0.6886247992515564\n","Epoch is 297 and loss is 0.6884943842887878\n","Epoch is 298 and loss is 0.6883641481399536\n","Epoch is 299 and loss is 0.6882337927818298\n","Epoch is 300 and loss is 0.688103199005127\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","  y_pred=model.forward(X_test_tensor)\n","  y_pred=(y_pred>0.5).float()\n","  accuracy=(y_pred==y_test_tensor).float().mean()\n","  print(f'Accuracy is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iabyb3XVnA_Q","executionInfo":{"status":"ok","timestamp":1749992951906,"user_tz":-480,"elapsed":82,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"152ccdf8-de11-4290-8803-293bc26ccbc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 0.4566020369529724\n"]}]},{"cell_type":"code","source":["summary(model,input_size=(455,30))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c--RDiJjntzA","executionInfo":{"status":"ok","timestamp":1749992951911,"user_tz":-480,"elapsed":31,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"c99a830d-0b24-4058-976e-9bda2ecc25bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","TwoLayerNN                               [455, 1]                  --\n","├─Linear: 1-1                            [455, 5]                  155\n","├─ReLU: 1-2                              [455, 5]                  --\n","├─Linear: 1-3                            [455, 1]                  6\n","├─Sigmoid: 1-4                           [455, 1]                  --\n","==========================================================================================\n","Total params: 161\n","Trainable params: 161\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 0.07\n","==========================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 0.02\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.08\n","=========================================================================================="]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["To Simplify the work we will put the (self.input_layer=nn.Linear(num_feature,5)\n","    self.hidden_layer=nn.Linear(5,1)\n","    self.relu=nn.ReLU()\n","    self.sigmoid=nn.Sigmoid()) into a sequential container."],"metadata":{"id":"m9__yxKDoDEW"}},{"cell_type":"markdown","source":["In PyTorch, containers are classes or data structures designed to hold and organize neural network components such as layers, modules, parameters, or other sub-networks. They help in structuring complex neural network architectures and managing components efficiently.\n","Link:https://medium.com/@karuneshu21/containers-pytorch-10bc14a63aa4"],"metadata":{"id":"RXhow5Mwobr4"}},{"cell_type":"markdown","source":["What is nn.Sequential?<br>\n","Ans: nn.Sequential is a container module in PyTorch that lets you stack layers and operations in the exact order you want them to run, without needing to manually define the forward() method."],"metadata":{"id":"cmLstKz0ppGQ"}},{"cell_type":"code","source":["# create model class\n","import torch\n","import torch.nn as nn\n","\n","\n","\n","class TwoLayerNN(nn.Module):\n","  def __init__(self,num_feature):\n","\n","    super().__init__()\n","    #Making Sequential Container\n","\n","    self.network=nn.Sequential(\n","        nn.Linear(num_feature,5),\n","        nn.ReLU(),\n","        nn.Linear(5,1),\n","        nn.Sigmoid()\n","    )\n","\n","\n","  def forward(self,input_features):\n","    out=self.network(input_features)\n","    return out\n","\n"],"metadata":{"id":"fhk1II6ln03_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr=0.001\n","epochs=300\n","loss_function=nn.BCELoss()\n","model = TwoLayerNN(X_train_tensor.shape[1])\n","\n","\n","optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n","\n","for epoch in range(epochs):\n","  y_pred=model(X_train_tensor)\n","  loss=loss_function(y_pred,y_train_tensor.view(-1,1))\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  print(f'Epoch is {epoch+1} and loss is {loss}')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZlSITVd4FiP","executionInfo":{"status":"ok","timestamp":1749995933667,"user_tz":-480,"elapsed":373,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"d4b69089-9d02-4bba-f0b3-1c00d832fdf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch is 1 and loss is 0.6784456968307495\n","Epoch is 2 and loss is 0.6781927943229675\n","Epoch is 3 and loss is 0.677940309047699\n","Epoch is 4 and loss is 0.6776874661445618\n","Epoch is 5 and loss is 0.6774352192878723\n","Epoch is 6 and loss is 0.6771834492683411\n","Epoch is 7 and loss is 0.6769318580627441\n","Epoch is 8 and loss is 0.6766799688339233\n","Epoch is 9 and loss is 0.6764283180236816\n","Epoch is 10 and loss is 0.6761767268180847\n","Epoch is 11 and loss is 0.6759248971939087\n","Epoch is 12 and loss is 0.6756730675697327\n","Epoch is 13 and loss is 0.6754213571548462\n","Epoch is 14 and loss is 0.6751700639724731\n","Epoch is 15 and loss is 0.6749191284179688\n","Epoch is 16 and loss is 0.6746683120727539\n","Epoch is 17 and loss is 0.6744175553321838\n","Epoch is 18 and loss is 0.6741669774055481\n","Epoch is 19 and loss is 0.6739153265953064\n","Epoch is 20 and loss is 0.6736639142036438\n","Epoch is 21 and loss is 0.6734126210212708\n","Epoch is 22 and loss is 0.6731615662574768\n","Epoch is 23 and loss is 0.6729108691215515\n","Epoch is 24 and loss is 0.6726602911949158\n","Epoch is 25 and loss is 0.6724099516868591\n","Epoch is 26 and loss is 0.6721602082252502\n","Epoch is 27 and loss is 0.6719118356704712\n","Epoch is 28 and loss is 0.6716635823249817\n","Epoch is 29 and loss is 0.6714158058166504\n","Epoch is 30 and loss is 0.6711684465408325\n","Epoch is 31 and loss is 0.6709214448928833\n","Epoch is 32 and loss is 0.6706748008728027\n","Epoch is 33 and loss is 0.6704283356666565\n","Epoch is 34 and loss is 0.6701817512512207\n","Epoch is 35 and loss is 0.6699352264404297\n","Epoch is 36 and loss is 0.669687807559967\n","Epoch is 37 and loss is 0.6694406867027283\n","Epoch is 38 and loss is 0.6691944003105164\n","Epoch is 39 and loss is 0.6689496040344238\n","Epoch is 40 and loss is 0.6687050461769104\n","Epoch is 41 and loss is 0.6684609055519104\n","Epoch is 42 and loss is 0.6682174801826477\n","Epoch is 43 and loss is 0.6679742336273193\n","Epoch is 44 and loss is 0.6677314043045044\n","Epoch is 45 and loss is 0.6674889326095581\n","Epoch is 46 and loss is 0.6672466397285461\n","Epoch is 47 and loss is 0.6670039892196655\n","Epoch is 48 and loss is 0.6667616367340088\n","Epoch is 49 and loss is 0.6665201783180237\n","Epoch is 50 and loss is 0.6662807464599609\n","Epoch is 51 and loss is 0.6660422682762146\n","Epoch is 52 and loss is 0.6658036112785339\n","Epoch is 53 and loss is 0.6655648350715637\n","Epoch is 54 and loss is 0.6653274297714233\n","Epoch is 55 and loss is 0.6650901436805725\n","Epoch is 56 and loss is 0.6648532748222351\n","Epoch is 57 and loss is 0.6646164655685425\n","Epoch is 58 and loss is 0.6643801927566528\n","Epoch is 59 and loss is 0.6641442179679871\n","Epoch is 60 and loss is 0.6639084815979004\n","Epoch is 61 and loss is 0.6636727452278137\n","Epoch is 62 and loss is 0.6634371876716614\n","Epoch is 63 and loss is 0.6632033586502075\n","Epoch is 64 and loss is 0.6629704833030701\n","Epoch is 65 and loss is 0.6627379059791565\n","Epoch is 66 and loss is 0.6625056266784668\n","Epoch is 67 and loss is 0.6622737646102905\n","Epoch is 68 and loss is 0.6620421409606934\n","Epoch is 69 and loss is 0.6618105173110962\n","Epoch is 70 and loss is 0.6615788340568542\n","Epoch is 71 and loss is 0.6613467931747437\n","Epoch is 72 and loss is 0.6611146926879883\n","Epoch is 73 and loss is 0.6608824133872986\n","Epoch is 74 and loss is 0.6606503129005432\n","Epoch is 75 and loss is 0.6604176759719849\n","Epoch is 76 and loss is 0.6601846814155579\n","Epoch is 77 and loss is 0.6599512696266174\n","Epoch is 78 and loss is 0.6597171425819397\n","Epoch is 79 and loss is 0.6594831347465515\n","Epoch is 80 and loss is 0.6592492461204529\n","Epoch is 81 and loss is 0.6590149402618408\n","Epoch is 82 and loss is 0.6587800979614258\n","Epoch is 83 and loss is 0.6585456132888794\n","Epoch is 84 and loss is 0.6583112478256226\n","Epoch is 85 and loss is 0.6580771803855896\n","Epoch is 86 and loss is 0.6578433513641357\n","Epoch is 87 and loss is 0.6576099395751953\n","Epoch is 88 and loss is 0.6573770642280579\n","Epoch is 89 and loss is 0.6571451425552368\n","Epoch is 90 and loss is 0.656913161277771\n","Epoch is 91 and loss is 0.656681478023529\n","Epoch is 92 and loss is 0.6564493179321289\n","Epoch is 93 and loss is 0.6562171578407288\n","Epoch is 94 and loss is 0.6559851765632629\n","Epoch is 95 and loss is 0.6557533144950867\n","Epoch is 96 and loss is 0.6555216908454895\n","Epoch is 97 and loss is 0.6552897095680237\n","Epoch is 98 and loss is 0.6550586819648743\n","Epoch is 99 and loss is 0.6548279523849487\n","Epoch is 100 and loss is 0.6545973420143127\n","Epoch is 101 and loss is 0.6543669104576111\n","Epoch is 102 and loss is 0.6541376709938049\n","Epoch is 103 and loss is 0.6539083123207092\n","Epoch is 104 and loss is 0.6536792516708374\n","Epoch is 105 and loss is 0.6534503102302551\n","Epoch is 106 and loss is 0.6532220244407654\n","Epoch is 107 and loss is 0.6529935002326965\n","Epoch is 108 and loss is 0.6527649760246277\n","Epoch is 109 and loss is 0.6525368094444275\n","Epoch is 110 and loss is 0.6523086428642273\n","Epoch is 111 and loss is 0.6520802974700928\n","Epoch is 112 and loss is 0.6518520712852478\n","Epoch is 113 and loss is 0.6516234278678894\n","Epoch is 114 and loss is 0.6513949036598206\n","Epoch is 115 and loss is 0.6511666178703308\n","Epoch is 116 and loss is 0.6509385108947754\n","Epoch is 117 and loss is 0.6507104635238647\n","Epoch is 118 and loss is 0.6504828333854675\n","Epoch is 119 and loss is 0.6502555012702942\n","Epoch is 120 and loss is 0.6500282883644104\n","Epoch is 121 and loss is 0.6498011946678162\n","Epoch is 122 and loss is 0.6495745182037354\n","Epoch is 123 and loss is 0.6493480205535889\n","Epoch is 124 and loss is 0.6491215825080872\n","Epoch is 125 and loss is 0.6488959789276123\n","Epoch is 126 and loss is 0.6486706137657166\n","Epoch is 127 and loss is 0.6484451293945312\n","Epoch is 128 and loss is 0.6482201218605042\n","Epoch is 129 and loss is 0.64799565076828\n","Epoch is 130 and loss is 0.6477715373039246\n","Epoch is 131 and loss is 0.6475476026535034\n","Epoch is 132 and loss is 0.6473237872123718\n","Epoch is 133 and loss is 0.6471003890037537\n","Epoch is 134 and loss is 0.6468772292137146\n","Epoch is 135 and loss is 0.6466544270515442\n","Epoch is 136 and loss is 0.6464324593544006\n","Epoch is 137 and loss is 0.646210789680481\n","Epoch is 138 and loss is 0.6459893584251404\n","Epoch is 139 and loss is 0.6457683444023132\n","Epoch is 140 and loss is 0.6455474495887756\n","Epoch is 141 and loss is 0.6453263759613037\n","Epoch is 142 and loss is 0.6451050043106079\n","Epoch is 143 and loss is 0.6448836922645569\n","Epoch is 144 and loss is 0.644663393497467\n","Epoch is 145 and loss is 0.6444437503814697\n","Epoch is 146 and loss is 0.6442233324050903\n","Epoch is 147 and loss is 0.64400315284729\n","Epoch is 148 and loss is 0.6437831521034241\n","Epoch is 149 and loss is 0.6435630917549133\n","Epoch is 150 and loss is 0.6433428525924683\n","Epoch is 151 and loss is 0.6431230306625366\n","Epoch is 152 and loss is 0.642903745174408\n","Epoch is 153 and loss is 0.6426848769187927\n","Epoch is 154 and loss is 0.6424662470817566\n","Epoch is 155 and loss is 0.6422476172447205\n","Epoch is 156 and loss is 0.6420292854309082\n","Epoch is 157 and loss is 0.6418110132217407\n","Epoch is 158 and loss is 0.6415930390357971\n","Epoch is 159 and loss is 0.6413753032684326\n","Epoch is 160 and loss is 0.6411578059196472\n","Epoch is 161 and loss is 0.6409403681755066\n","Epoch is 162 and loss is 0.6407228708267212\n","Epoch is 163 and loss is 0.6405048966407776\n","Epoch is 164 and loss is 0.6402869820594788\n","Epoch is 165 and loss is 0.6400690674781799\n","Epoch is 166 and loss is 0.6398512721061707\n","Epoch is 167 and loss is 0.63963383436203\n","Epoch is 168 and loss is 0.6394164562225342\n","Epoch is 169 and loss is 0.6391991972923279\n","Epoch is 170 and loss is 0.6389824151992798\n","Epoch is 171 and loss is 0.6387658715248108\n","Epoch is 172 and loss is 0.6385496854782104\n","Epoch is 173 and loss is 0.6383340954780579\n","Epoch is 174 and loss is 0.6381192207336426\n","Epoch is 175 and loss is 0.6379043459892273\n","Epoch is 176 and loss is 0.6376897692680359\n","Epoch is 177 and loss is 0.637475311756134\n","Epoch is 178 and loss is 0.6372610330581665\n","Epoch is 179 and loss is 0.6370468735694885\n","Epoch is 180 and loss is 0.6368317604064941\n","Epoch is 181 and loss is 0.6366168260574341\n","Epoch is 182 and loss is 0.6364020109176636\n","Epoch is 183 and loss is 0.6361875534057617\n","Epoch is 184 and loss is 0.6359724998474121\n","Epoch is 185 and loss is 0.6357578039169312\n","Epoch is 186 and loss is 0.6355434656143188\n","Epoch is 187 and loss is 0.6353290677070618\n","Epoch is 188 and loss is 0.6351150274276733\n","Epoch is 189 and loss is 0.6349009871482849\n","Epoch is 190 and loss is 0.6346871256828308\n","Epoch is 191 and loss is 0.6344729661941528\n","Epoch is 192 and loss is 0.634259045124054\n","Epoch is 193 and loss is 0.634044885635376\n","Epoch is 194 and loss is 0.6338308453559875\n","Epoch is 195 and loss is 0.633617103099823\n","Epoch is 196 and loss is 0.6334033608436584\n","Epoch is 197 and loss is 0.6331897974014282\n","Epoch is 198 and loss is 0.6329763531684875\n","Epoch is 199 and loss is 0.6327630877494812\n","Epoch is 200 and loss is 0.6325499415397644\n","Epoch is 201 and loss is 0.6323369145393372\n","Epoch is 202 and loss is 0.6321240067481995\n","Epoch is 203 and loss is 0.63191157579422\n","Epoch is 204 and loss is 0.6316993236541748\n","Epoch is 205 and loss is 0.6314877271652222\n","Epoch is 206 and loss is 0.63127601146698\n","Epoch is 207 and loss is 0.6310644149780273\n","Epoch is 208 and loss is 0.6308528184890747\n","Epoch is 209 and loss is 0.6306413412094116\n","Epoch is 210 and loss is 0.6304299235343933\n","Epoch is 211 and loss is 0.6302185654640198\n","Epoch is 212 and loss is 0.6300068497657776\n","Epoch is 213 and loss is 0.6297953128814697\n","Epoch is 214 and loss is 0.6295837163925171\n","Epoch is 215 and loss is 0.6293723583221436\n","Epoch is 216 and loss is 0.6291610598564148\n","Epoch is 217 and loss is 0.6289499402046204\n","Epoch is 218 and loss is 0.6287388205528259\n","Epoch is 219 and loss is 0.6285278797149658\n","Epoch is 220 and loss is 0.6283171772956848\n","Epoch is 221 and loss is 0.628106951713562\n","Epoch is 222 and loss is 0.6278975605964661\n","Epoch is 223 and loss is 0.6276891827583313\n","Epoch is 224 and loss is 0.6274809837341309\n","Epoch is 225 and loss is 0.6272730827331543\n","Epoch is 226 and loss is 0.6270652413368225\n","Epoch is 227 and loss is 0.626857340335846\n","Epoch is 228 and loss is 0.6266493201255798\n","Epoch is 229 and loss is 0.626441240310669\n","Epoch is 230 and loss is 0.6262331008911133\n","Epoch is 231 and loss is 0.6260249018669128\n","Epoch is 232 and loss is 0.6258166432380676\n","Epoch is 233 and loss is 0.6256083250045776\n","Epoch is 234 and loss is 0.6254000663757324\n","Epoch is 235 and loss is 0.6251919865608215\n","Epoch is 236 and loss is 0.6249839067459106\n","Epoch is 237 and loss is 0.6247760653495789\n","Epoch is 238 and loss is 0.6245682835578918\n","Epoch is 239 and loss is 0.6243605613708496\n","Epoch is 240 and loss is 0.6241530776023865\n","Epoch is 241 and loss is 0.6239456534385681\n","Epoch is 242 and loss is 0.6237382292747498\n","Epoch is 243 and loss is 0.623530924320221\n","Epoch is 244 and loss is 0.6233238577842712\n","Epoch is 245 and loss is 0.6231168508529663\n","Epoch is 246 and loss is 0.6229099035263062\n","Epoch is 247 and loss is 0.6227030754089355\n","Epoch is 248 and loss is 0.622496485710144\n","Epoch is 249 and loss is 0.6222901344299316\n","Epoch is 250 and loss is 0.6220836639404297\n","Epoch is 251 and loss is 0.6218774318695068\n","Epoch is 252 and loss is 0.6216713786125183\n","Epoch is 253 and loss is 0.6214651465415955\n","Epoch is 254 and loss is 0.6212586164474487\n","Epoch is 255 and loss is 0.6210524439811707\n","Epoch is 256 and loss is 0.6208463311195374\n","Epoch is 257 and loss is 0.6206403374671936\n","Epoch is 258 and loss is 0.6204344630241394\n","Epoch is 259 and loss is 0.62022864818573\n","Epoch is 260 and loss is 0.6200229525566101\n","Epoch is 261 and loss is 0.6198179125785828\n","Epoch is 262 and loss is 0.6196138262748718\n","Epoch is 263 and loss is 0.6194099187850952\n","Epoch is 264 and loss is 0.6192061305046082\n","Epoch is 265 and loss is 0.619002640247345\n","Epoch is 266 and loss is 0.618798553943634\n","Epoch is 267 and loss is 0.6185945272445679\n","Epoch is 268 and loss is 0.6183905005455017\n","Epoch is 269 and loss is 0.6181854605674744\n","Epoch is 270 and loss is 0.6179795861244202\n","Epoch is 271 and loss is 0.6177736520767212\n","Epoch is 272 and loss is 0.617567241191864\n","Epoch is 273 and loss is 0.6173611879348755\n","Epoch is 274 and loss is 0.6171560287475586\n","Epoch is 275 and loss is 0.6169509887695312\n","Epoch is 276 and loss is 0.616746187210083\n","Epoch is 277 and loss is 0.6165414452552795\n","Epoch is 278 and loss is 0.6163375973701477\n","Epoch is 279 and loss is 0.6161336898803711\n","Epoch is 280 and loss is 0.615929901599884\n","Epoch is 281 and loss is 0.6157262921333313\n","Epoch is 282 and loss is 0.6155240535736084\n","Epoch is 283 and loss is 0.6153221130371094\n","Epoch is 284 and loss is 0.6151204705238342\n","Epoch is 285 and loss is 0.6149191856384277\n","Epoch is 286 and loss is 0.6147179007530212\n","Epoch is 287 and loss is 0.6145168542861938\n","Epoch is 288 and loss is 0.6143161058425903\n","Epoch is 289 and loss is 0.6141154170036316\n","Epoch is 290 and loss is 0.6139147877693176\n","Epoch is 291 and loss is 0.6137144565582275\n","Epoch is 292 and loss is 0.6135141849517822\n","Epoch is 293 and loss is 0.6133139133453369\n","Epoch is 294 and loss is 0.613113522529602\n","Epoch is 295 and loss is 0.6129134893417358\n","Epoch is 296 and loss is 0.6127135753631592\n","Epoch is 297 and loss is 0.6125139594078064\n","Epoch is 298 and loss is 0.6123147010803223\n","Epoch is 299 and loss is 0.6121156811714172\n","Epoch is 300 and loss is 0.6119168996810913\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","  y_pred=model.forward(X_test_tensor)\n","  y_pred=(y_pred>0.5).float()\n","  accuracy=(y_pred==y_test_tensor).float().mean()\n","  print(f'Accuracy is {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6lw26zH4Tnv","executionInfo":{"status":"ok","timestamp":1749995939325,"user_tz":-480,"elapsed":24,"user":{"displayName":"Ayan Panja","userId":"07386613023293252846"}},"outputId":"6be4d15f-c55e-4530-d895-a1f15e68514c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 0.5517082214355469\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}